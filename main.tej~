%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\vk}[1]{#1}
\newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
Monocular Sphere-Based Occupancy Mapping and Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  % We present a new method of estimating inverse depth and using such estimates to build an occupancy map composed of spheres and points from sparse point cloud data obtained from a single RGB camera, for example by visual-inertial SLAM.
  We present a novel method of building an occupancy representation using only a monocular camera and odometry data, that enables fast, safety-aware navigation on inexpensive UAVs. 
  % and demonstrate how our method enables inexpensive UAVs to navigate quickly in unknown, unstructured 3D environments in simulation and in the real world.
  % Our approach consists of a visual keypoint tracking and inverse-depth estimation module robust to odometry errors, connected to a module that builds a map of free-space spheres and obstacle points directly out of the sparse visual points.
  Our approach consists of two main parts -- first, we track visual keypoints and estimate their inverse-depth, even with noisy odometry.
  Then, from thus obtained sparse 3D point clouds, we estimate visible free space by a bounding mesh, and incrementally fill free space with a graph of intersecting spheres, which are used for rapid safety-aware path planning.
  % Our method consists of 
  % We then show how such map can be used for real-time planning and exploration on-board resource-constrained robots in the real world and simulation.
  % We open-source the code as a ROS module, which constructs an occupancy map from RGB and odometry data suitable for path planning.
  We demonstrate how our method can be used to achieve, to the best of our knowledge, the first ever autonomous UAV exploration in unstructured 3D environments, using only a single camera and IMU as its sensors.
  % Lastly, we demonstrate how our method can be used to enable a UAV to autonomously explore unstructured, 3D environments.
  % This is, to the best of our knowledge, the first existing UAV system that can perform 3D exploration using only a single camera and IMU.
  % We open-source the code for the 3 separable modules -- mapping, inverse-depth estimation and exploration.-- mapping, inverse-depth estimation and exploration.
  We open-source the code for our approach to provide occupancy mapping and navigation to any UAV with a single camera and any odometry.

Code--- %\href{
\href{TODO}{TODO}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
% Darpaa \cite{darpa_cerberus_wins}
To actively map and navigate unknown environments, fully autonomous UAVs need to build and continuously update some internal representation of the environment, which allows them to find paths to different goals, to know which areas have or haven't been observed, and where the UAV should move to observe new areas.
% The most commonly used type of such representation is an occupancy grid \cite{occupancy_moravec}.
The vast majority of today's autonomous UAV systems with the greatest capabilities \cite{darpa_cerberus_wins, beneath}, construct an occupancy grid \cite{occupancy_moravec}, commonly the efficient octree-based implementation of OctoMap \cite{octoma]}, and use it as the base representation for planning, exploration or constructing higher-level abstractions \cite{spheremap}.
However, to build an occupancy grid, one needs to provide a dense point-cloud of range measurements, due to the raycasting nature of the updates.
Thus, state-of-the-art exploration and navigation UAV systems have been constrained to expensive, heavy or limited-range sensors such as LiDARs, depth cameras or stereo-camera pairs, which can provide these dense point clouds.

To the best of the authors¿ knowledge, there is currently no known way of building an occupancy grid, or a representation that would offer the same capabilities, on robots equipped with only a single mocoular camera in combination with an inexpensive metric sensor, such as an IMU or a global-positioning system receiver. However, being able to build such a representation that would allow safe and rapid exploration and navigation on-board UAVs with such inexpensive sensors would economically unlock many potential applications, such as swarms of  UAVs for search and rescue missions / (would make autonomous navigation drastically more affordable). In addition, RGB cameras are also useful for object recognition and thus make sense to have on many UAVs already, and when moving, they can also provide rough estimates on very distant objects distance, far beyond the ~50m range of LiDARs, but this advantage has not yet been fully utilized in existing research.
% The vast majority of autonomous navigation and exploration research in the real world, such as in the DARPA SubT Challenge \cite{darpa_cerberus_wins}, has so far been focused on robots equipped with costly sensors that provide dense range data, such as LiDARs \cite{beneath}, depth cameras [TODO] or stereo camera pairs \cite{explor_stereo}.
% Even though single vision sensors are much cheaper, existing approaches for using them for online map building and autonomous navigation on UAVs is still severely limited.
% Most navigation approaches for UAVs with only a single camera and no range sensors are reactive behaviors \cite{avoidance_mono1}, or make strong assumptions about environmental structure \cite{corridors_mono_nav_2009}.
\subsection{Related Works}

\subsection{Contributions}
To move towards enabling vision-based autonomous navigation in unknown, unstructured 3D environments for UAVs, our paper brings the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  \item A novel occupancy mapping method which constructs a sphere-based occupancy representation using only sparse 3D point data from visual sensors and odometry as input -- an iteration of our previous work \cite{spheremap}, which now however does not require the costly construction of an occupancy grid
    \item A monocular-visual inverse-depth estimator which tracks and triangulates visual keypoints, using metrically scaled odometry (visual-inertial, GPS, or others), while being robust to odometry errors by estimating a purely visual translations trajectory and scaling it to match the input odometry.
      % (such as when the odometry is obtained from GPS)
    % \item Experimental verification that the proposed methods are well suited for safety-aware navigation and frontier-based exploration on a UAV equipped with only visual-inertial sensors in unstructured, unknown environments and an open-sourced system for such capabilities.
    \item An approach for monocular-inertial exploration on a UAV using the proposed occupancy mapping method and representation, experimentally verified in unstructured 3D environments on a UAV equipped with only a single camera and IMU as primary sensors, in simulation and in the real world.
    % \item We opensource the code for building the SphereMap, using odometry and a sparse 3D pointcloud (or points from the proposed inverse depth estimator) as input, along with example code for using the new occupancy map for path planning, as a ROS package
\end{enumerate}

\begin{figure}[t]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/explor_forest2.png}
  \centering
  \caption{TEMPORARY IMG - The map resulting from the proposed occupancy mapping method after exploring in a simulated outdoor environment -- green spheres describe free space while red points correspond to triangulated visual keypoints, and green points are the currently visible ones.}
  \label{fig:smap_intro}
\end{figure}

\section{RELATED WORKS}
\subsection{Occupancy Mapping}
For a UAV to be able to find plan paths through a previously unknown environment and not just reactively avoid obstacles, it needs to build a representation that is suitable for such planning.
For 3D environments, Octomap \cite{octomap} is a popular octree-based implementation of an occupancy grid, in which the authors build an occupancy representation at a fixed highest resolution (smallest voxels), but collapse any 8 adjacent voxels with the same value into a single, larger voxel, which can considerably speed up many computations and save memory.
Topological representations and higher-level abstractions, such as Voronoi Graphs TODO-CITE or TODO-CITE-TOPOMAP, can allow faster or safer path planning, but are nearly always built on top of an occupancy grid, sometimes even offline or offboard.

TODO - MENTION VOXBLOX

% The standard approach is to use a sensor which provides a dense depth map, such as a depth camera or a stereo camera pair, or range sensors which provide dense point clouds, such as a LiDAR or time-of-flight camera, and integrate these measurements into an occupancy grid TODO-CITE.
The standard approach for building an occupancy representation is to cast rays through an occupancy grid, using sensors which provide dense 3D point clouds, such as an RGB-D camera, LiDAR, time-of-flight camera, or a stereo camera pair. 
Building an occupancy grid using only a single camera and some form of odometry is, to the author's best knowledge, still mostly unexplored in literature, and our proposed method achieves that.
TODO-REF THE EXISTING BAD ONES!!

% Occupancy grids are very general-purpose for path planning purposes, or extracting higher-level topological information about environmental shape \cite{spheremap} TODO-CITE, but they can be problematic for path planning, and can be computationally expensive to construct if the range measurements are very far, due to the raycasting nature of occupancy updates.

% TODO - jointly represents topology and occupancy?

In our previous work \cite{spheremap}, we demonstrated that representing free space with a graph of intersecting spheres can describe free space more compactly than an occupancy grid, and allow orders of magnitude faster path planning than A* or RRT* on an occupancy grid, in addition to explicitly describing the minimum distance to obstacles at each node.
It can thus be thought to represent both occupancy and topology.
However, in \cite{spheremap}, the SphereMap was built out of an OctoMap, which had to also be constructed during flight, taking major computational resources.

In this paper, we present a method to build the SphereMap occupancy representation \textit{directly} without needing to construct an occupancy grid and to construct it out of \textit{sparse} 3D point data from a monocular camera, 
% thus enabling rapid safety-aware path planning for resource-constrained UAVs without costly range sensors.
Thus, our proposed method enables any UAV equipped with a single camera and some form of odometry (e.g. visual-inertial or GPS) to quickly and safely navigate unknown, unstructured 3D environments, without any costly range sensors.
Our occupancy mapping method can easily use dense point clouds as well, but is able to function with sparse ones, which would be problematic for the traditional raycasting approach of updating occupancy grids.

% Topological representations, which are some sort of abstraction built from an occupancy map, are often used for UAV path planning to assure quick and safe path planning.
% An example is TODO-CITE-VOXGRAPH/SKELETON, in which TODO.

% In our previous work \cite{spheremap}, we built a graph of spheres that describe the free space on top of Octomap \cite{octomap}.
% A graph of intersecting spheres can describe free space more compactly than voxels, while also explicitly storing the distance to obstacles, thus allowing fast and safety-aware planning.

% % In addition, in TODO-CITE-SPHEREMAP, we also segmented the graph of spheres into convex regions and allowed 

% However, in \cite{spheremap}, we built the abstract representation \textit{on top} of Octomap \cite{octomap}, which takes considerable resources to update.
% % In addition, 
% In the approach proposed in this paper, we skip this step entirely and build the occupancy map directly out of sparse point cloud data from visual sensors, thus increasing efficiency and enabling the use of such representation on vision-based systems.

% Compared to the methods above, our method for occupancy mapping does not attempt to create dense map of environmental surfaces, but instead accepts the currently visible sparse point cloud from a visual/visual-inertial SLAM, which is already a necessary module for vision-based UAV systems without global positioning systems, makes a very rough depth approximation as in FLAME TODO-CITE, and estimates the visible free space with sampled spheres.

\subsection{Monocular Inverse Depth Estimation}
The task of estimating the depth, or distance of points in space, using just a single RGB camera is challenging.
Recently, deep learning methods have been researched that can provide metric depth from RGB images only TODO-CITE, but such methods can be problematic to deploy on robotic systems.
The ability to estimate depth up to metric scale is usually dependent on the network's knowledge of the metric sizes of objects or textures, and thus such networks can have problems when deployed in environments they haven't been trained on.
Recently, NERF-based approaches for SLAM have also been proposed, which build a dense map of an environment in real-time.
However, GPUs are required for running any of these efficiently, and they add weight and power consumption constraints, thus severely limiting flight time and adding cost, which is not suitable for low-cost UAV systems.

There are also learning-free methods for recovering depth from RGB images, but they also require the pose of the camera associated with each image as input.
The most notable is FLAME \cite{flame}, in which the authors track a large amount of visual keypoints, estimate their inverse depth using the prior pose knowledge, and interpolate these points into a mesh, which is the output of the module. 
The authors state that this method is highly dependent on the quality of the localization.
Another similar method is \cite{depth_dynamic}, TODO.

Traditional methods of obtaining distance measurements with an RGB camera is using visual SLAM algorithms, such as TODO-CITE.
Using only a single camera though, one can build a map of an environment this way, but with the scale of the map being unmeasurable, and thus the map is unusable for path planning for a robot.
% If a metric sensor, such as an 
Adding a sensor that can recover the metric scale -- such as another RGB camera or an inertial measurement unit (IMU) -- allows one to run visual/visual-inertial SLAM algorithms, and thus obtain a map that is scaled correctly.
However, any map built using visual/visual-inertial SLAM is usually very \textit{sparse}, compared to maps built using dense depth sensors, such as LiDARs or RGB-D cameras, which makes it difficult to use such map for any kind of autonomy.

TODO - mention how pts from visual slam usually bad

An important concept popularized for SLAM is using estimating and representing inverse depth of visual keypoints instead of simply the euclidean distance.
This allows the system to represent and optimize points that are very far away, even points in infinity.
Knowing that there are points \textit{very} far away in some direction is an essential piece of information, as it allows a UAV to know that it is safe to fly in that direction for large distances.
However, implementing such reasoning has not yet been researched much in literature.

Compared to the methods above, our inverse depth measurement module is essentially a simplified version of FLAME \cite{flame} with several changes that make it more suitable for our method of occupancy mapping.
We also offer an alternative usage mode, where the system uses the triangulated points coming directly from the currently deployed visual/visual-inertial SLAM, but as discussed further in TODO-REF, these are usually only very near the robot and do not fully use the long range of visual sensors.
% TODO - rgb to visual sensors, cuz can use greyscale!


\subsection{Vision-Based UAV Autonomy}
As detailed in surveys such as \cite{drones_survey}, constructing reliable vision-based autonomous UAV systems is still a challenge.
While with LiDARs and RGB-D cameras, today's robots are able to explore and build a precise map of environments that span several kilometers \cite{darpa_cerberus_wins} without human intervention, such autonomy has not yet been achieved in vision-based systems. 

% TODO - the paper where they fly indoor with drone, structured af

% TODO - autonomous STEREO exploration

% TODO - some reactive obstacle avoidance

% TODO - dodging of obstacles using depth estim


% The notable achievements of vision-based UAV systems are TODO.
% \cite{nav_and_landing}.
% \cite{corridors_mono_nav_2009}
% UAV exploration using a 3D occupancy grid with range sensors has been widely researched in the past.
% Exploration on a UAV equipped with visual sensors has been achieved only with a stereo camera TODO-CITE-AUTONOMOUS, and in practically the same way as with depth sensors.

% On UAVs equipped with no primary range sensors and only a single camera, not much has yet been achieved in terms of autonomy except for short flights in 

Most of these systems are for reactive behavior or behavior in environments with a specific structure. 
In our paper, we demonstrate that our novel occupancy mapping approach allows a UAV to perform real-time receding-horizon exploration and safety-aware path planning in unstructured environments, which has so far, to the authors' best knowledge, been researched only in robots with distance sensors such as LiDARs and RGB-D cameras.

TODO - REWRITE

\section{Sphere-Based Occupancy Mapping}
The most common way of occupancy mapping today is to use a voxel-based representations, such as OctoMap \cite{octomap}, and then set the voxel occupancy probabilities by raycasting through the map, according to distance measurements from depth sensors.
This is a general-purpose representation, but not much suitable for fast path planning.
We propose to represent free space by spheres and obstacles by points, similarly to our previous work \cite{spheremap}.
In this section, we explain how such map can be built using only sparse 3D point data and how it is useful for UAV autonomy.
% We propose an alternative, more lightweight approach to estimating free space over large distances, by constructing a polyhedron from the measured points and the robot, and inscribing spheres in that polyhedron, as shown in image TODO.
% However, if we get very long-distance measurements in some direction, such raycasting approach might need many iterations to fully fill the free space in distant areas.
% Thus, even though raycasting might need tens of thousands of raycasts 

\subsection{Map Updating}
\subsubsection{From Sparse Visual Keypoints to Depth}
The input of our mapping system is odometry and sparse 3D points, which can be taken directly from visual or visual-inertial odometry, which is already a necessary module for robotic autonomy.
We allow also inputting the inverse-depth covariance of individual points and correspondences of points between frames (so that a single point isn't added to the map twice, but rather just moved), but they are not necessary.

At each update iteration, we interpolate the visible input points with a mesh by projecting it to the camera's image plane, computing their Delaunay triangulation, and creating a temporary 3D mesh using the output of the Delaunay triangulation, as in FLAME \cite{flame}.
We name this mesh as the \textit{obstacle mesh} $F_o$, and we also construct a \textit{visibility mesh} $F_v$ from the camera's focal point and the outer edges of $F_o$.
Together, $F_o$ and $F_v$ form a \textit{bounding mesh} $F_t$, which is watertight and all these meshes are used for updating the map at each iteration, which is further divided into these parts:
\subsubsection{Adding Points and Spheres}
Adding new spheres is quite straightforward -- we sample points inside $F_t$ by raycasting from the camera and sampling at random distances.
A potential new sphere's radius is determined by its distance to $F_t$.
However, as in \cite{spheremap}, we also perform a redundancy check for all potential spheres and add them only if they are not redundant (meaning that they would have a significant overlap with already existing spheres).
As in \cite{spheremap}, we also find nearby spheres and connect them if they intersect.
New obstacle points are added to the map simply if their inverse-depth covariance is low (or not provided) and they are further from other points in the map by some predefined minimal distance, which specifies the resulting map surface detail.
\subsubsection{Updating Existing Points and Spheres}
Updating existing spheres and points is slightly more complex.
An old sphere might lie only partially in $F_t$, but that does not mean we should reduce its radius.
Thus, a sphere that intersects with $F_t$ or lies inside it has its radius updated to
\begin{equation}
  r_{new} = \min \left( \max \left( r_{prev}, d(x_s, F_{fov}) \right)
  , d(x_s, F_{obs}) \right),
\end{equation}
where TODO.

We give priority to the information provided by the meshes from the current frame, and so if any point lies deep enough by some constant in the currently observed free space described by $F_t$, it is deleted from the map.
% Additionally, if point correspondences 

\subsection{Rapid Safety-Aware Path Planning}
Another benefit of using spheres for free-space description is that by sampling a point inside any sphere, we immediately get a lower bound on the distance to obstalces, whereas this is not immediately available in a voxel-based representation.
Thus, it is possible to use the SphereMap for sampling-based planning such as with RRT*.
But because we also build a graph of intersecting spheres, our representation can be used for search-based planning with spheres as nodes. 
The path planning can thus optimize not only path length, but also distance to obstacles, while also being several orders of magnitude faster due to the sparsity of spheres in wide areas, which we have already examined in our previous work \cite{spheremap}.

The SphereMap has potential for agile fight as well, since it is possible to bias the sphere sampling to sample along/near the planned trajectory of a UAV, and thus rapidly build an occupancy map primarily in the flight direction.
Testing this against random sampling is outside the scope of this paper.

A potential problem is that in the case of a narrow field-of-view, all nearby spheres can be too small for safe navigation at the beginning of flight.
For this reason, we specify a clearing distance $d$, so that the path planning can move through unsafe space up to $d$ away from the UAV, but after reaching safe space, cannot return to unsafe space.
Thanks to explicitly penalizing distance to obstacles, TODO

\subsection{Submap Separation and Close-Measurement Trusting}
By using very distant points in the occupancy mapping, the errors in occupancy can be high over long distances.
But instead of discarding such noisy information, we present a way of using it, and that is -- by separating the map into submaps, and for any point in space always trusting a closer measurement than a farther one.
We implement the second feature by remembering the observation distance of any sphere or point in the map, and only allow updating the sphere if it is observed from a closer distance.
This leads to the effect that if we first estimate the position of a wall and free space in front of it wrongly, but get closer and thus get better measurements, the map will be updated, but if the UAV gets further than 1.2x the closest measurement distance and observes the wall again, the map will not be changed, as shown in image TODO.
% TODO - ???

% \subsection{Runtime Analysis and }
\section{Monocular Visual Keypoint Inverse-Depth Estimation with Noisy Odometry}

\begin{figure}[!htb]
  % \includegraphics[width=8.5cm]{fig/fire.png}
    \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.2\height} {0.1\width} {0.1\height}}, clip=true]{fig/fire.png}
  \centering
  \caption{TEMPORARY IMG - Illustration of the function of the tracking and depth estimation module - TODO}
  \label{fig:fire}
\end{figure}

Ideally, the input of our occupancy mapping method can be the triangulated visual keypoints from some visual-inertial SLAM.
Unfortunately, to obtain those, one would need to make modifications into the implementation for any SLAM currently deployed.
In addition, when for example switching from visual to GNSS-based odometry in outdoor environments, the points from SLAM would become unavailable.

For these reasons, we also present a new module that tracks visual keypoints and estimates their inverse depth, using only monocular camera data and \textit{any} form of odometry, even with noise.
Our method is heavily inspired by FLAME \cite{flame}, as in our method we also estimate the inverse-depth of visually tracked keypoints, and then interpolate the depth between them using Delaunay triangulation in the same way, but our method makes significant improvements for the sake of robustness to odometry errors.

First, FLAME is a keyframeless method, and updates at a fixed rate.
However, when the UAV is stationary, this causes the inverse depth estimates to drift and become unusable.
Thus, our method only updates the keyframes' inverse depth estimates when enough translational motion has been made, according to the odometry.
Because we are also not trying to estimate a precise mesh, we do not perform the variational mesh smoothing as in FLAME, thus freeing up computational resources.

% Second, the authors of FLAME state that their method is sensitive to odometry errors.
% As that can happen quite often in real-world deployment, we address this issue by running our inverse-depth estimation in a scale-less manner, and then scaling the estimated trajectory and depth estimates according to the odometry up to some amount of keyframes in the past.

% TODO - better explain!!

% Our proposed occupancy mapping method takes as input triangulated visual keypoints.
% Our proposed occupancy mapping method takes as input triangulated visual keypoints.
% Most visual SLAM algorithms only use points close to the robot and optimize the points' 3D positions instead of an inverse-depth representation, which can be very useful for estimating occupancy.
% Without information about faraway points, occupancy mapping can be slow.
% % (TODO-CHECK)

% FLAME \cite{flame} is an available robotics module for estimating inverse depth using image data and 3D poses of the images, but as the authors state,
% % (TODO-CHECK)
% the module is highly dependent on correct pose inputs, and the method also has an issue that if there is little motion, the inverse depth is still updated, which can cause the estimated depth image to become corrupted after several seconds.
% For these reasons, and to focus more on occupancy mapping rather than precise mesh estimation as in FLAME, we propose and opensource a simplified version of the FLAME algorithm.
% Our module tracks visual keypoints and estimates their inverse depth in a bayesian manner, identically as in FLAME, but these updates are done only on keyframes after some pre-defined distance is traveled since the last keyframe.
% All previously triangulated points are tracked, but their distance measurement is updated only when the points have enough parallax and have been matched by the RANSAC homography estimation between keyframes.

Second, FLAME takes the metric scale from the poses of the two consecutive images, but under noisy odometry, this can cause significant problems.
Our approach instead does very simple purely visual SLAM over several keyframes, by simply aggregating the pose estimates between individual keyframes (as long as some keypoints are tracked and triangulated), thus providing an unscaled robocentric trajectory estimate.
This trajectory of $N$ keyframes is then scaled so that the distance of the first and last tracked keyframe is the same as the odometry distance between the keyframes, as shown in TODO-FIG.
% TODO - explain better
This way, even if the odometry is locally noisy or imprecise, the scale estimate can be more robust over larger distances.
The inverse depth estimates are added to the points using this scale.
% This module can then be used to enrich the short-distance visual points from visual-inertial SLAM to estimate free space further into the distance (as we demonstrate in TODO-REF), but it can also allow \textit{any} system with a camera and even noisy odometry to create our proposed occupancy map and allow it to autonomously navigate and explore. 

% \section{Monocular-Inertial Exploration Using the Sphere-Based Occupancy Map}
\section{Exploration Using the Sphere-Based Occupancy Map and Inverse-Depth Estimator}
In this section, we detail how our proposed occupancy mapping method and representation are highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a single camera.

In our approach, adapt the simple receding-horizon next-best-view frontier-based exploration.
In other words, we search for reachable viewpoints, which should uncover some frontiers (updated continually in the occupancy mapping), assign values to them according to the length and safety (computed as in \cite{spheremap}) of the path leading to them, and then choose the best one.

This is a fairly standard approach, with several important changes for monocular systems in outdoor environments.
First, for depth sensors, it is enough to point the robot's sensors in the direction of the frontier, and it will usually uncover some free space.
The case of a frontier not being uncovered -- due to e.g. nonreflective or distant surfaces -- is usually neglected in exploration approaches. 
However, when we use visual keypoints as input for the depth estimation, such a situation can occur quite often, for example when trying to uncover textureless walls.

For this reason, we keep a history of the number of times a frontier was "observed", by which we mean that the robot has chosen to view it and went to the corresponding viewpoint.
We then simply discard any frontiers that have been viewed more than 2 times from any future selection, as shown in TODO-FIG.

Second, triangulating points requires the robot to move, so we cannot simply point the robot's camera in the direction of the frontiers.
We solve this by adding a short, several meter translation-only trajectory upon reaching any viewpoint, so that any potential points in the direction of the frontiers are triangulated and it can be uncovered.

Thanks to using a sphere-based representation, we can find paths not only according to  TODO-PRAISE-SPHERES.
% However, while using visual keypoints as input, there can 

\section{Experiments}
% \subsection{Effect of Distant Points on Map Quality}
\subsection{Large-Scale Exploration in Simulation}
In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.

TODO - RUN, EVALUATE, ADD TABLE AND IMGS

TODO - RUNTIME ANALYSIS NOTE
% In this experiment, we demonstrate the performance of the proposed occupancy mapping while using distant points with high depth covariance, against using closer-range points, which are usually available from most visual SLAM algorithms.
% As you can see in TODO, by using the distant points, the UAV is able to quickly construct an estimate of 

\subsection{Real-World UAV Monocular-Inertial Exploration}
In this experiment we demonstrate that the proposed method of occupancy mapping using only a single monocular RGB camera and IMU is useful for path and exploration planning on a real-world UAV platform in a mixed indoor-outdoor warehouse setting.

The UAV is TODO-HARDWARE \cite{mrs_uav_robust_system}.

The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and occupancy mapper modules.
For demonstration purposes, we developed next-best-view frontier-based exploration using our proposed occupancy representation.
The exploration module simply samples potential reachable viewpoints by RRT in its nearby space at a fixed rate and stores those.
Upon reaching a viewpoint, the UAV selects a new one, by a combination of distance, path safety (same as in our previous work TODO-CITE), and the amount of visible frontiers.
We also take into account only frontier points that lie close to some obstacle points -- essentially points that lie close to some surfaces, so the UAV doesn't fly off exploring into the sky.
Additionally, we only allow path planning where some map points would be visible, as discussed in TODO-REF, to not lose visual odometry tracking.

As can be seen from TODO-REF, and multimedia materials supporting the experiment, the occupancy representation is quite suited for such a task of vision-based autonomy.
The UAV explored TODO.
It is currently not possible to compare TODO.
% \subsection{Comparison }
% \subsection{Runtime Analysis}
% Here we analyze the efficiency of the individual parts of the occupancy mapping, method and show thaa

\begin{figure}[!htb]
  \includegraphics[width=8.5cm]{fig/exper1.png}
  \includegraphics[width=8.5cm]{fig/exper4.png}
  % \includegraphics[width=8.5cm]{fig/exper3.png}
  % \includegraphics[width=8.5cm]{fig/exper4.png}
  \centering
  \caption{TEMPORARY IMG - Exploration experiment visualization}
  \label{fig:exper_real}
\end{figure}

\subsection{Runtime analysis}
Here we analyze the runtimes of this method, and TODO.

TODO - tabulka


\section{CONCLUSION}
In this paper, we presented a novel sphere-based occupancy mapping method 
% together with a robust monocular inverse-depth estimator,
using sparse point cloud data from monocular visual sensors,
and demonstrated how it can enable any UAV equipped with a single camera and any form of odometry to quickly and safely navigate and explore unknown, unstructured 3D environments.
We have proven through extensive testing in simulation and in the real world, that robust autonomous navigation and exploration on UAVs without expensive, traditionally used range sensors, is possible.

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
% \bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
% \bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
% \bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
% \bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
