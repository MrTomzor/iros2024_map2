%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\vk}[1]{#1}
\newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
Monocular-Inertial Sphere-Based Occupancy Mapping and Safety-Aware Exploration on a UAV}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  % We present a new method of estimating inverse depth and using such estimates to build an occupancy map composed of spheres and points from sparse point cloud data obtained from a single RGB camera, for example by visual-inertial SLAM.
  We present a method of building an occupancy representation using only a monocular camera and odometry data in unknown, unstructured 3D environments, and demonstrate the utility of this representation for rapid safety-aware planning for UAVs in simulation and in the real world.
  Our method consists of a visual keypoint tracking and inverse-depth estimation module robust to odometry errors, connected to a module that builds a map of free-space spheres and obstacle points directly out of the sparse visual points.
  % Our method consists of 
  % We then show how such map can be used for real-time planning and exploration on-board resource-constrained robots in the real world and simulation.
  % We open-source the code as a ROS module, which constructs an occupancy map from RGB and odometry data suitable for path planning.
  This is, to the best of our knowledge, the first existing UAV system that can perform 3D exploration using only a single camera and IMU.
  We open-source the code for the 3 separable modules -- mapping, inverse-depth estimation and exploration.-- mapping, inverse-depth estimation and exploration.

Code--- %\href{
\href{https://github.com/MrTomzor/navigation_unity_testbed}{https://github.com/MrTomzor/navigation\_unity\_testbed}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Autonomous navigation using only a single RGB camera and inertial/encoder sensors is highly desirable due to the low cost of such sensors.
However, the majority of robotics research on autonomous mapping and autonomous navigation today is developed for robots equipped with strong depth sensors such as RGB-D cameras or LiDARs \cite{darpa_cerberus_wins}.

Darpaa \cite{darpa_cerberus_wins}
beep boop

However, the vast majority of autonomous navigation in the real world has been deployed on UAVs equipped with powerful depth sensors such as LiDARs and RGB-D cameras, which drastically increase the cost of such systems compared to simple cameras.
In addition, RGB-D cameras usually have range up to TODO m and LiDARs up to TODO m, whereas with vision sensors, one can potentially see that some points lie extremely far away, and thus infer that there is vast free space up to those obstacles, such as when flying on a field and observing a tree line in the distance.
% Enabling safe vision-based autonomous navigation for UAVs is especially exciting because such methods are critical for real-world large-scale deployment of UAVs, especially in search and rescue operations.

Even though vision sensors are much cheaper and potentially have higher range, their use for robotic autonomous map building and navigation is still severely limited.
Most such approaches in vision-based robotics are reactive behaviors TODO-CITE, or make strong assumptions about environmental structure TODO-CITE.

To move towards the reality of vision-based autonomous navigation in unknown, unstructured 3D environments for UAVs, our paper offers the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
    \item A novel occupancy mapping module which constructs a sphere-based occupancy representation using only sparse 3D point data from visual sensors and odometry as input -- an iteration of our previous work TODO-CITE, which now however does not require the costly construction of an occupancy grid
    \item A monocular-visual inverse-depth estimator module which tracks and triangulates visual keypoints, using metrically scaled odometry (visual-inertial, GPS, or others), while being robust to odometry errors by estimating a purely visual translations trajectory and scaling it to match the odometry.
      % (such as when the odometry is obtained from GPS)
    \item Experimental verification that the proposed methods are well suited for safety-aware navigation and frontier-based exploration on a UAV equipped with only visual-inertial sensors in unstructured, unknown environments and an open-sourced system for such capabilities.
    % \item We opensource the code for building the SphereMap, using odometry and a sparse 3D pointcloud (or points from the proposed inverse depth estimator) as input, along with example code for using the new occupancy map for path planning, as a ROS package
\end{enumerate}

\section{RELATED WORKS}
\subsection{Monocular Inverse Depth Estimation}
The task of estimating the depth, or distance of points in space, using just a single RGB camera is challenging.
Recently, deep learning methods have been researched that can provide metric depth from RGB images only TODO-CITE, but such methods can be problematic to deploy on robotic systems.
The ability to estimate depth up to metric scale is usually dependent on the network's knowledge of the metric sizes of objects or textures, and thus such networks can have problems when deployed in environments they haven't been trained on.
Recently, NERF-based approaches for SLAM have also been proposed, which build a dense map of an environment in real-time.
However, GPUs are required for running any of these efficiently, and they add weight and power consumption constraints, thus severely limiting flight time and adding cost, which is not suitable for low-cost UAV systems.

There are also learning-free methods for recovering depth from RGB images, but they also require the pose of the camera associated with each image as input.
The most notable is FLAME \cite{flame}, in which the authors track a large amount of visual keypoints, estimate their inverse depth using the prior pose knowledge, and interpolate these points into a mesh, which is the output of the module. 
The authors state that this method is highly dependent on the quality of the localization.
Another similar method is \cite{depth_dynamic}, TODO.

Traditional methods of obtaining distance measurements with an RGB camera is using visual SLAM algorithms, such as TODO-CITE.
Using only a single camera though, one can build a map of an environment this way, but with the scale of the map being unmeasurable, and thus the map is unusable for path planning for a robot.
% If a metric sensor, such as an 
Adding a sensor that can recover the metric scale -- such as another RGB camera or an inertial measurement unit (IMU) -- allows one to run visual/visual-inertial SLAM algorithms, and thus obtain a map that is scaled correctly.
However, any map built using visual/visual-inertial SLAM is usually very \textit{sparse}, compared to maps built using dense depth sensors, such as LiDARs or RGB-D cameras, which makes it difficult to use such map for any kind of autonomy.

TODO - mention how pts from visual slam usually bad

An important concept popularized for SLAM is using estimating and representing inverse depth of visual keypoints instead of simply the euclidean distance.
This allows the system to represent and optimize points that are very far away, even points in infinity.
Knowing that there are points \textit{very} far away in some direction is an essential piece of information, as it allows a UAV to know that it is safe to fly in that direction for large distances.
However, implementing such reasoning has not yet been researched much in literature.

Compared to the methods above, our inverse depth measurement module is essentially a simplified version of FLAME TODO-CITE with several changes that make it more suitable for our method of occupancy mapping.
We also offer an alternative usage mode, where the system uses the triangulated points coming directly from the currently deployed visual/visual-inertial SLAM, but as discussed further in TODO-REF, these are usually only very near the robot and do not fully use the long range of visual sensors.
% TODO - rgb to visual sensors, cuz can use greyscale!

\subsection{Occupancy and Topology Representations}
For a UAV to be able to plan safe paths in an environment in a non-reactive way, it needs a representation that is suitable for such planning.

The standard occupancy representation in robotics is an occupancy grid, which can be in 2D or 3D, depending on the robot and task.
The most commonly used representation for any kind of robotic autonomy in 3D is Octomap \cite{octomap}, in which the authors build an occupancy representation at a fixed highest resolution (smallest voxels), but collapse any 8 adjacent voxels with the same value into a single, larger voxel, which can considerably speed up many computations.
Similar methods exist with slight variations, such as TODO.

Topological representations, which are some sort of abstraction built from an occupancy map, are often used for UAV path planning to assure quick and safe path planning.
An example is TODO-CITE-VOXGRAPH/SKELETON, in which TODO.

In our previous work \cite{spheremap}, we built a graph of spheres that describe the free space on top of Octomap \cite{octomap}.
A graph of intersecting spheres can describe free space more compactly than voxels, while also explicitly storing the distance to obstacles, thus allowing fast and safety-aware planning.
% In addition, in TODO-CITE-SPHEREMAP, we also segmented the graph of spheres into convex regions and allowed 

However, in \cite{spheremap}, we built the abstract representation \textit{on top} of Octomap \cite{octomap}, which takes considerable resources to update.
% In addition, 
In the approach proposed in this paper, we skip this step entirely and build the occupancy map directly out of sparse point cloud data from visual sensors, thus increasing efficiency and enabling the use of such representation on vision-based systems.

% Compared to the methods above, our method for occupancy mapping does not attempt to create dense map of environmental surfaces, but instead accepts the currently visible sparse point cloud from a visual/visual-inertial SLAM, which is already a necessary module for vision-based UAV systems without global positioning systems, makes a very rough depth approximation as in FLAME TODO-CITE, and estimates the visible free space with sampled spheres.

\subsection{Vision-Based UAV Autonomy}
As detailed in surveys such as \cite{drones_survey}, constructing reliable vision-based autonomous UAV systems is still a challenge.
While with LiDARs and RGB-D cameras, today's robots are able to explore and build a precise map of environments that span several kilometers \cite{darpa_cerberus_wins} without human intervention, such autonomy has not yet been achieved in vision-based systems. 

The notable achievements of vision-based UAV systems are TODO.
\cite{nav_and_landing}.
\cite{corridors_mono_nav_2009}

Most of these systems are for reactive behavior or behavior in environments with a specific structure. 
In our paper, we demonstrate that our novel occupancy mapping approach allows a UAV to perform real-time receding-horizon exploration and safety-aware path planning in unstructured environments, which has so far, to the authors' best knowledge, been researched only in robots with distance sensors such as LiDARs and RGB-D cameras.

% TODO - the paper where they fly indoor with drone

% TODO - dodging of obstacles using depth estim

% TODO - our method allows PLANNING over larger distances

\section{Sphere-Based Occupancy Map}
The most common way of occupancy mapping today is to use a voxel-based representations, such as OctoMap TODO-CITE, and then set the voxel occupancy probabilities by raycasting through the map, according to distance measurements from depth sensors.
This is a general-purpose representation, but not much suitable for fast path planning.
We propose to represent free space by spheres and obstacles by points, similarly to our previous work TODO-CITE-SPHEREMAP.
In this section, we explain how such map can be built using only sparse 3D point data and how it is useful for UAV autonomy.
% We propose an alternative, more lightweight approach to estimating free space over large distances, by constructing a polyhedron from the measured points and the robot, and inscribing spheres in that polyhedron, as shown in image TODO.
% However, if we get very long-distance measurements in some direction, such raycasting approach might need many iterations to fully fill the free space in distant areas.
% Thus, even though raycasting might need tens of thousands of raycasts 

\subsection{Map Updating}
\subsubsection{From Sparse Visual Keypoints to Depth}
The input of our mapping system is odometry and sparse 3D points, which can be taken directly from visual or visual-inertial odometry, which is already a necessary module for robotic autonomy.
We allow also inputting the inverse-depth covariance of individual points and correspondences of points between frames (so that a single point isn't added to the map twice, but rather just moved), but they are not necessary.

At each update iteration, we interpolate the visible input points with a mesh by projecting it to the camera's image plane, computing their Delaunay triangulation, and creating a temporary 3D mesh using the output of the Delaunay triangulation, as in FLAME \cite{flame}.
We name this mesh as the \textit{obstacle mesh} $F_o$, and we also construct a \textit{visibility mesh} $F_v$ from the camera's focal point and the outer edges of $F_o$.
Together, $F_o$ and $F_v$ form a \textit{bounding mesh} $F_t$, which is watertight and all these meshes are used for updating the map at each iteration, which is further divided into these parts:
\subsubsection{Adding Points and Spheres}
Adding new spheres is quite straightforward -- we sample points inside $F_t$ by raycasting from the camera and sampling at random distances.
A potential new sphere's radius is determined by its distance to $F_t$.
However, as in TODO-CITE-SPHEREMAP, we also perform a redundancy check for all potential spheres and add them only if they are not redundant (meaning that they would have a significant overlap with already existing spheres).
As in TODO-CITE-SPHEREMAP, we also find nearby spheres and connect them if they intersect.
New obstacle points are added to the map simply if their inverse-depth covariance is low (or not provided) and they are further from other points in the map by some predefined minimal distance, which specifies the resulting map surface detail.
\subsubsection{Updating Existing Points and Spheres}
Updating existing spheres and points is slightly more complex.
An old sphere might lie only partially in $F_t$, but that does not mean we should reduce its radius.
Thus, a sphere that intersects with $F_t$ or lies inside it has its radius updated to
\begin{equation}
  r_{new} = \min \left( \max \left( r_{prev}, d(x_s, F_{fov}) \right)
  , d(x_s, F_{obs}) \right),
\end{equation}
where TODO.

We give priority to the information provided by the meshes from the current frame, and so if any point lies deep enough by some constant in the currently observed free space described by $F_t$, it is deleted from the map.
% Additionally, if point correspondences 

\subsection{Rapid Safety-Aware Path Planning}
Another benefit of using spheres for free-space description is that by sampling a point inside any sphere, we immediately get a lower bound on the distance to obstalces, whereas this is not immediately available in a voxel-based representation.
Thus, it is possible to use the SphereMap for sampling-based planning such as with RRT*.
But because we also build a graph of intersecting spheres, our representation can be used for search-based planning with spheres as nodes. 
The path planning can thus optimize not only path length, but also distance to obstacles, while also being several orders of magnitude faster due to the sparsity of spheres in wide areas, which we have already examined in our previous work CITE-SPHEREMAP.

The SphereMap has potential for agile fight as well, since it is possible to bias the sphere sampling to sample along/near the planned trajectory of a UAV, and thus rapidly build an occupancy map primarily in the flight direction.
Testing this against random sampling is outside the scope of this paper.

A potential problem is that in the case of a narrow field-of-view, all nearby spheres can be too small for safe navigation at the beginning of flight.
For this reason, we specify a clearing distance $d$, so that the path planning can move through unsafe space up to $d$ away from the UAV, but after reaching safe space, cannot return to unsafe space.
Thanks to explicitly penalizing distance to obstacles, TODO

\subsection{Submap Separation and Close-Measurement Trusting}
By using very distant points in the occupancy mapping, the errors in occupancy can be high over long distances.
But instead of discarding such noisy information, we present a way of using it, and that is -- by separating the map into submaps, and for any point in space always trusting a closer measurement than a farther one.
We implement the second feature by remembering the observation distance of any sphere or point in the map, and only allow updating the sphere if it is observed from a closer distance.
This leads to the effect that if we first estimate the position of a wall and free space in front of it wrongly, but get closer and thus get better measurements, the map will be updated, but if the UAV gets further than 1.2x the closest measurement distance and observes the wall again, the map will not be changed, as shown in image TODO.
% TODO - ???

% \subsection{Runtime Analysis and }
\section{Simplified Visual Inverse-Depth Estimator}
Our proposed occupancy mapping method takes as input triangulated visual keypoints.
Most visual SLAM algorithms only use points close to the robot and optimize the points' 3D positions instead of an inverse-depth representation, which can be very useful for estimating occupancy.
Without information about faraway points, occupancy mapping can be slow.
% (TODO-CHECK)

FLAME TODO-CITE is an available robotics module for estimating inverse depth using image data and 3D poses of the images, but as the authors state,
% (TODO-CHECK)
the module is highly dependent on correct pose inputs, and the method also has an issue that if there is little motion, the inverse depth is still updated, which can cause the estimated depth image to become corrupted after several seconds.

For these reasons, and to focus more on occupancy mapping rather than precise mesh estimation as in FLAME, we propose and opensource a simplified version of the FLAME algorithm.
Our module tracks visual keypoints and estimates their inverse depth in a bayesian manner, identically as in FLAME, but these updates are done only on keyframes after some pre-defined distance is traveled since the last keyframe.
All previously triangulated points are tracked, but their distance measurement is updated only when the points have enough parallax and have been matched by the RANSAC homography estimation between keyframes.

Furthermore, FLAME takes the metric scale from the poses of the two consecutive images, but under noisy odometry, this can cause significant problems.
Our approach instead does very simple purely visual SLAM over several keyframes by simply aggregating the pose estimates between individual keyframes, thus providing an unscaled robocentric trajectory estimate.
This trajectory of $N$ keyframes is then scaled so that the distance of the first and last tracked keyframe is the same as the odometry distance between the keyframes.
% TODO - explain better
This way, even if the odometry is locally noisy or imprecise, the scale estimate can be more robust over larger distances.
The inverse depth estimates are added to the points using this scale.

This module can then be used to enrich the short-distance visual points from visual-inertial SLAM to estimate free space further into the distance (as we demonstrate in TODO-REF), but it can also allow \textit{any} system with a camera and even noisy odometry to create our proposed occupancy map and allow it to autonomously navigate and explore. 

\section{Experiments}
\subsection{Effect of Distant Points on Map Quality}
In this experiment, we demonstrate the performance of the proposed occupancy mapping while using distant points with high depth covariance, against using closer-range points, which are usually available from most visual SLAM algorithms.
As you can see in TODO, by using the distant points, the UAV is able to quickly construct an estimate of 

\subsection{Real-World UAV Monocular-Inertial Exploration}
In this experiment we demonstrate that the proposed method of occupancy mapping using only a single monocular RGB camera and IMU is useful for path and exploration planning on a real-world UAV platform in a mixed indoor-outdoor warehouse setting.

The UAV is TODO-HARDWARE \cite{mrs_uav_robust_system}.

The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and occupancy mapper modules.
For demonstration purposes, we developed next-best-view frontier-based exploration using our proposed occupancy representation.
The exploration module simply samples potential reachable viewpoints by RRT in its nearby space at a fixed rate and stores those.
Upon reaching a viewpoint, the UAV selects a new one, by a combination of distance, path safety (same as in our previous work TODO-CITE), and the amount of visible frontiers.
We also take into account only frontier points that lie close to some obstacle points -- essentially points that lie close to some surfaces, so the UAV doesn't fly off exploring into the sky.
Additionally, we only allow path planning where some map points would be visible, as discussed in TODO-REF, to not lose visual odometry tracking.

As can be seen from TODO-REF, and multimedia materials supporting the experiment, the occupancy representation is quite suited for such a task of vision-based autonomy.
The UAV explored TODO.
It is currently not possible to compare TODO.
% \subsection{Comparison }

\subsection{Runtime Analysis and Octomap Comparison}

\section{CONCLUSION}
aaaa
% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
% \bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
% \bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
% \bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
% \bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
