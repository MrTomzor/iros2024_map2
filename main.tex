%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\todo}[1]{{\hypersetup{allcolors=red}{\color{red} {#1}}}}
\newcommand{\todocaption}[1]{{\color{red} {#1}}}

% \newcommand{\vk}[1]{#1}
% \newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
% Monocular Sphere-Based Occupancy Mapping and Large-Scale Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Sparse Monocular SLAM for Robust Large-Scale 3D Exploration on Inexpensive UAVs}

% FINAL TWO
% Sphere-Based Mapping using Sparse Monocular SLAM Points for Robust Large-Scale 3D Exploration on Inexpensive UAVs}
Monocular-Inertial Exploration of Large-Scale Outdoor Environments using Sparse SLAM Keypoints on an Inexpensive UAV}

% % PolySphereMap - Occupancy Mapping using Sparse Monocular SLAM for Large-Scale 3D Exploration on Inexpensive UAVs}
% PolySphereMap - Mapping and Exploring Large-Scale 3D Environments using a Monocular Camera on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  We present a novel approach to describing 3D space, using only a monocular camera and pose estimates.
  Our main contribution is representing an environment by a graph of intersecting free-space spheres and a set of triangulated visual keypoints with remembered measurement uncertainty, which is fundamentally different from grid-based occupancy maps. 
  We present a method of building this representation in real-time, using only sparse 3D point data obtained from monocular SLAM by inscribing spheres inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  We further describe an exploration system that uses the advantages brought by the novel representation to overcome some of the challenges of monocular vision-based UAV autonomy and demonstrate
large-scale autonomous exploration in unstructured indoor/outdoor 3D environments, using only a single monocular camera and an IMU onboard an inexpensive UAV in simulation and in the real world.
  We open-source the code for our methods to provide 3D mapping and exploration capabilities to any UAV with a monocular camera and available pose estimation.

Code--- %\href{
\href{TODO}{TODO}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
The autonomous UAV systems with the greatest capabilities documented in recent literature \cite{darpa_cerberus_wins, beneath} usually construct an occupancy grid \cite{occupancy_moravec}, commonly the efficient octree-based implementation of OctoMap \cite{octomap}, and use it as the base representation for planning, exploration or constructing higher-level spatial abstractions \cite{spheremap}.
However, building a detailed occupancy grid requires dense pointclouds of range measurements due to the nature of occupancy grid updates --- which is to cast rays towards the measurement points and update the occupancy of traversed cells.
% TODO - check if all
Thus, state-of-the-art exploration and navigation UAV systems have been constrained to expensive, heavy or limited-range sensors such as LiDARs, depth cameras or stereo-camera pairs, which can provide these dense point clouds.

% To the best of the authors' knowledge, \todo{there is currently no known way of building a 3D occupancy grid (or a representation that would offer the same capabilities) on robots equipped with only a single monocular camera in combination with inexpensive sensors that can recover the metric scale, such as an IMU or a global-positioning system.}
Building an occupancy representation for exploration using only a monocular camera for depth sensing is challenging for many reasons, and has so far been demonstrated only in small-scale indoor environments.
However, being able to build a representation that would allow safe and rapid exploration and navigation on-board UAVs with such inexpensive sensors would unlock many potential applications, such as swarms of disposable UAVs for search and rescue missions.
In addition, cameras are useful for object recognition and other types of vision-based perception beneficial for autonomous UAVs, and thus 
% it makes sense to have them equipped on many UAV applications already.
they are already equipped on UAVs in most real-world applications.
Furthermore, a single moving camera can provide rough distance estimates on very distant objects when using the inverse depth parametrization \cite{inverse_depth}, far beyond the approx. 50m range of LiDARs.

\begin{figure}[!t]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  % \includegraphics[width=8.5cm]{fig/explor_forest2.png}
  % \includegraphics[width=8.5cm]{fig/intro_poly.png}
    % \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.05\height} {0.05\width} {0.1\height}}, clip=true]{fig/intro_poly.png}
    \adjincludegraphics[width=8.5cm, trim={{0.0\width} {0.00\height} {0.00\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}
  \centering
  \caption{Visualization of the proposed map representation and how it is updated. The green polyhedron is an estimate of currently visible free space and is formed by the visible triangulated keypoints and the camera's position (axes). The mapped spheres (green), obstacle points (red) and frontier points (purple) are updated by the polyhedron and visible keypoints, as described in Sec. \ref{sec:map_building}}
  \label{fig:smap_intro}
\end{figure}

In this paper, we present a novel approach to 3D mapping that takes in a sparse pointcloud of visual keypoints tracked by monocular SLAM (already a necessary part of many vision-based UAV systems), and instead of traditionally casting rays through a grid and updating its cells, we construct a polyhedron of approximate visible free space at each frame, inscribe spheres of free space inside that polyhedron, and connect them together to form a graph of intersecting spheres that allows for fast and safety-aware planning.
We further show how our mapping method enables autonomous exploration and navigation in large-scale outdoor environments on a UAV using only a single camera and IMU as its sensors, which, to the authors' best knowledge, has not yet been achieved in literature.
% We open-source the code so that anyone with a UAV equipped with a single camera and a source of odometry can easily enable exploration and rapid, safety-aware navigation.
% We open-source the code make a detailed analysis .
% , and unlocks a wide variety potential low-cost UAV applications.

% The vast majority of autonomous navigation and exploration research in the real world, such as in the DARPA SubT Challenge \cite{darpa_cerberus_wins}, has so far been focused on robots equipped with costly sensors that provide dense range data, such as LiDARs \cite{beneath}, depth cameras [TODO] or stereo camera pairs \cite{explor_stereo}.
% Even though single vision sensors are much cheaper, existing approaches for using them for online map building and autonomous navigation on UAVs is still severely limited.
% Most navigation approaches for UAVs with only a single camera and no range sensors are reactive behaviors \cite{avoidance_mono1}, or make strong assumptions about environmental structure \cite{corridors_mono_nav_2009}.

\subsection{Related Works -- Spatial Representations}
% The environment representation similar to our previous work \cite{spheremap} -- in \cite{spheremap}, we also built a graph of intersecting free-space spheres, but 
In robotic exploration, the vast majority of approaches use a grid-based representation of occupied, unknown and free space, either in the form of an euclidean signed distance field (ESDF) \cite{voxblox} or an occupancy grid \cite{occupancy_moravec, octomap}.
A grid-based representation, while being the basis for a substantial amount of robotic path planning and higher-level spatial abstraction methods \cite{topomap, skeletons, spheremap}, has several drawbacks for usage on UAVs without dense range sensors in varying-scale environments.

Firstly, both ESDFs and occupancy grids are most commonly updated by raycasting towards measured obstacle points.
% This becomes problematic when the available measured points are very sparse, such as points tracked by sparse monocular SLAM.
Some works overcome this limitation to some degree -- 
in \cite{from_monoslam_to_explo}, the authors take points from semi-dense monocular SLAM and raycast towards them to build an OctoMap \cite{octomap} occupancy grid, but only demonstrate their local exploration approach built on top of this mapping pipeline to work inside a single room.

Compared to \cite{from_monoslam_to_explo}, the authors of \cite{los_maps} use sparse monocular SLAM, they also cast rays towards the measured points to build an occupancy grid, and show exploration on a UAV across multiple rooms.
They set the occupancy grid cell size to be 0.5m, which is relatively large compared to the UAV size. 
Such rough voxel size cannot represent the distance to obstacles in high resolution, and if the grid is misaligned with the building, narrow passages can be represented as non-traversible.
% and in [ROOM-DENSEDEPTH], the authors essentially perform dense pixel-based estimation with known poses from sparse SLAM, which works in well-illuminated rooms but may have problems in larger-scale or texture-poor environments.
% Second, even though memory-efficient octree-based representations of 3D grids are available, most notably the OctoMap library \cite{octomap}, a fundamental problem of grids is that the smallest-voxel size must be set by the user.
This is the second fundamental problem of grid-based representations -- the smallest-voxel size must be set by the user.
If this is set as too large, narrow passages will not be captured in the map, and if it is too small, the map will need large amounts of raycasts to set all the cells' occupancy values, which is computationally expensive.

% In our previous work \cite{spheremap}, we have shown how a graph of intersecting spheres can be built out of an occupancy grid, and that this representation allows path planning that has explicit information about distance to obstacles, in addition to being orders of magnitude faster than on an occupancy grid.
In our previous work \cite{spheremap}, we have shown that representing free space by a graph of intersecting spheres can efficiently describe both large-space while keeping detailed information in narrow spaces, in additon to allowing orders of magnitude faster path planning than grid-based representations.
% In \cite{spheremap}, we also demonstrated that this representation allows path planning that has explicit information about distance to obstacles, in addition to allowing orders of magnitude faster path planning than on an occupancy grid.
In this paper, we represent free space using spheres as well, but with two major differences: 
% In this paper, we represent the free space using spheres as well, but also add the obstacle points into the map and use them for constraining the sphere radii, but with \todo{two major differences 

\begin{itemize}
    % \item now \textit{we do not need to build the occupancy grid as an intermediate step}, which frees up significant computational resources, and \textit{we construct the map using sparse point-clouds from monocular SLAM}, which are problematic to use for occupancy grids.
  \item In this paper, we build the graph of spheres \textit{directly} out of the sparse 3D pointclouds and pose estimates obtained from monocular SLAM, without needing to build an intermediate occupancy grid before computing the sphere radii as in \cite{spheremap}, which frees up significant computational resources.
% In addition, there is no current documented way to build an occupancy grid using sparse point clouds, although this could also be interesting to research.
% TODO-CHECK
  \item Unlike \cite{spheremap}, which only represents free space, the representation presented in this paper contains obstacle points corresponding to triangulated visual keypoints with remembered measurement distance, which plays an important role when updating the map at different measurement distances (see Sec. \ref{sec:distance-based}). 
        % to overcome the challenge of incorrect measurements at larger distances.
\end{itemize}

\subsection{Related Works -- Vision-Based Exploration}

Vision-based UAV autonomous exploration has so far, by far not reached the levels of autonomy as when using dense depth sensors, and remains a challenge, as documented in recent surveys \cite{drones_survey}, but some progress has been made.
% TODO - mention reactive only
In \cite{explor_stereo}, the authors presented the first ever vision-based UAV system that can explore and build a map (a 3D occupancy grid) on a UAV equipped with stereo cameras and also a supporting downward-facing optical flow sensor for better motion estimation.
The authors used dense pointclouds coming from the stereo cameras for building an OctoMap \cite{octomap} occupancy representation in the same way as with depth data from an RGB-D camera or LiDAR, which allowed them to use similar exploration techniques as on systems with those sensors.

For UAVs equipped with a monocular camera, to the authors' best knowledge, all the existing works have demonstrated autonomous 3D mapping and exploration on a UAV only indoor and at the scale of a single room \cite{from_monoslam_to_explo, cnn_explo_singleroom} or a few rooms \cite{los_maps, simon2023mononav}.
% It can be said that the main challenge here is that the majority of robotics methods is based on first building an occupancy grid using dense data measurements from a depth camera, LiDAR or stereo cameras, but obtaining such data from a monocular camera is difficult.
% The discussed approaches either accept that the data from visual SLAM is very sparse, or they use deep-learning-based depth estimation to obtain dense data.
Monocular SLAM, both sparse and dense, cannot produce measurement points on textureless areas, and the existing approaches either accept that the data from visual SLAM can be very sparse, or they use deep-learning-based depth estimation to obtain dense data.

The authors of \cite{from_monoslam_to_explo, los_maps} argue against using frontier-based methods \cite{paper_frontier_grandpa}, due to the facts that on textureless areas, no points will ever be generated and thus those frontiers would not be uncovered by a monocular camera.
The authors have presented alternative exploration approaches, but have only shown them to work in small-scale environments.
In our exploration approach, thanks to not using raycasting in our proposed mapping pipeline and also blocking explored viewpoints, we show that frontier-based exploration is possible and assures global coverage of the explored environments.

In a different approach, the authors of \cite{simon2023mononav, cnn_explo_singleroom} use deep-learning-based single image depth estimation to obtain dense point clouds from a monocular camera, essentially turning it into a depth camera.
However, using learning-based monocular depth estimation models brings additional problems, most important for fully autonomous UAVs being not providing real-time performance and the problem of domain independence, as discussed in recent surveys \cite{deep_mono_depth_2021, deep_mono_depth_2019}, in addition to the higher cost and weight of a UAV with a GPU.

\todo{TODO - cite corridors!!!}

% In \cite{corridors_mono_nav_2009}, the authors presented a system for monocular UAV navigation and demonstrated autonomous flight and map-building in an office corridor.

Compared to the above mentioned approaches, we demonstrate that our approach achieves exploration and navigation in 3D using a monocular camera and an IMU in outdoor, large-scale environments, with all of the computations running on-board the UAV, without requiring a GPU.

\subsection{Contributions}
To move towards enabling robust vision-based autonomous navigation in unknown, unstructured 3D environments for inexpensive UAVs, our paper brings the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  \item A novel 3D mapping approach, which constructs a sphere-based map using sparse pointclouds obtained from monocular SLAM as the only depth information.
    % TODO - maybe define this as GEOMETRIC-PRIMITIVE-GRAPH MAP???
    \item A system that demonstrates large-scale 3D outdoor exploration on a UAV equipped with only a monocular camera and IMU as its sensors, enabled by the proposed mapping method, 
      experimentally verified in simulation and in the real world.
    \item Open-sourced code for the novel mapping method, along with path-planning and exploration functionalities and example simulation scripts
\end{enumerate}

\section{Sphere-Based Map Built From Sparse Visual SLAM Keypoints}
We propose to represent free space by a graph of intersecting spheres, similarly to our previous work \cite{spheremap}, and obstacles by points corresponding to triangulated visual keypoints from visual SLAM.
In this section, we explain how such map can be built directly from the outputs of visual SLAM, without needing any intermediate occupancy grid computation, and how the sparsity and high depth uncertainty of the input pointclouds is handled. 
\subsection{Obtaining Sparse Pointclouds from a Monocular Camera}
A single map update takes as input 1) a 3D pointcloud corresponding to visual keypoints, potentially sparse, at metric scale, which can be obtained from monocular-inertial SLAM \cite{openvins, orbslam3}, or for example by fusing purely monocular SLAM with a global positioning system, and 2) a pose estimate of the UAV.
In the experiments in this paper, we obtain the pointclouds and pose estimates from OpenVINS \cite{openvins}, using its mode of inverse depth estimation.

Our method works best when using inverse-depth parametrization \cite{inverse_depth} of point locations instead of estimating full 3D positions of visual keypoints in the SLAM.
Utilizing the inverse-depth parametrization allows the UAV to know that there are points in nearly infinite distance in some direction.
These points can be used to estimate that up to some distance, there is likely free space in that direction, such as when a UAV sees a tree line in the distance, or flies towards a distant building.
There can be object that are too small to be detected by the visual SLAM at large distances, so the free-space estimates might be wrong, but we show how to deal with these issues in Sec. \ref{sec:distance-based}.

Our method can, in principle, work with pointclouds from any source, even dense sensors such as depth cameras or LiDARs, and representing everything as elements in space allows storing information about e.g. some space looking as free according to camera data, but occupied according to a depth camera (such as in a window).
However, we leave it for future work to fully utilize this potential for multimodality.
\subsection{Map Updating}
\label{sec:map_building}
A single update iteration of the map can be broken down into these consecutive steps:
\subsubsection{Constructing the Visible Free Space Polyhedron}
The first step is to construct a polygon of space that is estimated to be free in the current timestep.
To interpolate depth between the sparse keypoints, we employ a simplified version of the approach described in FLAME \cite{flame}.
There, the authors focus on constructing a precise mesh from visual keypoint measurements, but here, we care primarily about mapping the free space, for planning purposes, and thus we do not perform the mesh optimization or splitting the 2D interpolations that are done in \cite{flame}.

We project the currently tracked \todo{triangulated points $X$} from visual SLAM into the image plane and compute their Delaunay triangulation. 
By connecting the points in 3D according to their Delaunay triangulation in 2D, we obtain a mesh $F_o$ that we call an \textit{obstacle mesh}, which is used only for the current frame.
We estimate the currently visible free space as the volume between the camera's focal point and all points on $F_o$.
This space is enclosed by connecting all points that lie on the \todo{edge} of $F_o$ to the camera's focal point, which forms an \textit{estimated visible free space polyhedron} $P_f$, visualized in Fig.\ref{fig:smap_intro}.

\subsubsection{Updating Existing Spheres}
Next, we update the radii of spheres that could be updated by $P_f$ or $X$.
To bound the update time of this step, we specify a maximum allowed sphere radius $r_{max}$, which allows us to quickly filter out all spheres whose centers fall outside a \todo{bounding box} around $P_f$, inflated by $r_{max}$. 
Then, for any remaining sphere with a center $x$ and radius $r_{k}$, the updated radius is computed as
\begin{equation}
  r_{k+1} = \min \left( \max \left( r_{k}, d(x, P_f) \right)
  , d(x, X \cup X_p), r_{max} \right),
  \label{eq:update}
\end{equation}
where $d(x, P_f)$ is the signed distance to $P_f$ (positive if the point is inside the polyhedron, negative if outside) and $d(x,X \cup X_p)$ is the minimum distance to all input obstacle points $X$, and to obstacle points $X_p$ which lie in the map, fall into $P_f$, but are not deleted in the current frame, as described more in Sec. \ref{sec:distance-based}.
Finally, we delete all spheres with $r_{k+1} < r_{min}$, where $r_{min}$ is the smallest allowed sphere radius specified by the user. 

% where TODO.
\subsubsection{Sampling New Spheres}
To introduce new spheres into the map, we sample a fixed number of points inside $P_f$ at random distances between the camera and the obstacle mesh $F_o$.
This is a simplistic approach, and could be improved for faster flight, for example by sampling along the predicted trajectory of the UAV at high speeds.
A potential new sphere's radius is determined in the same way as for the old spheres in the previous step in Eq. \ref{eq:update} with $r_k = 0$.
If the potential radius is larger than $r_{min}$, the sphere is added to the map.

\subsubsection{Recomputing and Sparsifying Sphere Graph}
After all the sphere radii updates have been made, we update the graph of spheres used for path planning, so that all intersecting spheres are connected in the graph.
% TODO - EXPLAIN???
Furthermore, to constrain map update time and path planning time, we perform a redundancy check on the updated and added spheres in the same way as in our previous work \cite{spheremap}.
If any sphere TODO, it is deleted from the map.
This way, large open areas are covered by only a few spheres and can be planned over quickly, and tight corridors have a higher density of spheres, capturing the information about potential paths and distances to obstacles in more detail.
\subsubsection{Updating Obstacle Points}
\label{sec:distance-based}
As the final step of the update, we decide which visible points $X$ to add into the map, and which points in the map to delete.
An important part of our method is that we store the minimum of the distances that any point $x$ has been observed from, denoted further as $d_{m, x}$.
This is due to the fact that the distance uncertainty of any point \todo{grows drastically} with the distance from camera in monocular SLAM \cite{inverse_depth}.

An apparent problem can thus arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 50m).
The newly visible points corresponding to the wall can have large position errors, as shown in Fig.\ref{fig:distbased}.
We also cannot simply delete every map point that falls into $P_f$, because the UAV might be at a distance where the object corresponding to the point is just too far to be seen as a keypoint in the image.

We solve both these problems by deleting any map point $x$ that falls into $P_f$ only if
\begin{equation}
  |x - p_{cam}| < 0.75 \cdot d_{m,x}
\end{equation}
where $p_{cam}$ is the position of the camera.
Additionally, points \todo{seen from a closer distance can erase points seen at larger distances, if they fall close enough to them, and in the same way, new points seen at a large distance are not added to the map, if there are more accurately measured points near them, also visualized in Fig.\ref{fig:distbased}.}

\begin{figure}[!t]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/distbased3.pdf}
  \centering
  \caption{Diagram of sphere sampling and map point management: 
  Visible free-space polygon (-);
  Newly sampled spheres (O);
  Triangulated keypoints visible at current frame (X); 
  Points already in the map (X) added from a previous pose (red). 
  Smaller points and higher-opacity spheres have been observed from lower distances.
  Dashed crosses correspond to points that will be deleted (red) or not added to the map (black).
  }
  \label{fig:distbased}
\end{figure}

% \section{Monocular-Inertial Exploration Using the Sphere-Based Map}
% \section{Safety-Aware Monocular-Inertial 3D Exploration}
\section{Safe Monocular-Inertial Exploration using the Sphere-Based Map}
In this section, we detail how our proposed mapping method and representation are highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a single camera.
In principle, we employ the commonly used receding-horizon next-best-view (RHNBV) [TODO-CITE] strategy, but with several major differences in methodology that are neccessary in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.

In traditional exploration approaches with dense distance sensors, it is often sufficient to move the robot to a boundary between free space and unknown space (known as a frontier), and assume that the distance sensors will uncover some additional space behind the frontier and thus expand the map.

Such approach will not, in principle, work well with a robot that only has a monocular camera for depth sensing.
The first thing that needs to be considered is that such a robot requires \textit{translational} motion to gain any sort of depth measurements when using methods built on structure-from-motion (SfM) [TODO-CITE], which is most of visual SLAM algorithms.
% as in monocular structure-from-motion (SfM) TODO-CITE (we do not consider deep learning methods which exploit knowledge about sizes of objects).
% Excessive rotation without translation cannot lead to reasonable distance estimates.
% Too much rotation with too little translation of the camera will cause the monocular SLAM or any SfM-based approach to not give reliable depth estimates.
With rotation only, or motions that have too much rotation compared to translation, reliable depth estimates cannot be obtained.

This motivates the first major distinction of our approach compared to traditional RHNBV --- when planning a path to a goal viewpoint that could uncover some frontiers, we compute the target headings on the path so that the UAV maintains a fixed heading for at least $d_{c}$ meters on the path before reaching the end viewpoint, as visualized in Fig. TODO-FIG.
In the experiments detailed in Sec. \autoref{sec:experiments}, we set $d_{c}=TODO$.
This way, the UAV has enough translational motion before reaching a given viewpoint, so that if there are visible visual keypoints from that viewpoint, it will most likely gain enough distance measurements to them so they can be used for the depth estimation described in Sec. \autoref{sec:map_building}.

This is an important part of the method also due to the second major distinction -- we block the sampling of new exploration viewpoints near explored viewpoints.
This is due to the fact that real-world environments often contain textureless areas (mainly in buildings or other man-made structures).
Because in our mapping approach, obstacle points correspond to triangulated visual keypoints, no points will be added on textureless surfaces, but they will be on the boundary of free-space, so there will be frontiers there.
However, such frontiers are uncoverable, since there is nothing behind them, and for this reason, we block the sampling of viewpoints near visited ones, so that the UAV doesn't keep coming back to look at a blank wall.

Thirdly, because our map representation is fundamentally different from an occupancy grid, we compute the frontiers in a different way than with an occupancy grid --- we sample points along the visible free-space polyhedron described in Sec. \autoref{sec:map_building} at each map update, and add the points as frontiers, if they do not lie inside any free-space sphere and if they are at some user-defined distance from all map obstacle points.
If they do not meet these criteria in any following update, they are deleted. 

In 3D outdoor exploration, it is also important to assign different value to different frontiers. 
We constrain the exploration to only consider frontiers that lie near some obstacle points.
Thus, the UAV does not explore up into the sky, and explores near texture-rich areas, leading to fewer possible failure situations due to losing visual odometry tracking.
We then add new exploration viewpoints into the map only if a sufficient number of such frontier points would be visible from a given viewpoint.

To allow exploration even in large-scale environments, we employ a local-global approach, basically a simplified version of TODO-REF.
The UAV first tries finding paths to any exploration viewpoints up to a user-defined "local exploration radius".
If no reachable viewpoints are found in the radius, the UAV tries finding paths to all exploration viewpoints stored in the map.
Thanks to the planning efficiency of the sphere graph, this global replanning usually does not take more than a few seconds, but could be made even faster by implementing any sort of long-distance planning abstraction graph, as in TODO or TODO-REF-OWN, which we leave for future work.

% Lastly, using the sphere-based representation allows us to flexibly weigh path length and path proximity to obstacles. We use the same path cost criterion as in \cite{spheremap}, and thus we can 
% Lastly, because the free space is represented by a graph of spheres, we immediately have an upper bound on the distance of obstacles at each graph node, and thus it is possible to efficiently weigh path length versus path proximity to obstacles, 

With all of these considerations, a UAV with only a monocular camera and IMU can perform 3D volumetric exploration in large-scale outdoor environments, as we show in Sec. \autoref{sec:experiments}.
\todo{
It would be interesting to compare our method to monocular exploration using an occupancy grid at a similar scale, but, to the best of our knowledge, this has not yet been documented in literature.
Thus, we can only talk about the benefits of our proposed representation --- which is that it allows much more efficient path planning that takes into account the distance to obstacles than using an occupancy grid, as we discuss more in \cite{spheremap}.
}
As future work, we also propose to utilize the measurement distance of free-space spheres so that path planning can, for example, force the UAV to fly slower and always forward-facing in spheres that have been observed from a large distance and might contain some obstacles not observable from far away.
% TODO - rewrite something something with potential!

\section{Experiments}
\label{sec:experiments}
\subsection{Large-Scale Exploration in Simulation}
In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.

TODO - RUN, EVALUATE, ADD TABLE AND IMGS

TODO - RUNTIME ANALYSIS NOTE
% In this experiment, we demonstrate the performance of the proposed occupancy mapping while using distant points with high depth covariance, against using closer-range points, which are usually available from most visual SLAM algorithms.
% As you can see in TODO, by using the distant points, the UAV is able to quickly construct an estimate of 

\begin{figure}[!htb]
  \includegraphics[width=8.5cm]{fig/sim_exper_gazebo.png}
  \includegraphics[width=8.5cm]{fig/sim_exper_end.png}
  % \includegraphics[width=8.5cm]{fig/exper3.png}
  % \includegraphics[width=8.5cm]{fig/exper4.png}
  \centering
  % \caption{TEMPORARY IMG - Exploration experiment visualization}
  \caption{The resulting map (bottom) after 20min of exploring an urban environment (top) in simulation, explained in more detail in TODO-REF. Black arrows indicate visited frontier viewpoints, frontiers are shown as purple points. Notice that the UAV also went to explore up on the rooftops, where it could see the metal towers.}
  \label{fig:exper_sim}
\end{figure}

\subsection{Real-World UAV Monocular-Inertial Exploration}
In this experiment we demonstrate that the proposed mapping method using only a single monocular RGB camera and IMU is useful for path and exploration planning on a real-world UAV platform in a mixed indoor-outdoor warehouse setting.

The UAV is TODO-HARDWARE \cite{mrs_uav_robust_system}.

The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and mapper modules.

As can be seen from TODO-REF, and multimedia materials supporting the experiment, the constructed representation is quite suited for the necessary level of vision-based autonomy.
The UAV explored TODO.
It is currently not possible to compare TODO.
\begin{figure}[!htb]
  \includegraphics[width=8.5cm]{fig/exper1.png}
  \includegraphics[width=8.5cm]{fig/exper4.png}
  % \includegraphics[width=8.5cm]{fig/exper3.png}
  % \includegraphics[width=8.5cm]{fig/exper4.png}
  \centering
  \caption{TEMPORARY IMG - Exploration experiment visualization}
  \label{fig:exper_real}
\end{figure}

\subsection{Runtime analysis}
Here we analyze the runtimes of this method, and TODO.

\begin{figure}[!htb]
  % \includegraphics[width=8.5cm]{}
  \adjincludegraphics[width=9cm, trim={{0.05\width} {0.00\height} {0.05\width} {0.00\height}}, clip=true]{fig/smap_runtimes_tmp.pdf}
  \centering
  \caption{Runtime analysis of parts of the map update detailed in \autoref{sec:map_building}.}
  \label{fig:runtimes}
\end{figure}



\section{CONCLUSION}
In this paper, we presented a novel sphere-based spatial mapping method 
% together with a robust monocular inverse-depth estimator,
using sparse point cloud data from monocular visual sensors,
and demonstrated how it can enable any UAV equipped with a single camera and any form of odometry to quickly and safely navigate and explore unknown, unstructured 3D environments.
We have demonstrated, through testing in simulation and in the real world, that our new mapping approach and spatial representation enable safe, large-scale autonomous navigation and exploration on UAVs without traditionally used expensive range sensors.

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
% \bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
% \bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
% \bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
% \bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
