%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\todo}[1]{{\hypersetup{allcolors=red}{\color{red} {#1}}}}
\newcommand{\todocaption}[1]{{\color{red} {#1}}}

% \newcommand{\vk}[1]{#1}
% \newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
% Monocular Sphere-Based Occupancy Mapping and Large-Scale Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Sparse Monocular SLAM for Robust Large-Scale 3D Exploration on Inexpensive UAVs}

% FINAL TWO
% Sphere-Based Mapping using Sparse Monocular SLAM Points for Robust Large-Scale 3D Exploration on Inexpensive UAVs}
% Monocular-Inertial Exploration of Large-Scale Outdoor Environments using Sparse SLAM Keypoints on an Inexpensive UAV}
Monocular-Inertial UAV Exploration and Mapping of Large-Scale Outdoor Environments using Sparse Visual SLAM}

% % PolySphereMap - Occupancy Mapping using Sparse Monocular SLAM for Large-Scale 3D Exploration on Inexpensive UAVs}
% PolySphereMap - Mapping and Exploring Large-Scale 3D Environments using a Monocular Camera on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  We present a novel approach to mapping 3D space for navigation purposes, using only pose estimates and keypoints from sparse monocular-inertial SLAM as input.
  % Our main contribution is representing an environment by a graph of intersecting free-space spheres and a set of triangulated visual keypoints with remembered measurement uncertainty, which is fundamentally different from grid-based occupancy maps. 
  We propose to represent an environment by a graph of intersecting free-space spheres and a set of obstacle points with remembered measurement distance, which is fundamentally different from traditionally used grid-based maps. 
  % We present a method of building this representation in real-time, using only sparse 3D point data obtained from monocular SLAM by inscribing spheres inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  % This representation can be built in real-time, using only sparse 3D point data obtained from monocular SLAM by inscribing spheres inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  % In our approach, the map is built in real-time by roughly interpolating depth between visible 3D points from visual SLAM while 
  % Spheres are inscribed inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  A polyhedron of approximated visible free-space is formed at each map iteration from tracked SLAM points and the camera's position.
  % Spheres are inscribed inside this polyhedron.
  This polyhedron is used for updating sphere radii and deleting obstacle points.
  % Spheres are inscribed inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  Furthermore, we describe an exploration approach that uses the advantages of the novel representation to overcome the challenges of monocular vision-based UAV autonomy.
  The presented method allows
large-scale autonomous exploration in unstructured indoor/outdoor 3D environments, using only a single monocular camera and an IMU onboard a possibly micro-scale UAV.
  We open-source the code for our methods to provide 3D mapping and exploration capabilities to any UAV with visual-inertial state estimation.

Code--- %\href{
\href{TODO}{TODO}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
The autonomous UAV systems operating in unknown environments documented in recent literature \cite{darpa_cerberus_wins, beneath} usually construct an occupancy grid \cite{occupancy_moravec}, such as the efficient octree-based implementation of OctoMap \cite{octomap}, and use it as the base representation for planning, exploration and constructing higher-level spatial abstractions \cite{topomap, spheremap}.
However, building a detailed occupancy grid requires dense pointclouds of range measurements due to the raycasting nature of occupancy grid updates.
% which is to cast rays towards the measurement points and update the occupancy of traversed cells.
% TODO - check if all
Thus, state-of-the-art exploration and navigation UAV systems have been constrained to expensive, heavy or limited-range sensors such as LiDARs, depth cameras or stereo-camera pairs, which can provide these dense point clouds.

Building an occupancy representation for exploration using only a monocular camera for depth sensing is challenging for many reasons, and has so far been demonstrated only in small-scale indoor environments.
However, being able to build a representation that would allow safe and rapid navigation in unknown environments on-board UAVs with such small, light-weight and  inexpensive sensors, would unlock many potential applications, such as swarms of disposable UAVs for search and rescue missions.
% TODO - split
In addition, cameras are useful for object recognition and other types of vision-based perception beneficial for autonomous UAVs, and thus 
% it makes sense to have them equipped on many UAV applications already.
they are already equipped on UAVs in most real-world applications.
Furthermore, a single camera onboard a moving UAV can theoretically provide rough distance estimates on distant objects when using the inverse depth parametrization \cite{inverse_depth}, far beyond the approx. 50m range of LiDARs.

\begin{figure}[!t]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  % \includegraphics[width=8.5cm]{fig/explor_forest2.png}
  % \includegraphics[width=8.5cm]{fig/intro_poly.png}
    % \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.05\height} {0.05\width} {0.1\height}}, clip=true]{fig/intro_poly.png}
    \adjincludegraphics[width=8.5cm, trim={{0.0\width} {0.00\height} {0.00\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}
  \centering
  \caption{Visualization of the proposed map representation and how it is updated. The green polyhedron is an estimate of currently visible free space and is formed by the visible triangulated keypoints and the camera's position (axes). The mapped spheres (green), obstacle points (red) and frontier points (purple) are updated by the polyhedron and visible keypoints, as described in Sec. \ref{sec:map_building}}
  \label{fig:smap_intro}
\end{figure}

In this paper, we present a novel approach to 3D mapping that takes in a sparse pointcloud of visual keypoints tracked by monocular SLAM (already a necessary part of many vision-based UAV systems).
Instead of traditionally casting rays through a grid and updating its cells, we construct a polyhedron of approximate visible free space at each frame, inscribe spheres of free space inside that polyhedron, and connect them together to form a graph of intersecting spheres.
By storing obstacles as a set of points, we can assign additional metrics to each point, such as measurement distance, or, in future work, the modality of its origin.
This allows for rule-based measurement fusion, which is critical for dealing with monocular cameras, as we explain in \autoref{sec:distance-based}.
% In this representation, we assign additional metrics
% This representation allows for fast and safety-aware planning \cite{spheremap}.
We further show how the proposed mapping method enables autonomous exploration in real-world and simulated large-scale outdoor environments on a UAV using only a single camera and IMU as its sensors, which, to the authors' best knowledge, has not yet been achieved in literature.

\subsection{Related Works -- Spatial Representations}
\label{sec:representations_rw}

% The environment representation similar to our previous work \cite{spheremap} -- in \cite{spheremap}, we also built a graph of intersecting free-space spheres, but 
In robotic exploration, the majority of approaches use a grid-based representation of occupied, unknown, and free space, either in the form of an euclidean signed distance field (ESDF) \cite{voxblox} or an occupancy grid \cite{occupancy_moravec, octomap}.
A grid-based representation, while being the basis for a substantial amount of robotic path planning and higher-level spatial abstraction methods \cite{topomap, skeletons, spheremap}, has several drawbacks for usage on UAVs without dense range sensors in varying-scale environments.

Firstly, both ESDFs and occupancy grids are most commonly updated by casting rays towards measured 3D points and updating the values of traversed cells.
Using this technique with sparse points from monocular SLAM, e.g. in texture-poor environments, can lead to gaps in the map.
Existing works address this issue in multiple ways:
% Treating sparse points from monocular SLAM as 
% The existing works on using using monocular SLAM points for 3D mapping treat the SLAM points as range measurements and perform the raycasting method as well. 
% With only sparse 
% This becomes problematic when the available measured points are very sparse, such as points tracked by sparse monocular SLAM.
% Some works were motivated to overcome this limitation -- 
In \cite{from_monoslam_to_explo}, the authors take points from semi-dense monocular SLAM and raycast towards them to build an OctoMap \cite{octomap} occupancy grid, but only demonstrate their local exploration approach built on top of this mapping pipeline to work inside a single room with textured walls.
The authors of \cite{los_maps} use sparse monocular SLAM, they also cast rays towards the measured points to build an occupancy grid, and show exploration on a UAV across multiple rooms.
They set the occupancy grid cell size to be 0.5m, which is relatively large compared to the UAV size. 
Such rough voxel size cannot represent the distance to obstacles in high resolution, and if the grid is misaligned with the building, narrow passages can be represented as non-traversible.
% and in [ROOM-DENSEDEPTH], the authors essentially perform dense pixel-based estimation with known poses from sparse SLAM, which works in well-illuminated rooms but may have problems in larger-scale or texture-poor environments.
% Second, even though memory-efficient octree-based representations of 3D grids are available, most notably the OctoMap library \cite{octomap}, a fundamental problem of grids is that the smallest-voxel size must be set by the user.

This is the second fundamental problem of grid-based representations -- the smallest-voxel size must be set by the user.
If this is set as too large, narrow passages will not be captured in the map, and if it is too small, the map will need large amounts of raycasts to set all the cells' occupancy values, which is computationally expensive.
% In our previous work \cite{spheremap}, we have shown how a graph of intersecting spheres can be built out of an occupancy grid, and that this representation allows path planning that has explicit information about distance to obstacles, in addition to being orders of magnitude faster than on an occupancy grid.
In our previous work \cite{spheremap}, we have shown that representing free space by a graph of intersecting spheres can efficiently describe large-space while keeping detailed information in narrow spaces.
In additon, it allows orders of magnitude faster path planning than grid-based representations.
% In \cite{spheremap}, we also demonstrated that this representation allows path planning that has explicit information about distance to obstacles, in addition to allowing orders of magnitude faster path planning than on an occupancy grid.
In this paper, we represent free space using spheres as well, but with two major differences: 
% In this paper, we represent the free space using spheres as well, but also add the obstacle points into the map and use them for constraining the sphere radii, but with \todo{two major differences 

\begin{itemize}
    % \item now \textit{we do not need to build the occupancy grid as an intermediate step}, which frees up significant computational resources, and \textit{we construct the map using sparse point-clouds from monocular SLAM}, which are problematic to use for occupancy grids.
  % \item In this paper, we build the graph of spheres \textit{directly} out of the sparse 3D pointclouds and pose estimates obtained from monocular SLAM, without needing to build an intermediate occupancy grid before computing the sphere radii as in \cite{spheremap}, which frees up significant computational resources.
  \item In \cite{spheremap}, the method required building a local occupancy grid with a LiDAR for updating the spheres. 
    In this paper, we build the graph of spheres \textit{directly} out of 3D measurement points without any intermediate occupancy grid, and only require a monocular camera running visual SLAM instead of a LiDAR.
% In addition, there is no current documented way to build an occupancy grid using sparse point clouds, although this could also be interesting to research.
% TODO-CHECK
  \item Additionally, the representation proposed in this paper stores visual keypoints as obstacle points and remembers their measurement distance. 
    These points are important for correctly estimating free space when previously seen obstacles are momentarily not detected by the visual SLAM (see Sec. \ref{sec:distance-based}). 
        % to overcome the challenge of incorrect measurements at larger distances.
\end{itemize}

\subsection{Related Works -- Vision-Based Exploration}

Vision-based UAV autonomous exploration has not yet reached the levels of autonomy as when using dense depth sensors, and remains a challenge, as documented in recent surveys \cite{drones_survey}, but some progress has been made.
% TODO - mention reactive only
In \cite{explor_stereo}, the authors presented the first ever vision-based UAV system that can explore and build a map (a 3D occupancy grid) on a UAV equipped with stereo cameras and also a supporting downward-facing optical flow sensor for better motion estimation.
The authors used dense pointclouds coming from the stereo cameras for building an OctoMap \cite{octomap} occupancy representation in the same way as with depth data from an RGB-D camera or LiDAR.
This allowed them to use similar exploration techniques as on systems with such sensors.

For UAVs equipped with only a monocular camera, to the authors' best knowledge, all the existing works have demonstrated autonomous 3D mapping and exploration on a UAV only indoor and at the small scale of a single room \cite{from_monoslam_to_explo, cnn_explo_singleroom} or a few simple rooms \cite{los_maps, simon2023mononav}.
% It can be said that the main challenge here is that the majority of robotics methods is based on first building an occupancy grid using dense data measurements from a depth camera, LiDAR or stereo cameras, but obtaining such data from a monocular camera is difficult.
% The discussed approaches either accept that the data from visual SLAM is very sparse, or they use deep-learning-based depth estimation to obtain dense data.
Monocular SLAM, both sparse and dense, cannot produce measurement points on textureless areas, and the existing approaches either accept that the data from visual SLAM can be very sparse, or they use deep-learning-based depth estimation to obtain dense data.

The authors of \cite{from_monoslam_to_explo, los_maps} argue against using frontier-based methods \cite{paper_frontier_grandpa}, due to the facts that no points will ever be generated on textureless areas, and thus those frontiers would not be uncovered by a monocular camera.
The authors have presented alternative exploration approaches, but they have only shown them to work in small-scale environments.
In our exploration approach, thanks to not using raycasting in the mapping pipeline and also blocking explored viewpoints, we show that frontier-based exploration is possible and assures global coverage of the explored environments.

In a different approach, the authors of \cite{simon2023mononav, cnn_explo_singleroom} use deep-learning-based single image depth estimation to obtain dense point clouds from a monocular camera, essentially turning it into a depth camera.
However, using learning-based monocular depth estimation models brings additional problems.
As discussed in recent surveys \cite{deep_mono_depth_2021, deep_mono_depth_2019}, this approach struggles to provide real-time onboard performance and it is not assured that it will work reliably in domains that the network was not trained on.
In addition, a GPU significantly raises the cost and weight of a UAV.

% \todo{TODO - cite corridors!!!}

% In \cite{corridors_mono_nav_2009}, the authors presented a system for monocular UAV navigation and demonstrated autonomous flight and map-building in an office corridor.

Compared to the above mentioned approaches, we demonstrate that our approach achieves exploration and navigation in 3D using a monocular camera and an IMU in outdoor, large-scale environments, with all of the computations running on-board the UAV, without requiring a GPU.

\subsection{Contributions}
To enable robust vision-based autonomous navigation in unknown, unstructured 3D environments for inexpensive UAVs, our paper brings the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  \item A novel 3D mapping approach, which constructs a sphere-based map using sparse pointclouds obtained from monocular-inertial SLAM as the only depth information.
    % TODO - maybe define this as GEOMETRIC-PRIMITIVE-GRAPH MAP???
    \item A system that demonstrates large-scale 3D outdoor exploration on a UAV equipped with only a monocular camera and IMU as its sensors, enabled by the proposed mapping method and 
      validated in real-world conditions.
    \item Open-sourced code for the novel mapping and exploration approach, along with example simulation scripts for replicating our results.
\end{enumerate}

\section{Sphere-Based Map Built From Sparse Visual SLAM Keypoints}
\label{sec:map_building}
% In this section, we explain how such map can be built directly from the outputs of visual SLAM, without needing any intermediate occupancy grid computation, and how the sparsity and high depth uncertainty of the input pointclouds is handled. 
% In this section, we first explain the two fundamental features of our method for handling the problems of using sparse monocular SLAM keypoints as depth measurements.
% Th
% Then, we describe the process of constructing the map.
% This section describes the proposed mapping approach.
This section describes the proposed mapping approach.
In \autoref{sec:structure}, we introduce the novel spatial representation and its properties.
% In Section \ref{sec:structure}, we describe how to construct a visible free-space polyhedron, using sparse SLAM keypoints and safely extended by virtual 3D points when moving through wide areas.
% In Section \ref{sec:polyhedron}, we describe how to construct a visible free-space polyhedron using sparse visual SLAM keypoints at a given time, extended by virtual points when moving through large-scale areas.
In \autoref{sec:polyhedron}, we construct an estimated visible free-space polyhedron using motion information and SLAM points available at a given time.
% Next, in Section \ref{sec:existing_spheres_update}, we explain how the free-space polyhedron is used to update radii of 
In \autoref{sec:distance-based}, the free-space polyhedron and keypoint measurement distances are used to determine which points are added or deleted from the map.
Next, in \autoref{sec:existing_spheres_update}, we describe how the free-space polyhedron is used to update the radii of existing and newly sampled free-space spheres.
% As the last step, in \autoref{sec:existing_spheres_update}, we update the connections of modified spheres and prune the sphere graph to keep it sparse.
As the last step, in \autoref{sec:graph_update}, we update the connections of the modified spheres and prune the sphere graph to keep it sparse for rapid path planning.

% \subsection{Map Data Structure}
% \subsection{Free-Sphere and Obstacle-Point Map Representation}
\subsection{Map Representation and Notation}
\label{sec:structure}
We propose to represent free space by a graph of intersecting spheres.
% and obstacles as a set of 3D points corresponding to triangulated visual keypoints from visual SLAM.
Each sphere at the $k$-th update iteration has a static center $\mathbf{c}$ and changing radius $r_k$.
The radius $r_k$ represents the distance to the nearest unknown space and obstacles at $\mathbf{c}$.
We maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres, and use it for path planning.
Furthermore, we maintain a set of obstacle points $\mathbf{X}$, which correspond to the textured surfaces measured by the visual SLAM.
For each obstacle point $\mathbf{x} \in \mathbf{X}$, we store the lowest distance that the point was measured from $d_{\mathbf{X}, \min}$.
A lower minimal measurement distance of a visual keypoint means that the point is less likely to be noise and we make it more difficult to erase it from the map.
This is a necessary feature for safe flight when using sparse visual SLAM keypoints as the only depth information, as explained more in \autoref{sec:distance-based}. 
% In the experimental section, we specifically use OpenVINS \cite{openvins} for the monocular-inertial pose and 3D point estimation.
% The main motivations for using this representation instead of a traditional grid-based representation are discussed in detail in \autoref{sec:representations_rw}.
% The following sections describe the individual steps of a single map update iteration.

\subsection{Map Update Algorithm}
The mapping runs on-board the UAV in real-time, using only the pose and 3D landmark information from visual SLAM as its inputs, approximately at 5Hz.
A single map update iteration consists of the following steps in order:
\subsubsection{Constructing the Visible Free Space Polyhedron}
\label{sec:polyhedron}
The first step is to construct a polygon of space that is estimated to contain free space based on the information in the current timestep.
To interpolate depth between the sparse keypoints, we employ a simplified version of the approach described in FLAME \cite{flame}.
There, the authors focus on constructing a precise mesh from visual keypoint measurements, but here, we care primarily about mapping the free space, for planning purposes, and thus we do not perform the mesh optimization or splitting the 2D interpolations that are done in \cite{flame}.

We project the currently tracked visual SLAM points into the image plane and compute their Delaunay triangulation. 
By connecting the points in 3D according to their Delaunay triangulation in 2D, we obtain a mesh $\mathbf{F}_d$ that we call a \textit{depth mesh}, which is used only for the current frame.
We estimate the currently visible free space as the volume between the camera's focal point and all points on $\mathbf{F}_d$.
This space is enclosed by connecting all points that lie on the edge of $\mathbf{F}_d$ to the camera's focal point, which forms an \textit{estimated visible free space polyhedron} $\mathbf{P}_f$, visualized in Fig.\ref{fig:smap_intro}.

When moving in open areas, such as over a field, visual SLAM often tracks only a few keypoints on the ground, which would cause free space to be estimated only below the UAV and not allow flight across the field.
% To allow safe flight in the absence of visual keypoints in front of the UAV, we keep track of a set of \textit{virtual keypoints} in front of the UAV.
To allow safe flight in open areas, we make the following assumption: 
Consider a virtual keypoint $\mathbf{x}_{vir}$ at time $t_2$ that falls into the UAV's FoV since $t_1$ in the past up to $t_2$ (i.e. it could be tracked by visual SLAM).
Then, if there is a point on the UAV's trajectory between $t_1$ and $t_2$ such that there would be enough parallax for estimating the 3D position of $\mathbf{x}_{vir}$ (according to a user-defined threshold), and if the visual SLAM has not triangulated any point at approximately that position, then $\mathbf{x}_{vir}$ must lie in free space.
This approximation of course fails in the case of featureless walls, but works quite well in large-scale outdoor environments.

% We keep a set of virtual points $\mathbf{x}_vir$ in front of the UAV at all times, and if some of the points become are estimated as free based on the above assumption, 
We periodically check the above condition for a set of virtual points $\mathbf{x}_vir$ at a fixed distance in front of the UAV at each iteration. 
We take the points that fulfill this condition and add them to the creation of the \textit{depth mesh} $\mathbf{F}_d$, as shown in TODO-FIG.
However, we discard points that would fall into the Delaunay triangulation of the true obstacle points, since there we have an existing depth estimate.
This way, we can safely estimate additional freespace, in the case of sufficiently textured wide outdoor areas.
% These virtual points are only used for estimating free space, and are not added as obstacles and cannot be used to delete existing points

% The visual SLAM usually has 

\begin{figure}[!h]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/distbased3.pdf}
  \centering
  \caption{Diagram of sphere sampling and map point management: 
  Visible free-space polygon (-);
  Newly sampled spheres (O);
  Triangulated keypoints visible at current frame (X); 
  Points already in the map (X) added from a previous pose (red). 
  Smaller points and higher-opacity spheres have been observed from lower distances.
  Dashed crosses correspond to points that will be deleted (red) or not added to the map (black).
  }
  \label{fig:distbased}
\end{figure}


\subsubsection{Updating Obstacle Points}
\label{sec:distance-based}
As the next step of the update, we decide which tracked SLAM points to add into the map points $\mathbf{X}$, and which points in the map to delete.
An important part of our method is that we store the minimum of the distances that any point $\mathbf{x}$ has been observed from, denoted further as $d_{m, x}$.
% This is due to the fact that the distance uncertainty of any point \todo{grows drastically} with the distance from camera in monocular SLAM \cite{inverse_depth}.
An apparent problem can arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 20m).
% The newly visible points corresponding to the wall can have large position errors, as shown in Fig.\ref{fig:distbased}, or not be detected as a point at all.
% The visible points corresponding to the wall might not be detected as SLAM keypoints while p.
A situation can occur where the previously mapped points are now triangulated at an incorrect position, or are not triangulated by the visual SLAM at all (e.g. due to insufficient parallax), while some other points that lie beyond the original points are triangulated, as shown in Fig \ref{fig:distbased}.
% We also cannot simply delete every map point that falls into $\mathbf{P}_f$, because the UAV might be at a distance where the object corresponding to the point is just too far to be seen as a keypoint in the image.
Thus, we cannot simply delete every map point that falls into $\mathbf{P}_f$, because then the original points (red in Fig \ref{fig:distbased}) would be wrongly deleted and assumed as free space.

We solve this problem by deleting any map point $x$ that falls into $\mathbf{P}_f$ only if
\begin{equation}
  |\mathbf{x} - \mathbf{p}_{cam}| < 1.1 \cdot d_{m,x}
\end{equation}
where $p_{cam}$ is the position of the camera.
The points that lie in $\mathbf{P}_f$ and are not deleted are called \textit{protected points} and are used to constrain sphere radii in the following update step.
Additionally, to prioritize closer measurements, points seen from a closer distance can replace points seen at larger distances, if they are measured close enough to them. 
In the same way, new points seen at a large distance are not added to the map, if there are more accurately measured points near them, also visualized in Fig.\ref{fig:distbased}.

\subsubsection{Updating Existing Spheres}
\label{sec:existing_spheres_update}
Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or input points $\mathbf{X}_{in}$.
To bound the update time of this step, we specify a maximum allowed sphere radius $r_{max}$, which allows us to quickly filter out all spheres whose centers fall outside a bounding box around $\mathbf{P}_f$, inflated by $r_{max}$. 
Then, for any remaining sphere with a center $\mathbf{c}$ and radius $r_{k}$, the updated radius is computed as
\begin{equation}
  r_{k+1} = \min \left( \max \left( r_{k}, d(\mathbf{c}, \mathbf{P}_f) \right)
  , d(\mathbf{c}, \mathbf{X}_{in} \cup \mathbf{X}_p), r_{max} \right),
  \label{eq:update}
\end{equation}
where $d(x, \mathbf{P}_f)$ is the signed distance to $\mathbf{P}_f$ (positive if the point is inside the polyhedron, negative if outside) and $d(\mathbf{x},\mathbf{X}_{in} \cup \mathbf{X}_p)$ is the minimum distance to all input obstacle points $\mathbf{X}_{in}$, and to the protected map points $\mathbf{X}_p$ described in Sec. \ref{sec:distance-based}.
Finally, we delete all spheres with $r_{k+1} < r_{min}$, where $r_{min}$ is the smallest allowed sphere radius specified by the user. 

% where TODO.
\subsubsection{Sampling New Spheres}
\label{sec:adding_spheres}
To introduce new spheres into the map, we sample a fixed number of points inside $\mathbf{P}_f$ at random distances between the camera and the depth mesh $F_o$.
This is a simplistic approach, and could be improved for faster flight, for example by sampling along the predicted trajectory of the UAV at high speeds.
A potential new sphere's radius is determined in the same way as for the old spheres in the previous step in Eq. \ref{eq:update} with $r_k = 0$.
If the potential radius is larger than $r_{min}$, the sphere is added to the map.

\subsubsection{Recomputing and Sparsifying Sphere Graph}
\label{sec:graph_update}
After all the sphere radii updates have been made, we update the graph of spheres used for path planning, so that all intersecting spheres are connected in the graph.
% TODO - EXPLAIN???
Furthermore, to constrain map update time and path planning time, we perform a redundancy check on the updated and added spheres in the same way as in our previous work \cite{spheremap}.
If any sphere is found to be redundant, it is deleted from the map.
This way, large open areas are covered by only a few spheres and can be planned over quickly, and tight corridors have a higher density of spheres, capturing the information about potential paths and distances to obstacles in more detail.

% \section{Monocular-Inertial Exploration Using the Sphere-Based Map}
% \section{Safety-Aware Monocular-Inertial 3D Exploration}
\section{Safe Monocular-Inertial Exploration using the Sphere-Based Map}
In this section, we detail how our proposed mapping method and representation are highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a single camera.
In principle, we employ the commonly used receding-horizon next-best-view (RHNBV) [TODO-CITE] strategy, but with several major differences in methodology that are neccessary in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.

\subsection{Forced Translation when reaching Viewpoints}
In traditional exploration approaches with dense distance sensors, it is often sufficient to move the robot to a boundary between free space and unknown space (known as a frontier), and assume that the distance sensors will uncover some additional space behind the frontier and thus expand the map.

Such approach will not, in principle, work well with a robot that only has a monocular camera for depth sensing.
The first thing that needs to be considered is that such a robot requires \textit{translational} motion to gain any sort of depth measurements when using methods built on structure-from-motion (SfM) [TODO-CITE], which is most of visual SLAM algorithms.
% as in monocular structure-from-motion (SfM) TODO-CITE (we do not consider deep learning methods which exploit knowledge about sizes of objects).
% Excessive rotation without translation cannot lead to reasonable distance estimates.
% Too much rotation with too little translation of the camera will cause the monocular SLAM or any SfM-based approach to not give reliable depth estimates.
With rotation only, or motions that have too much rotation compared to translation, reliable depth estimates cannot be obtained.

This motivates the first major distinction of our approach compared to traditional RHNBV --- when planning a path to a goal viewpoint that could uncover some frontiers, we compute the target headings on the path so that the UAV maintains a fixed heading for at least $d_{c}$ meters on the path before reaching the end viewpoint, as visualized in Fig. TODO-FIG.
In the experiments detailed in Sec. \autoref{sec:experiments}, we set $d_{c}=TODO$.
This way, the UAV has enough translational motion before reaching a given viewpoint, so that if there are visible visual keypoints from that viewpoint, it will most likely gain enough distance measurements to them so they can be used for the depth estimation described in Sec. \autoref{sec:map_building}.

\subsection{Explored Viewpoint Blocking}
This is an important part of the method also due to the second major distinction -- we block the sampling of new exploration viewpoints near explored viewpoints.
This is due to the fact that real-world environments often contain textureless areas (mainly in buildings or other man-made structures).
Because in our mapping approach, obstacle points correspond to triangulated visual keypoints, no points will be added on textureless surfaces, but they will be on the boundary of free-space, so there will be frontiers there.
However, such frontiers are uncoverable, since there is nothing behind them, and for this reason, we block the sampling of viewpoints near visited ones, so that the UAV doesn't keep coming back to look at a blank wall.

\subsection{Frontier Sampling on Free-Space Polyhedron}
Thirdly, because our map representation is fundamentally different from an occupancy grid, we compute the frontiers in a different way than with an occupancy grid --- we sample points along the visible free-space polyhedron described in Sec. \autoref{sec:map_building} at each map update, and add the points as frontiers, if they do not lie inside any free-space sphere and if they are at some user-defined distance from all map obstacle points.
If they do not meet these criteria in any following update, they are deleted. 

In 3D outdoor exploration, it is also important to assign different value to different frontiers. 
We constrain the exploration to only consider frontiers that lie near some obstacle points.
Thus, the UAV does not explore up into the sky, and explores near texture-rich areas, leading to fewer possible failure situations due to losing visual odometry tracking.
We then add new exploration viewpoints into the map only if a sufficient number of such frontier points would be visible from a given viewpoint.

\subsection{Local-Global Exploration}
To allow exploration even in large-scale environments, we employ a local-global approach, basically a simplified version of TODO-REF.
The UAV first tries finding paths to any exploration viewpoints up to a user-defined "local exploration radius".
If no reachable viewpoints are found in the radius, the UAV tries finding paths to all exploration viewpoints stored in the map.
Thanks to the planning efficiency of the sphere graph, this global replanning usually does not take more than a few seconds, but could be made even faster by implementing any sort of long-distance planning abstraction graph, as in TODO or TODO-REF-OWN, which we leave for future work.

\subsection{Safety-Aware Planning}
TODO - forward heading + using distance to obstacles

% Lastly, using the sphere-based representation allows us to flexibly weigh path length and path proximity to obstacles. We use the same path cost criterion as in \cite{spheremap}, and thus we can 
% Lastly, because the free space is represented by a graph of spheres, we immediately have an upper bound on the distance of obstacles at each graph node, and thus it is possible to efficiently weigh path length versus path proximity to obstacles, 

With all of these considerations, a UAV with only a monocular camera and IMU can perform 3D volumetric exploration in large-scale outdoor environments, as we show in Sec. \autoref{sec:experiments}.
\todo{
It would be interesting to compare our method to monocular exploration using an occupancy grid at a similar scale, but, to the best of our knowledge, this has not yet been documented in literature.
Thus, we can only talk about the benefits of our proposed representation --- which is that it allows much more efficient path planning that takes into account the distance to obstacles than using an occupancy grid, as we discuss more in \cite{spheremap}.
}
As future work, we also propose to utilize the measurement distance of free-space spheres so that path planning can, for example, force the UAV to fly slower and always forward-facing in spheres that have been observed from a large distance and might contain some obstacles not observable from far away.
% TODO - rewrite something something with potential!

\section{Experiments}
\label{sec:experiments}
In this section, we present the performed experiments and show the performance of our method in large-scale simulated (Sec. \ref{sec:sim_exper} and real-world (Sec. \ref{sec:real_exper}) environments.
Our implementation of the proposed mapping and exploration methods is single-threaded, written in python and uses the Delaunay triangulation implementation from TODO.
As shown in Sec \ref{sec:runtimes}, even with this simple implementation, the mapping keeps below $1s$ per map update on a standard CPU, and is thus suitable for real-world deployment.

The UAV used in the experiments was equipped with a monocular fisheye Bluefox TODO camera, and TODO IMU.
The MRS UAV system \cite{mrs_uav_robust_system} was used for trajectory generation and control.
% Damping in the form of 3D-printed flexible elements 
In real-world experiments, the camera and IMU are separated from the rest of the UAV by 3D-printed damping elements, which significantly reduces the IMU noise caused by propellers.
For state estimation and as the source of sparse visual keypoints, we used OpenVINS \cite{openvins} in the inverse-depth mode, without loop closures.


\subsection{Real-World UAV Monocular-Inertial Exploration}
\label{sec:real_exper}
The most notable real-world experiment is illustrated in TODO-IMG-REF
In this experiment, the UAV explored up to $60m$ forward around the side of an abandoned farmhouse fully autonomously in an $8\text{min}$ mission.
In this experiment, we bounded the area for generating exploration goals to a 70x10x8 bounding box, and the UAV explored all the available space in this region.
The UAV successfully avoided and mapped all the debris, bushes and the house walls in the area, and after exploring all available goals, it autonomously returned to the starting position.
Also note in TODO-IMG-REF that the UAV explored a considerable amount of space in the open field to the left of the house.
This was made possible by our approach to safe estimation of free space in the absence of triangulated SLAM keypoints, described in \autoref{sec:polyhedron}.
% The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and mapper modules.

We have tested the proposed approach extensively in a variety of other real-world environments, including a forest path, a tree in a wide open field, and around a large house.
Videos from all real-world experiments are available at TODO-YOUTUBE-LINK. 
Some of these experiments required the safety pilot to take control due to an important limitation of the method that we encountered during testing --- 
since the visual SLAM tracks and triangulates visual key\textit{points}, visual lines (caused by e.g. thin tree branches or painted manmade poles) are not sensed in our mapping pipeline.
This caused the UAV to nearly crash into tree crowns, which did not have many leaves at that time, multiple times.
This limitation could be resolved by integrating short-range depth sensors as well, even ultrasound sensors could potentially be fused into the map.
Additionally, the visual SLAM could be modified to estimate 3D poses of lines as well, as for example in TODO, but we leave this for future work.

% \begin{figure}[!htb]
%   % \includegraphics[width=8.5cm]{fig/exper1.png}
%   \includegraphics[width=8.5cm]{fig/exper4.png}
%   % \includegraphics[width=8.5cm]{fig/exper3.png}
%   % \includegraphics[width=8.5cm]{fig/exper4.png}
%   \centering
%   \caption{TEMPORARY IMG - Exploration experiment visualization}
%   \label{fig:exper_real}
% \end{figure}

\begin{figure}[ht]
  \definecolor{green}{HTML}{3FA63F}
  \definecolor{red}{HTML}{EB4E3F}
  \definecolor{blue}{HTML}{52B8E8}
  \newcommand{\greendot}{\raisebox{0pt}{\tikz{\draw[green,fill=green] (0,0) circle (2.0pt);}}}
  \newcommand{\reddot}{\raisebox{0pt}{\tikz{\draw[red,fill=red] (0,0) circle (2.0pt);}}}
  \newcommand{\bluedot}{\raisebox{0pt}{\tikz{\draw[blue,fill=blue] (0,0) circle (2.0pt);}}}
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t126.png}};
      % \draw [latex-latex](3.2,0.9) -- (3.2,6.8);
      % \node[align=center] at (3.6, 4) {\scriptsize \color{black}\SI{70}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.025) {\footnotesize \color{black}$t=126\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t206.png}};
     %    \begin{scope}[x={(a.south east)},y={(a.north west)}]
     %      \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=206\text{s}$};
     %    \end{scope}
     %  \end{tikzpicture}
     % \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t370.png}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.025) {\footnotesize \color{black}$t=370\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t516.png}};
      \draw [latex-latex](4.05,0.4) -- (4.05,7.3);
      \node[align=center] at (3.6, 3.4) {\scriptsize \color{black}\SI{80}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.025) {\footnotesize \color{black}$t=516\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=90, origin=c, width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/house.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=180, origin=c, width=1.0\linewidth, trim={{0.15\width} {0.3\height} {0.2\width} {0.0\height}}, clip=true]{fig/house.png}};
        % \begin{scope}[x={(a.south east)},y={(a.north west)}]
        %   \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=516\text{s}$};
        % \end{scope}
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    TODO
  }
  \label{fig:exper_real}
\end{figure}


\subsection{Large-Scale Exploration in Simulation}
\label{sec:sim_exper}
% In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.
In this experiment, we showcase the capabilities of our approach in a simulated TODOxTODO wide urban area.
The experiment was conducted using the Gazebo simulator, with essentially the same UAV and sensory setup.
\autoref{fig:exper_sim} shows the timelapse of an example exploration mission.
After running the exploration mission 20 times, the UAV managed to explore the entire environment in 18/20 runs.
In the remaining 2 runs, the UAV crashed either into a large featureless metal pole or the roof of a gazebo where the texture only consists of lines.
The crashes were caused by the same limitation as discussed in \autoref{sec:real_exper}.
Aside from this limitation, the mapping and exploration can be said to work robustly in static environments.
Note also that in the experiment shown in \autoref{fig:exper_sim}, the UAV explored up on the roofs of the buildings when it was able to use the large towers to estimate depth and create enough free space for safely moving up to the roof.

\begin{figure}[!htb]
  \includegraphics[width=8.5cm]{fig/sim_exper_gazebo.png}
  \includegraphics[width=8.5cm]{fig/sim_exper_end.png}
  % \includegraphics[width=8.5cm]{fig/exper3.png}
  % \includegraphics[width=8.5cm]{fig/exper4.png}
  \centering
  % \caption{TEMPORARY IMG - Exploration experiment visualization}
  \caption{The resulting map (bottom) after 20min of exploring an urban environment (top) in simulation, explained in more detail in TODO-REF. Black arrows indicate visited frontier viewpoints, frontiers are shown as purple points. Notice that the UAV also went to explore up on the rooftops, where it could see the metal towers.}
  \label{fig:exper_sim}
\end{figure}

\subsection{Runtime analysis}
\label{sec:runtimes}
% Here we analyze the runtimes of this method, and TODO.
To provide more insight into the real-time capability of the proposed mapping method, we measured the runtimes of individual parts of the map update iterations during one of the simulated exploration missions described in \autoref{sec:sim_exper}. 
The resulting graph is shown in \autoref{fig:runtimes}.
In that mission, the UAV explored the entire TODOxTODO environment.
Thanks to only updating the map near the UAV, the update time remains relatively bounded, even when the visible free-space polyhedron is large.


\begin{figure}[!htb]
  % \includegraphics[width=8.5cm]{}
  \adjincludegraphics[width=9cm, trim={{0.05\width} {0.00\height} {0.05\width} {0.00\height}}, clip=true]{fig/smap_runtimes_tmp.pdf}
  \centering
  \caption{Runtime analysis of parts of the map update detailed in \autoref{sec:map_building}. TODO-MEANINGS OF PARTS}
  \label{fig:runtimes}
\end{figure}



\section{CONCLUSION}
% In this paper, we presented a novel sphere-based spatial mapping method 
% % together with a robust monocular inverse-depth estimator,
% using sparse point cloud data from monocular visual sensors,
% and demonstrated how it can enable any UAV equipped with a single camera and any form of odometry to quickly and safely navigate and explore unknown, unstructured 3D environments.
% We have demonstrated, through testing in simulation and in the real world, that our new mapping approach and spatial representation enable safe, large-scale autonomous navigation and exploration on UAVs without traditionally used expensive range sensors.

The spatial representation proposed in this paper, consisting of spheres and points, offers an alternative to the traditionally used grid-based representations.
We have presented a method of constructing this representation on-board a UAV, using only the keypoints from sparse monocular SLAM as input depth information.
% We have experimentally demonstrated how this new representation allows a UAV to explore and navigate large-scale outdoor environments in the real-world.
We have discussed and experimentally demonstrated how this new representation enables 3D exploration and navigation in real-world outdoor conditions on a UAV equipped with only a single camera and IMU as its sensors.
% Trough extensive experimental evaluation, we have showed the feasibility of doing pose estimation, autonomous mapping, and exploration on-board a single UAV equipped with a only a monocular camera and IMU as the used sensors.

In future work, we intend to explore the potential of the proposed representation for multimodal systems -- e.g. by intelligently fusing points and spheres based on the modality they were created from.

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word acknowledgment in America is without an e after the g. Avoid the stilted expression, One of us (R. B. G.) thanks . . .  Instead, try R. B. G. thanks. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, Synthetic structure of industrial plastics (Book style with paper title and editor), 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 1564.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, An approach to graphs of linear forms (Unpublished work style), unpublished.
% \bibitem{c5} E. H. Miller, A note on reflector arrays (Periodical styleAccepted for publication), IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleSubmitted for publication), IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style), IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, Infrared navigationPart I: An assessment of feasibility (Periodical style), IEEE Trans. Electron Devices, vol. ED-11, pp. 3439, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, A clustering technique for digital communications channel equalization using radial basis function networks, IEEE Trans. Neural Networks, vol. 4, pp. 570578, July 1993.
% \bibitem{c12} R. W. Lucky, Automatic equalization for digital communication, Bell Syst. Tech. J., vol. 44, no. 4, pp. 547588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, On the compatibility of adaptive controllers (Published Conference Proceedings style), in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 816.
% \bibitem{c14} G. R. Faulhaber, Design of service systems with priority reservation, in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 38.
% \bibitem{c15} W. D. Doyle, Magnetization reversal in films with biaxial anisotropy, in 1987 Proc. INTERMAG Conf., pp. 2.2-12.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, Radio noise currents n short sections on bundle conductors (Presented Conference Paper style), presented at the IEEE Summer power Meeting, Dallas, TX, June 2227, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, An analysis of surface-detected EMG as an amplitude-modulated noise, presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, Narrow-band analyzer (Thesis or Dissertation style), Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, Parametric study of thermal and chemical nonequilibrium nozzle flow, M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, Nonlinear resonant circuit devices (Patent style), U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
