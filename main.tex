%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\todo}[1]{{\hypersetup{allcolors=red}{\color{red} {#1}}}}
\newcommand{\todocaption}[1]{{\color{red} {#1}}}

% \newcommand{\vk}[1]{#1}
% \newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
% Monocular Sphere-Based Occupancy Mapping and Large-Scale Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Sparse Monocular SLAM for Robust Large-Scale 3D Exploration on Inexpensive UAVs}

% FINAL TWO
% Sphere-Based Mapping using Sparse Monocular SLAM Points for Robust Large-Scale 3D Exploration on Inexpensive UAVs}
% Monocular-Inertial Exploration of Large-Scale Outdoor Environments using Sparse SLAM Keypoints on an Inexpensive UAV}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Unstructured Environments using Sparse Visual SLAM}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by UAVs}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by Low-Cost UAVs}
% Monocular-Inertial Sphere-Based Mapping for a UAV Exploring Large-Scale Unstructured 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale Unstructured 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSphere: Monocular-Inertial Sphere-Based Mapping for UAV Exploration in Large-Scale 3D Environments}
% MonoSpheres: Monocular-Inertial Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSpheres: Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSpheres: Direct Sphere-Based Mapping for Large-Scale Monocular-Inertial UAV Exploration}
% MonoSpheres: Monocular-Inertial Sphere-Based Mapping for Large-Scale UAV Exploration}
% MonoSpheres: Sphere-Based Mapping with Sparse Depth for Large-Scale Monocular-Inertial 3D Exploration}
MonoSpheres: Large-Scale Monocular 3D Exploration using Direct Sphere-Based Mapping}
% MonoSphere: Monocular-Inertial Mapping and Exploration for UAV Exploration in Large-Scale 3D Environments}
% Monocular-Inertial 3D Exploration of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Interital Sphere-Based Mapping for Large-Scale UAV Exploration}

% % PolySphereMap - Occupancy Mapping using Sparse Monocular SLAM for Large-Scale 3D Exploration on Inexpensive UAVs}
% PolySphereMap - Mapping and Exploring Large-Scale 3D Environments using a Monocular Camera on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  % ------ VAR 1 WITHOUT MOTIVATION AND PARAGRAPH
  We present a novel spatial mapping approach that enables UAVs to explore large-scale 3D environments and requires only sparse pointclouds and pose estimates generated by monocular-inertial SLAM as input. 
  % Instead of traditionally casting rays of free space towards obstacle points to build an occupancy grid-based representation, our method inscribes free-space spheres inside a polyhedron constructed around the camera's position and the currently visible triangulated visual keypoints at each iteration. 
  Our method inscribes free-space spheres inside a polyhedron constructed around the camera's position and the visible SLAM points with low position covariance,
  instead of traditionally casting rays of free space towards the points to build an occupancy map. 
  % Our method builds a sphere-based map by inscribing free-space spheres inside a polyhedron constructed around the camera's position and the visible SLAM points with low position covariance.
  Intersecting spheres form a topometric map suitable for rapid safety-aware planning.  
  We also present a scheme for safely sampling additional free space in textureless areas where depth estimation is challenging, and show how this enables navigation in outdoor open-space areas. 
  Furthermore, we present a perception-aware exploration approach that is enabled by the proposed mapping method and explicitly handles the problematic properties of monocular depth estimation. 
  % The proposed methods are, to the authors' best knowledge, the first to achieve autonomous exploration in both indoor and outdoor unstructured 3D environments at the presented scale of up to \todo{100x100m}, using only a single monocular camera and an IMU onboard a UAV. 
We test the capabilities of our method in several real-world and simulated environments. 
  To the best of the authors' knowledge, the proposed method is the first to achieve both indoor and outdoor 3D monocular exploration in environments up to 170x110m in scale.
  % The proposed method is, to the authors' best knowledge, the first to achieve monocular exploration in real-world outdoor unstructured 3D environments.
  % In real-world and simulated missions, our method consistently explores both indoor and outdoor environments spanning up to 100x100m in scale.
  % In addition, our experimental results in simulation show the proposed methods achieve both indoor/outdoor
  We open-source the code of the proposed method to provide safety-aware 3D navigation capabilities to any vision-based UAV operating in unknown environments.

Code--- %\href{
In multimedia materials

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
In recent years, significant advances have been presented in mobile robot autonomy, enabling robot systems to explore unknown 3D environments that span kilometers in scale \cite{darpa_cerberus_wins, beneath}. 
% These advances are critical for enabling deployment of UAVs in real-world, unknown environments, where global navigation satellite systems (GNSS) are partially or completely unavailable.
% \todo{These advances are critical for the deployment of UAVs in real-world missions, such as in urban areas after an earthquake, a forest during a wildfire, or exploring caves on other planets.}
% \todo{These advances are critical for allowing UAVs to be deployed for search-and-rescue missions in urban areas after natural disasters, monitoring wildfires, or mapping caves on other planets.}
These advances are critical for allowing the deployment of uncrewed aerial vehicles (UAVs) in real-world applications such as search-and-rescue, wildfire response or planetary exploration.
% Thus, we are getting closer to swarms of UAVs that quickly create a map of an urban disaster site or a forest on fire for rescue teams.
% Thus, we could soon see swarms of UAVs that assist rescue teams by quickly mapping an urban disaster site or a forest on fire.
% This research could enable for example swarms of autonomous UAVs that assist rescue teams by quickly searching for survivors during natural disasters such as floods, earthquakes or wildfires. 
% Such environments include natural disaster sites 
% However, the state-of-the-art mapping and exploration methods depend heavily on dense range sensors (lidars - heavy, depth cameras - shortrange, stereo cameras - expensive, not available in thermal).
% However, state-of-the-art mapping and exploration methods depend on dense range sensors such as LiDARs or depth cameras.
However, the mapping and exploration methods used for autonomy in unknown environments currently rely on dense range sensors, such as LiDARs or depth cameras.
% , for building 3D maps for exploration and path planning.
Such sensors are heavy, expensive, and thus severely limit the affordability and flight time of autonomous UAVs.
% Monocular cameras and inertial measurement units (IMUs), on the other hand, are abundant in consumer electronics and vastly more commercially available.
% Enabling UAVs to autonomously map and navigate unknown environments using only such light-weight and inexpensive sensors could significantly lower the cost of many real world UAV applications.
Monocular cameras and inertial measurement units (IMUs), on the other hand, are orders of magnitude lighter and less expensive.
% Enabling UAVs to autonomously map and navigate unknown environments using only such sensors would significantly lower the cost of many real-world UAV applications.

% Navigating an unknown 3D environment using only a monocular camera and IMU is, however, still a challenging research problem.
% Exploring an unknown environment using only a monocular camera and IMU is, however, still a largely unsolved research problem.
% The existing approaches present success only at the scale of one or two indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav, cnn_explo_singleroom}, or outdoor, but with severely limiting assumptions on the environmental structure.
Monocular exploration of unstructured 3D environments has thus far been presented only at the scale of a few indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav, cnn_explo_singleroom}.
% While some works have addressed outdoor monocular exploration, they are restricted to a specific environmental structure, (e.g. corridors only \cite{corridors_mono_nav_2009} or involving a single object without additional obstacles \cite{information_driven_bbx_mapping}).
% We hypothesize that the main challenge lies in correctly estimating navigable space, since monocular depth estimation methods are not yet fully reliable and cannot simply be used in the same way as dense range sensors.
% We hypothesize that the main challenge lies in correctly estimating navigable space, since monocular depth estimation methods are not yet fully reliable, and focus on estimating free space in a different way than with dense sensors.
% These methods use sparse pointclouds from monocular SLAM or dense pointclouds from a monocular depth neural network to build a grid-based map in the same way as with dense range sensors.
% The existing approaches use monocular depth estimates to build a grid-based map in the same way as with dense range sensors \cite{octomap, voxblox}.
%
The existing monocular mapping approaches \cite{from_monoslam_to_explo, los_maps} use the same core approach which is \todo{used with dense range sensors:}
They build a grid-based map where cells are set as free if they are passed by rays that go from the sensor towards points that likely lie on some physical surface. 
% as is done as with dense range sensors.
% The existing mapping approaches cast rays of free space towards estimated 3D obstacle points to build a grid-based map in the same way as with dense range sensors.
%
% This mapping approach, however, leads to several problems that prohibit large-scale exploration, discussed in detail in \autoref{sec:related_works}.
% This mapping approach, however, leads to several problems that prohibit large-scale exploration (see \autoref{sec:related_works}).
% However, casting rays towards sparse pointclouds given by monocular SLAM can lead to large gaps in free space, prohibiting planning, while using depth neural networks can lead to crashes due to depth inaccuracies.
Some methods \cite{from_monoslam_to_explo, los_maps} cast rays towards sparse pointclouds generated by monocular SLAM, which leads to large gaps in free space, prohibiting planning in texture-sparse areas.
Other methods \cite{simon2023mononav, mono_airsim_nav_2025} use depth neural networks to obtain denser pointclouds. 
As confirmed by the authors, however, the deep neural depth estimates currently lack the reliablity for safe deployment and can often lead to the UAV crashing.
% but this often leads to crashes,
% since such networks are not yet very reliable.
% This can lead to gaps in free space \cite{from_monoslam_to_explo}, which prevent path planning, or wrongly estimated free, which can lead to crashes \cite{simon2023mononav}.

% The autonomous UAV systems operating in unknown environments documented in recent literature \cite{darpa_cerberus_wins, beneath} usually construct an occupancy grid \cite{occupancy_moravec}, such as the efficient octree-based implementation of OctoMap \cite{octomap}, and use it as the base representation for planning, exploration and constructing higher-level spatial abstractions \cite{topomap, spheremap}.
% However, building a detailed occupancy grid requires dense pointclouds of range measurements due to the raycasting nature of occupancy grid updates.
% which is to cast rays towards the measurement points and update the occupancy of traversed cells.
% TODO - check if all

% Building a map for exploration using only a monocular camera for depth sensing is challenging for many reasons \todo{vagni}, and has so far been demonstrated only in small-scale indoor environments.


% INTRO IMG%%{
% \begin{figure}[!t]
%   % \includegraphics[width=8.5cm]{fig/smap2.png}
%   % \includegraphics[width=8.5cm]{fig/explor_forest2.png}
%   % \includegraphics[width=8.5cm]{fig/intro_poly.png}
%     % \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.05\height} {0.05\width} {0.1\height}}, clip=true]{fig/intro_poly.png}
%     \adjincludegraphics[width=8.5cm, trim={{0.0\width} {0.00\height} {0.00\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}
%   \centering
%   % \caption{Visualization of the proposed map representation and how it is updated. The green polyhedron is an estimate of currently visible free space and is formed by the visible triangulated keypoints and the camera's position (axes). The mapped spheres (green), obstacle points (red) and frontier points (purple) are updated by the polyhedron and visible keypoints, as described in \autoref{sec:map_building}}
%   \caption{Visualization of the proposed map representation as described in \autoref{sec:map_building}}
%   \label{fig:smap_intro}
% \end{figure}

\begin{figure}[!t]
  \centering
     \begin{subfigure}[b]{0.98\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.1\height} {0.0\width} {0.1\height}}, clip=true]{fig/sar_mono_dji_2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_912_cam.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}};
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    % Illustration of the proposed method in a real-world outdoor environment with large open areas. 
    Illustration of the proposed approach. 
    The mapping pipeline uses information from sparse monocular-inertial SLAM (left) running onboard the UAV (top) to construct a large-scale map consisting of free-space spheres and obstacle points (right) for safety-aware exploration.
  }
  % \label{fig:mapping_pipeline}
  \label{fig:smap_intro}
\end{figure}
% %%}


% \todo{in contrast?}In this paper, we present a novel approach to 3D mapping of unstructured large-scale environments using only a monocular camera and IMU. 
% Our method utilizes a sparse pointcloud of 3D points produced by monocular-inertial SLAM, which is already a necessary part of many vision-based UAV systems.
% \todo{The main idea is...}
The main idea of our paper is that instead of traditionally tracing rays through a grid towards measured points and updating occupancy values of passed voxels \cite{octomap, voxblox, ufomap}, we construct a polyhedron of approximate visible free space at each iteration and inscribe spheres of free space inside that polyhedron.
Intersecting spheres are connected together to form a graph-of-spheres map representation that has been shown to allow rapid and safety-aware planning \cite{spheregraph, spheremap, bubbleplanner}.
This map is built online, directly from the sparse pointclouds, without requiring any grid-based map to be built first.
% Under assumptions that hold in the majority of real-world environments (see \autoref{sec:fake}), this approach accurately maps free space even in low-texture areas where the traditional raytracing approach might not initialize enough free space to allow flight. 

Combined with the sampling scheme described in \autoref{sec:fake}, this approach can accurately estimate free space even in low-texture areas where the traditional raytracing approach might not initialize sufficient free space to allow flight, as shown in our experimental results in \autoref{sec:sim_exper}. 
% and yields a topological graph that allows rapid safety-aware planning.}
By storing obstacles as a set of geometric primitives, we can also assign additional metrics to each obstacle, such as measurement distance. 
% or, in future work, the modality of its origin.
This allows for rule-based measurement fusion, which is critical for dealing with problems such as occlusions in monocular depth estimation, as we explain in \autoref{sec:distance-based}.
In several experiments, we show how this mapping approach, combined with perception-aware planning, enables exploration of large-scale unstructured areas which feature both textureless wide open spaces and cluttered passages.
% TODO - is neccesary?
% In this representation, we assign additional metrics
% This representation allows for fast and safety-aware planning \cite{spheremap}.
% In multiple experiments, we demonstrate how the proposed mapping method enables autonomous exploration in real-world, large-scale outdoor environments on a UAV using only a single monocular camera and IMU, which, to the authors' best knowledge, has not yet been presented in literature.
% This paper brings the following contributions:

% To enable safety-aware monocular vision-based autonomous exploration in unknown, unstructured 3D environments of large scales, our paper brings the following contributions:
% This paper brings the following contributions:
As a step towards low-cost vision-based UAVs safely and efficiently navigating any large-scale, unknown 3D environments, our paper brings the following contributions:

% Our paper brings the following contributions:
% To enable autonomous navigation in unstructured 3D environments of large scales for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
% To enable autonomous navigation in general environments for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  % \item A novel 3D mapping approach that constructs a sphere-based map directly from sparse pointclouds obtained by monocular-inertial SLAM, without needing any intermediate occupancy map, mapping both open-space and cluttered areas.
  \item A novel conceptual approach to 3D mapping that constructs a sphere-based map for safety-aware navigation using only sparse pointclouds and pose estimates from monocular-inertial SLAM as input,
    % with less conservative free-space estimation than traditional ray-casting methods.
    with the ability to map sufficient free space for flight even in texture-sparse open-space areas.
    % , without needing any intermediate occupancy map.
    % , mapping both open-space and cluttered areas.
  \item A perception-aware exploration approach, enabled by the proposed mapping method, that achieves large-scale 3D outdoor exploration on a UAV equipped with only a monocular camera and IMU, 
      validated in real-world conditions.
    \item Open-sourced code for the proposed methods, along with example simulation scripts for replicating our results, providing a benchmark for new monocular exploration methods.
\end{enumerate}


\section{Related Works}
\label{sec:related_works}

% The environment representation similar to our previous work \cite{spheremap} -- in \cite{spheremap}, we also built a graph of intersecting free-space spheres, but 
In the field of robotic exploration, environments are most often represented by a grid-based map of occupied, unknown, and free space, either in the form of a signed distance field \cite{voxblox} or an occupancy grid \cite{occupancy_moravec, octomap}.
These grid-based representations are the backbone for nearly all of robotic path planning, exploration \cite{darpa_cerberus_wins, beneath} and higher-level spatial abstraction methods \cite{topomap, skeletons, spheregraph, spheremap}.
However, these maps rely on dense distance sensors, such as LiDARs or depth cameras, to build an accurate map of free and occupied space.

% , has several drawbacks for usage on UAVs without dense range sensors in varying-scale environments.

% Firstly, both ESDFs and occupancy grids are updated by casting rays towards measured 3D points and updating the values of traversed voxels.
% With sensors that produce dense pointclouds, such as LiDARs or depth cameras, this approach accurately maps the true free space.
% However, using this technique with sparse distance measurements can lead to large gaps in the map, as documented in \cite{from_monoslam_to_explo}.
A monocular camera does not produce dense distance measurements by itself and thus it is difficult to apply the vast amount of available approaches for robot autonomy on monocular systems.
A recent line of research tries to bridge this gap by turning a monocular camera into a dense depth sensor using deep learning. 
% The authors of \cite{simon2023mononav, cnn_explo_singleroom} use deep-learning-based single image depth estimation models to obtain dense point clouds from a monocular camera, essentially turning it into a depth camera.
The authors of \cite{simon2023mononav, cnn_explo_singleroom} have demonstrated using such depth estimation models combined with traditional navigation approaches.
However, the authors demonstrate autonomy only at the scale of a single room, while also encountering collisions and map deformations due to depth estimation errors.
The authors of \cite{mono_airsim_nav_2025} present navigation with obstacle avoidance using pretrained and finetuned monocular depth models in larger outdoor environments in simulation, but also note a high collision and navigation failure rate due to the same problems.
As further discussed in recent surveys \cite{deep_mono_depth_2021, deep_mono_depth_2019}, current depth estimation models still struggle to provide real-time onboard performance and reliable generalization in domains that the model was not trained on.
In addition, they require a GPU which significantly raises the cost and weight of a UAV.
% Thus, in this paper, we chose not to go with learning-based depth estimation.
% In addition, to the fact that a GPU significantly raises the cost and weight of a UAV, this 

A different line of research focuses on taking sparse 3D points estimated by visual SLAM and working with those as the only depth measurements.
Most of this research has been on reactive obstacle avoidance \cite{flame, avoidance_mono1} or exploration with strong assumptions on the environment structure (e.g. corridors only \cite{corridors_mono_nav_2009} or assuming a single object with no other obstacles \cite{information_driven_bbx_mapping}).
% For monocular exploration in unstructured environments, the authors of \cite{from_monoslam_to_explo, los_maps} proposed taking the points from monocular SLAM, treating them as sparse distance measurements, and building grid-based maps in the same way as with dense pointclouds.
% For monocular exploration in unstructured environments, the authors of \cite{from_monoslam_to_explo, los_maps} proposed taking the points from monocular SLAM, treating them as sparse distance measurements, and building grid-based maps in the same way as with dense pointclouds.
For general monocular exploration in unstructured environments, the authors of \cite{from_monoslam_to_explo, los_maps} propose building grid-based maps by tracing rays of free space towards the monocular SLAM points. 
% A few these works \cite{from_monoslam_to_explo, los_maps} have demonstrated autonomous exploration in unstructured, indoor environments, 
% As documented in \cite{from_monoslam_to_explo} and as shown in [\todo{TODO-FIG}], this approach causes gaps to appear in the free space of the map, whenever the environment is not texture-rich.
As documented in \cite{from_monoslam_to_explo}, this approach causes gaps to appear in the free space of the map when the environment is not texture-rich.
The authors of \cite{from_monoslam_to_explo} further present an exploration method specifically to cover these gaps and demonstrate an increase in mapped volume.
\todo{However, the authors show this exploration approach to only work at the scale of a single room.}
% However, this only reduces the gaps, as this mapping method can never estimate free space near texture-sparse areas, and the authors show this exploration approach to only work at the scale of a single room.
Most importantly though, this mapping approach cannot estimate free space in textureless areas, especially outdoor (as shown in \autoref{sec:sim_exper}).
% \todo{This technique accurately covers free space when dense input pointclouds are available, such as from a LiDAR or depth camera. 
% If the voxel size is too small and/or the data is sparse, gaps will appear in the free space.
% In large open areas where many features are too far to be accurately triangulated, this can lead to no free space being initiated except near the ground, causing the robot to not explore into the open space.
% }.
% Existing works address this issue in multiple ways:
% Treating sparse points from monocular SLAM as 
% The existing works on using using monocular SLAM points for 3D mapping treat the SLAM points as range measurements and perform the raycasting method as well. 
% With only sparse 
% This becomes problematic when the available measured points are very sparse, such as points tracked by sparse monocular SLAM.
% Some works were motivated to overcome this limitation -- 
% In \cite{from_monoslam_to_explo}, the authors take points from semi-dense monocular SLAM and raycast towards them to build an OctoMap \cite{octomap} occupancy grid.
% However, they only demonstrate their local exploration approach built on top of this mapping pipeline to work inside a single room with textured walls.
% The authors of \cite{los_maps} work around the gap issue by setting the voxel size to be relatively larger compared to the UAV size, which causes more voxels to be passed by the sparse rays towards measured points.
% They present exploration on the scale of several indoor rooms and in simulation only. 

The authors of \cite{los_maps} also trace rays towards sparse SLAM points, and present exploration on the scale of multiple indoor rooms in simulation.
They work around the gap issue by setting the map voxel size to be relatively large compared to the UAV size.
A larger voxel size causes a higher percentage of voxels to be passed by the sparse rays towards measured points, reducing the gaps.
% This approach can however only work in constant-scale environments.
% This approach can however only work in constant-scale environments without narrow corridors.
% Such large voxel size, however, can cause narrow pathways to appear untraversible if the map is not perfectly aligned.

Nevertheless, grid-based maps built by raytracing suffer from the problem of choosing the smallest voxel resolution in multi-scale environments.
If the voxels are too big, then narrow passages can wrongly appear as untraversable or even completely closed, severely limiting the robot's navigation. 
If the minimum voxel size is too small compared to the density of data, gaps can appear in free space due to rays passing only a thin strip of voxels, especially with sparse depth data.
% Moreover, high map resolution requires large amounts of raycasts which can go beyond the computational power available on the onboard computers.
A different representation and mapping approach is thus desirable for these conditions.
% \todo{rict ze sfery maj min radius, ale nevadi to scalovani, neni to to samy co grid resolution}

% We argue that using a smaller voxel size would not solve this issue, since that would leave many gaps in the free space.
% and in [ROOM-DENSEDEPTH], the authors essentially perform dense pixel-based estimation with known poses from sparse SLAM, which works in well-illuminated rooms but may have problems in larger-scale or texture-poor environments.
% Second, even though memory-efficient octree-based representations of 3D grids are available, most notably the OctoMap library \cite{octomap}, a fundamental problem of grids is that the smallest-voxel size must be set by the user.

% This is the second fundamental problem of grid-based representations -- the smallest-voxel size must be set by the user.
% If this is set as too large, narrow passages will not be captured in the map, and if it is too small, the map will need large amounts of raycasts to set all the cells' occupancy values, which is computationally expensive.
% In our previous work \cite{spheremap}, we have shown how a graph of intersecting spheres can be built out of an occupancy grid, and that this representation allows path planning that has explicit information about distance to obstacles, in addition to being orders of magnitude faster than on an occupancy grid.

In recent work \cite{spheregraph, spheremap, bubbleplanner}, it has been shown that using a graph of intersecting free-space spheres, rather than a grid-based representation, is highly suitable for rapid scale-invariant safety-aware path planning.
% \todo{TODO - address octree repre vs spheremap, also smallest sphere size}
% A graph-of-spheres representation efficiently describes large spaces with a few large spheres, while describing narrow passages in more detail by densely-packed smaller spheres.
A graph-of-spheres representation efficiently describes large spaces with a few large spheres, while describing narrow passages in more detail by densely-packed smaller spheres.
In addition, the distance to obstacles is stored as the radius of each sphere, and is immediately available for safety-aware planning.
Thus, this representation combines the scalability of octree-based maps \cite{octomap} with the distance information availability from distance field maps \cite{voxblox}.
% In previous work \cite{spheremap, spheregraph, bubbleplanner}, it has been shown that representing free space by a graph of intersecting spheres \todo{solves this problem by} can efficiently describe large-space while keeping detailed information in narrow spaces.
% In \cite{spheremap}, we also demonstrated that this representation allows path planning that has explicit information about distance to obstacles, in addition to allowing orders of magnitude faster path planning than on an occupancy grid.
In this paper, we represent free space using spheres as well, but with two major differences: 
% In this paper, we represent the free space using spheres as well, but also add the obstacle points into the map and use them for constraining the sphere radii, but with \todo{two major differences 

  Firstly, the methods presented in \cite{spheregraph, spheremap , bubbleplanner} require building an intermediate local occupancy grid for computing the sphere-based map.
    This can be computationally expensive.
    Secondly, all methods for building a grid-based occupancy map require the data to be from dense depth sensors (LiDARs, RGB-D cameras, ...). 
    In this paper, we build the graph of spheres \textit{directly} out of input  pointclouds without building any intermediate occupancy grid, while also 
    requiring only \textit{sparse} pointclouds from a monocular camera running visual SLAM.

    % Firstly, all methods for building a map for 3D exploration require the data to be from dense depth sensors (LiDARs, RGB-D cameras, etc.). 
  % Secondly, the methods for constructing a graph-of-spheres topological map presented in \cite{spheremap, spheregraph, bubbleplanner} require first building an intermediate local occupancy grid.
    % This can be computationally expensive.
    % In this paper, we build the graph of spheres using only \textit{sparse} pointclouds from a monocular camera running visual SLAM, without building any intermediate occupancy grid.

    % \todo{in addition to only describing free space...}
  % Secondly, the representation proposed in this paper stores previously seen SLAM points as obstacle points and remembers their measurement distance. 
  %   These points are important for correctly updating free space when previously seen obstacles are momentarily not detected by the visual SLAM (see \ref{sec:distance-based}).
    % \todo{TIE IN OR END SECTION}
        % to overcome the challenge of incorrect measurements at larger distances.
% \end{itemize}

% Vision-based UAV autonomous exploration has not yet reached the levels of autonomy as when using dense depth sensors, and remains a challenge, as documented in recent surveys \cite{drones_survey}, but some progress has been made in small-scale indoor deployment and by using stereo cameras.
% TODO - mention reactive only
% In \cite{explor_stereo}, the authors presented the first ever vision-based UAV system that can explore and build a map (a 3D occupancy grid) on a UAV equipped with stereo cameras and also a supporting downward-facing optical flow sensor for better motion estimation.
% The authors used dense pointclouds coming from the stereo cameras for building an OctoMap \cite{octomap} occupancy representation in the same way as with depth data from an RGB-D camera or LiDAR.
% This allowed them to use similar exploration techniques as on systems with such sensors.

% For UAVs equipped with only a monocular camera, to the authors' best knowledge, all the existing works have demonstrated autonomous 3D mapping and exploration on a UAV only indoor and at the small scale of a single room \cite{from_monoslam_to_explo, cnn_explo_singleroom} or a few simple rooms \cite{los_maps, simon2023mononav}.
% Monocular SLAM, both sparse and dense, cannot produce measurement points on textureless areas, and the existing approaches either accept that the data from visual SLAM can be very sparse, or they use deep-learning-based depth estimation to obtain dense data.

% The authors of \cite{from_monoslam_to_explo, los_maps} argue against using frontier-based methods \cite{paper_frontier_grandpa}, due to the facts that no points will ever be generated on textureless areas, and thus those frontiers would not be uncovered by a monocular camera.
% The authors have presented alternative exploration approaches, but they have only shown them to work in small-scale environments.
% In our exploration approach, thanks to not using raycasting in the mapping pipeline and also blocking explored viewpoints, we show that frontier-based exploration is possible and assures global coverage of the explored environments.

% \todo{TODO - cite corridors!!!}

% In \cite{corridors_mono_nav_2009}, the authors presented a system for monocular UAV navigation and demonstrated autonomous flight and map-building in an office corridor.

% As in \cite {from_monoslam_to_explo, los_maps}, we also use 
% The mapping method proposed in this paper constructs free space in a fundamentally different, less conservative way than in \cite{from_monoslam_to_explo, los_maps}.

% TODO - repeating?
% To build a graph-of-spheres representation directly out of sensory data, one must build the map in a completely different manner than for a grid-based map.
% We propose to interpolate the depth between the sparse SLAM points and sample free-space spheres instead of casting rays of free space.
% Additionally, we present a method for sampling free space in wide open areas where objects are too far to be accurately triangulated. 
% In special cases, this can cause the free space to be overestimated, but it allows the UAV to fly where the sparse raycasting approach \cite{from_monoslam_to_explo, los_maps} would never estimate enough free space to even begin navigation -- specifically in open outdoor areas where there are no reliable visual features in the sky, and thus raycasting towards SLAM points would estimate free-space only near the ground. 

% Our proposed perception-aware exploration strategy, tightly coupled with the mapping method, handles the free-space inaccuracies and achieves safe and complete exploration.
% This sampling approach allows the UAV to fly where the sparse raycasting approach would never estimate enough free space to even begin navigation. 
% Additionally, we show that when this is coupled coupled with a perception-aware path planning and exploration, the UAV can navigate safely even when occasionally overestimating free space.
% We show that when coupled with perception-aware planning and an exploration approach suited for the novel map representation, the UAV can safely and completely explore unknown environments even when occasionally overestimating free space.
% This mapping approach is tightly coupled with perception-aware planning and an exploration approach suited for the novel map representation, which handles the occational overestimation of free space.
% This mapping approach is tightly coupled with perception-aware planning and an exploration approach suited for the novel map representation, which handles the occational overestimation of free space.
% Thus, the proposed method allows safe and complete explore unknown, multi-scale, indoor/outdoor environments, even without dense sensors or GPU-based depth estimation.
% In special cases concerning textureless objects, this can cause the UAV to 
% Compared to all the mentioned approaches to 3D monocular mapping and exploration, we demonstrate that our approach achieves exploration and navigation in 3D using only a monocular camera and an IMU in outdoor, large-scale environments, with all of the computations running on-board the UAV, without requiring a GPU.
% PIPELINE IMG%%{
\begin{figure*}[!htb]
  \def\subfigwidth{0.24\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_968_poly1_A.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_970_poly2_A.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_969_poly1_B.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_971_poly2_B.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_914_right_poly.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_927_right_free2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \caption{
    Illustration of the mapping pipeline. 
    (a) The stable 3D points (red) used by the monocular SLAM are the only input distance measurements. 
    Delaunay triangulation is performed on the input points projected to 2D as a rough depth interpolation. 
    % Additionally, , the TODO-fake points are added to the depth estimation.
    (b) The active assumed-freespace points (blue, see \autoref{sec:fake}) above the ground, where the SLAM cannot accurately localize any points, are added to construct the
    (c) resulting visible free-space polyhedron $\mathcal{P}_f$.
    % (d) Side view of how new spheres are sampled in the polyhedron and added to the others, as well as how new obstacle points are added to the map.
    (d) New spheres are sampled in the polyhedron and added to the graph, and the visible SLAM points are added to the set of obstacle points $\mathcal{X}$.
    % TODO - desc + delaunay w fake fspace vis
  }
  \label{fig:mapping_pipeline}
\end{figure*}
% %%}

\section{Sphere-Based Mapping Using Sparse Visual SLAM Points}
% \section{Direct Sphere-Based Mapping from Sparse Pointclouds}
\label{sec:map_building}


% In this section, we explain how such map can be built directly from the outputs of visual SLAM, without needing any intermediate occupancy grid computation, and how the sparsity and high depth uncertainty of the input pointclouds is handled. 
% In this section, we first explain the two fundamental features of our method for handling the problems of using sparse monocular SLAM keypoints as depth measurements.
% Th
% Then, we describe the process of constructing the map.
% This section describes the proposed mapping approach.

% This section describes the proposed mapping approach.
% In \autoref{sec:structure}, we introduce the novel spatial representation and its properties.
% In \autoref{sec:polyhedron}, we construct an estimated visible free-space polyhedron using motion information and SLAM points available at a given time.
% In \autoref{sec:distance-based}, the free-space polyhedron and keypoint measurement distances are used to determine which points are added or deleted from the map.
% Next, in \autoref{sec:existing_spheres_update}, we describe how the free-space polyhedron is used to update the radii of existing and newly sampled free-space spheres.
% % As the last step, in \autoref{sec:existing_spheres_update}, we update the connections of modified spheres and prune the sphere graph to keep it sparse.
% As the last step, in \autoref{sec:graph_update}, we update the connections of the modified spheres and prune the sphere graph to keep it sparse for rapid path planning.

% \subsection{Map Data Structure}
% \subsection{Free-Sphere and Obstacle-Point Map Representation}
% \subsection{Map Representation and Notation}
% \label{sec:structure}
% We propose to represent free space by a graph of intersecting spheres.
The proposed mapping approach represents free space by a graph of intersecting spheres.
% \todo{kdtree of centers}
% and obstacles as a set of 3D points corresponding to triangulated visual keypoints from visual SLAM.
Each sphere at the $k$-th update iteration has a static center $\mathbf{c}$ and changing radius $r_k$.
The radius $r_k$ represents the distance from $\mathbf{c}$ to the nearest unknown space or obstacle at time step $k$.
% We maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres, and use it for path planning.
To allow rapid path planning, we maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres.
% Furthermore, we maintain a set of obstacle points $\mathbf{X}$, which correspond to the textured surfaces measured by the visual SLAM.
% We represent obstacles by a set of points $\mathcal{X}$, which are .
To represent obstacles, we accumulate the stable (i.e. with low position covariance according to the SLAM) 3D points estimated by the monocular SLAM into a set of points $\mathcal{X}$ with user-defined minimal point-to-point distance.
The obstacle points are updated, merged and removed based on criteria designed specifically for monocular sensing, described in \autoref{sec:distance-based}.
% For simplicity, we do not store the position covariances of the points given by the SLAM.
% Instead, we parametrize the minimum distance $\rho_{min}$ between any two obstacle points.
% Instead, we set a minimum distance $\rho_{min}$ between any two obstacle points as a parameter.
% For simplicity, we parametrize the minimum distance $\rho_{min}$ between any two obstacle points instead of maintaining the position covariance of each SLAM point.
% For simplicity, we do not store the position covariance for each point, but rather we fuse points together up to a user-defined distance $\rho_{obs}$.

% For each obstacle point $\mathbf{x} \in \mathbf{X}$, we store the lowest distance that the point was measured from $d_{\mathbf{X}, \min}$.
% A lower minimal measurement distance of a visual keypoint means that the point is less likely to be noise and we make it more difficult to erase it from the map.
% This is a necessary feature for safe flight when using sparse visual SLAM keypoints as the only depth information, as explained more in \autoref{sec:distance-based}. 
The mapping runs onboard the UAV in real-time, using only the estimated pose and 3D points from monocular-inertial SLAM as its inputs.
In this paper, we use the sparse OpenVINS \cite{openvins} method as the SLAM frontend, but our method could work with any other monocular-inertial SLAM.
A single map update iteration consists of the following steps in order:
\subsection{Constructing the Visible Free-Space Polyhedron}
\label{sec:polyhedron}
The first step is to construct a polyhedron of space that is estimated to contain free space based on the information in the current timestep.
To interpolate depth between the sparse SLAM points, we employ a simplified version of the approach described in FLAME \cite{flame}, where
a precise mesh is constructed from visual keypoint measurements.
In this paper, we care primarily about mapping the free space for navigation purposes, and thus we do not perform the mesh optimization or split the 2D interpolations as in \cite{flame}.

We project the currently tracked visual SLAM points into the image plane and compute their Delaunay triangulation. 
By connecting the points in 3D according to their Delaunay triangulation in 2D, we obtain a mesh $\mathcal{F}_d$ that we call a \textit{depth mesh}, which is used only for the current frame.
We estimate the currently visible free space as the volume between the camera's position $\mathbf{p}_{cam}$ and all points on $\mathcal{F}_d$.
This space is enclosed by connecting all points that lie on the edge of $\mathcal{F}_d$ to the camera's focal point, which forms an \textit{estimated visible free space polyhedron} $\mathcal{P}_f$, illustrated in \autoref{fig:distbased}.

% \subsection{Estimating Free Space in Open Areas}
\subsection{Open-Area Free-Space Sampling (OFS)}
\label{sec:fake}
When moving in open areas, such as a grassy field without vertical obstacles in \autoref{fig:mapping_pipeline}, visual SLAM often tracks only a few keypoints on the ground. 
% This would \todo{normally} cause free space to be estimated only below the UAV and not allow flight across the field.
Using standard free-space estimation methods, this would cause free space to be estimated only below the UAV and not allow flight across the field.
% This causes free space to be estimated only below the UAV and not allow flight across the field.
% Using standard methods of free-space estimation, 
% To allow safe flight in the absence of visual keypoints in front of the UAV, we keep track of a set of \textit{virtual keypoints} in front of the UAV.
% \todo{mention unknown $=$ free?}
% To allow safe flight in open areas, we make the following assumption: 
% To allow safe flight in open areas, we make the following assumption: 
To allow safe flight in open areas, our method estimates additional free space based on the following assumption: 
% \todo{reformulate - consider an area of the image -> fake depth -> but up to some max dist -> see image}
% Consider a virtual keypoint $\mathbf{x}_{vir}$ at time $t$ that has been in the UAV's FoV from a given time $t-x$ in the past up to $t$ 
Consider a virtual point in space $\mathbf{x}_{vir}$ at time $t$ that has not left the camera's FoV from a given time $t-x$ in the past up to $t$ 
(i.e. it could have been tracked by visual SLAM).
% (i.e. it has not left the camera's FOV between $t-x$ and $t$).
Then, if there is a point on the UAV's trajectory between $t-x$ and $t$ such that there would be enough parallax for estimating the distance of $\mathbf{x}_{vir}$ (according to a user-defined threshold), and if the visual SLAM frontend has not estimated any stable 3D point at approximately that position, then $\mathbf{x}_{vir}$ must lie in free space.

This assumption allows estimating significantly more free space than when only tracing rays towards the visual SLAM points localized in 3D.
% This approximation does not hold close to large textureless walls, but works quite well in the vast majority of real-world environments.
% This assumption does not hold close to large textureless objects, and free space could thus be wrongly estimated in the rare cases when such objects occur.
However, it does not hold close to large textureless objects or thin objects where no features are measured by the sparse visual SLAM frontend.
% Therefore, free space could be wrongly estimated in the rare cases when such objects appear in the real world.
Therefore, free space can sometimes be wrongly estimated when such objects appear.
% In future work, this could be solved by adding low-cost, short-range distance sensors (e.g. an ultrasonic sensor), which are otherwise not usable for traditional dense raycast-based 3D mapping.
% This could be solved by adding low-cost, short-range distance sensors (e.g. an ultrasonic sensor), which are otherwise not usable for traditional dense raycast-based 3D mapping, 
% but that would be outside the scope of this paper.
% which we leave for future work.
% but we leave that for future work.
% However, when coupled with perception-aware flight
% However, such features are uncommon in most large-scale real-world environments.
% \todo{tadyten assumption jeste lip specifikovat a hodit mozna na zacatek / na konec k limitations}


% We keep a set of virtual points $\mathbf{x}_vir$ in front of the UAV at all times, and if some of the points become are estimated as free based on the above assumption, 
% We periodically check 
The mapping method periodically checks 
the described condition for a set of virtual points $\mathbf{x}_{vir}$ at a fixed distance in front of the UAV (see \autoref{fig:mapping_pipeline}) at each mapping iteration. 
% This is only done for a few points, as checking the condition for all points in the UAV's FOV would be computationally demanding. 
% The user can select the amount of these points and how far from the camera they lie, based on the amount of textureless areas and thin obstacles in the environment.
% We take the points that fulfill this condition and add them to the creation of the \textit{depth mesh} $\mathbf{F}_d$.
The points that fulfill this condition are added to the creation of the \textit{depth mesh} $\mathcal{F}_d$.
% To further mitigate the depth estimation problem near textureless areas,
% we discard points such points that would fall into the Delaunay triangulation of the true obstacle points, since there we have a better depth estimate from the SLAM points.
% However, we discard points that would fall into the Delaunay triangulation of the visible obstacle points, since there we have a better depth estimate from the SLAM points.
However, we discard points that would fall into the Delaunay triangulation of the visible obstacle points, since there is a more reliable depth estimate from interpolating the SLAM points.
Free space estimated in this manner also cannot erase existing obstacle points in the map.
% This also helps to correctly estimate depth when there is at least a little amount of visual texture near to textureless areas.
% This way, the problem of large featureless objects is mitigated in the majority of cases, when textured objects are visible nearby. 
% This way, we can safely estimate additional freespace, in the case of sufficiently textured wide outdoor areas.
% \todo{tadyten assumption jeste lip specifikovat a hodit mozna na zacatek / na konec k limitations}
% These virtual points are only used for estimating free space, and are not added as obstacles and cannot be used to delete existing points

% The visual SLAM usually has 

\begin{figure}[!h]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/distbased9.pdf}
  \centering
  \caption{Top-down illustration of sphere sampling and map point management: 
  The visible free-space polyhedron $\mathcal{P}_f^{t_2}$ is created using the currently tracked SLAM points (black crosses) and the camera's position $\mathbf{p}_{cam}^{t_2}$ at time $t_2$.
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  The size of a point $\mathbf{x}$ negatively corresponds to the distance $d_{\mathbf{x}}$ that it was measured from.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black), as there are better localized (smaller) points near them.
  The (dashed red) point seen far away from the camera at a previous time $t_1$ now lies in $\mathcal{P}_f^{t_2}$ close to the camera and will thus be deleted as noise.
  % The (dashed red) point will be deleted, as it lies in the free-space polyhedron at $t_2$, and it .
  The (dashed black) two points observed far from the camera at $t_2$ are not added to the map, as there are more precisely localized (red) points close to them.
  % New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  % New spheres are inscribed inside the visible free-space polyhedron $\mathbf{P}_f$.
  % Visible free-space polygon $\mathbf{P}_f$ (-);
  % Newly sampled spheres (O);
  % Triangulated keypoints visible at current frame (X); 
  % Points already in the map (X) added from a previous pose (red). 
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black).
  }
  \label{fig:distbased}
\end{figure}


\subsection{Updating Obstacle Points}
\label{sec:distance-based}
As the next step of the update, we decide which tracked SLAM points to add into the map points $\mathcal{X}$, and which points already in the map to delete.
An important part of the proposed mapping method is that we store the minimum distance from which any point $\mathbf{x}$ has been observed, denoted further as $d_{m, x}$.
% This is due to the fact that the distance uncertainty of any point \todo{grows drastically} with the distance from camera in monocular SLAM \cite{inverse_depth}.
% An apparent problem can arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 20m).
This information is important to solve problems that can arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 20m).
% The visible points corresponding to the wall might not be detected as SLAM keypoints while p.
Primarily, points corresonding to a small obstacle might momentarily not be detected by the visual frotend, while a larger obstacle behind the small one is detected.
% In such situation, the previously mapped points can be momentarily estimated to lie at an incorrect position, or might not be localized by the visual SLAM at all (e.g. due to insufficient parallax), while some other points that lie beyond the original points are triangulated.
% We also cannot simply delete every map point that falls into $\mathbf{P}_f$, because the UAV might be at a distance where the object corresponding to the point is just too far to be seen as a keypoint in the image.
Thus, we cannot simply delete every map point that falls into $\mathcal{P}_f$, because then the original points (small red points in \autoref{fig:distbased}) would be wrongly deleted and assumed as free space.
We solve this problem by deleting any map point $\mathbf{x}$ that falls into $\mathcal{P}_f$ only if
\begin{equation}
  \mathbf{x} \in \mathcal{P}_f \quad \text{and}  \quad |\mathbf{x} - \mathbf{p}_{cam}| < \alpha \cdot d_{m,x},
\end{equation}
where $p_{cam}$ is the position of the camera and $\alpha$ is a paremeter, which is usually set to $1.1$ to account for distance measurements uncertainty.
% Thanks to this condition, when a small obstacle seen previously from a short distance is currently not visible when further away, but objects are visible behind it, that obstacle will not be wrongly deleted from the map.
% where $p_{cam}$ is the position of the camera and $\alpha$ is a paremeter.
% We usually set $\alpha$ to $1.1$ to account for distance measurements uncertainty.
% We usually set $\alpha$ to $1.1$ to allow the mapping to modify points for a while when moving away from them.
% The parameter $\alpha$ could be set to $1$ to allow the mapping to modify points for a while when moving away from them.
% A higher value of $\alpha$ can be set 
The points that lie in the free-space polyhedron $\mathcal{P}_f$ and are not deleted are called \textit{protected points} $\mathcal{X}_p$.
These points are used to constrain sphere radii in the following update step.

Additionally, to prioritize closer measurements, points seen from a closer distance replace points seen from larger distances, if they are measured close enough to them. 
In the same way, new points seen at a large distance are not added to the map, if there are more accurately measured points near them, which is also visualized in \autoref{fig:distbased}. 
We enforce these two rules by deleting previously mapped points only $\mathbf{x}$ if
\begin{equation}
  \exists \mathbf{x}_{prev}: |\mathbf{x} - \mathbf{x}_{prev}| < a \quad \text{and} \quad d_{m,x_{prev}}  < d_{m,x}
\end{equation}
where $\mathbf{x}_{prev}$ is another point in the map with a higher position accuracy and $a$ is the minimal point-to-point distance set by the user.
% \todo{TODO - rewrite as sort?}

\subsection{Updating Existing Spheres}
\label{sec:existing_spheres_update}
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the protected points $\mathbf{X}_{p}$.
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the input points $\mathbf{X}_{in}$.
Next, we recompute the radii of spheres that could be updated by $\mathcal{P}_f$ or the input points $\mathcal{X}_{in}$.
To bound the update time of this step, we check the largest sphere radius $r_{max}$ in the map, which allows us to quickly filter out all spheres whose centers fall outside a bounding box around $\mathcal{P}_f$, inflated by $r_{max}$. 
Then, for any remaining sphere with a center $\mathbf{c}$ and radius $r_{k}$, the updated radius is computed as
\begin{equation}
  r_{k+1} = \min \left( \max \left( r_{k}, d(\mathbf{c}, \mathcal{P}_f) \right)
  , d(\mathbf{c}, \mathcal{X}_{in} \cup \mathcal{X}_p) \right).
  % r_{k+1} = \min \left( d(\mathbf{c}, \mathbf{X}_{in} \cup \mathbf{X}_p)
  % , \max \left( r_{k}, d(\mathbf{c}, \mathbf{P}_f) \right) \right).
  \label{eq:update}
\end{equation}
% where $d(x, \mathbf{P}_f)$ is the signed distance to $\mathbf{P}_f$ (positive if the point is inside the polyhedron, negative if outside) \todo{and} $d(\mathbf{x},\mathbf{X}_{in} \cup \mathbf{X}_p)$ is the minimum distance to all input obstacle points $\mathbf{X}_{in}$, and to the protected map points $\mathbf{X}_p$ described in \autoref{sec:distance-based}.
In this equation, $d(x, \mathcal{P}_f)$ is the signed distance to $\mathcal{P}_f$ (positive if the point is inside the polyhedron, negative if outside).
The value of $d(\mathbf{x},\mathcal{X}_{in} \cup \mathcal{X}_p)$ is the minimum distance to all input obstacle points $\mathcal{X}_{in}$, and to the protected map points $\mathcal{X}_p$ described in \autoref{sec:distance-based}.
Essentially, this equation means that the polyhedron $\mathcal{P}_f$ is used to increase the radius of a sphere, whereas the protected $\mathcal{X}_p$ and input points $\mathcal{X}_{in}$ are used to constrain it.
As the last part of this step, we delete all spheres with $r_{k+1} < r_{min}$, where $r_{min}$ is the smallest allowed sphere radius specified by the user.
% , usually slightly lower than the UAV's minimal allowed distance to obstacles. 

% where TODO.
\subsection{Sampling New Spheres}
\label{sec:adding_spheres}
To introduce new spheres into the map, we sample a fixed number of points inside $\mathcal{P}_f$ at random distances between the camera and the depth mesh $F_o$.
% \todo{This is a simplistic approach, and could be improved for faster flight, for example by sampling along the predicted trajectory of the UAV at high speeds.}
A potential new sphere's radius is determined in the same way as for the old spheres in the previous step in Eq. \ref{eq:update} with $r_k = 0$.
If the potential radius is larger than $r_{min}$, the sphere is added to the map.

\subsection{Recomputing and Sparsifying Sphere Graph}
\label{sec:graph_update}
After all the sphere radii updates have been made, we update the graph of spheres used for path planning, so that all intersecting spheres are connected in the graph.
% TODO - EXPLAIN???
Furthermore, to constrain map update time and path planning time, we perform a redundancy check on the updated and added spheres in the same way as in \cite{spheremap}.
If any sphere is found to be redundant, it is deleted from the map.
% \todo{this approach is important to cover large...}
This step is important to cover large open areas by only a few spheres to allow rapid planning, and tight corridors by a higher density of spheres, capturing the information about potential paths and distances in more detail.
% This way, large open areas are covered by only a few spheres and can be planned over quickly, and tight corridors have a higher density of spheres, capturing the information about potential paths and distances to obstacles in more detail.

% \section{Monocular-Inertial Exploration Using the Sphere-Based Map}
% \section{Safety-Aware Monocular-Inertial 3D Exploration}
% \section{Safe Monocular-Inertial Exploration using the Sphere-Based Map}
\section{Perception-Aware Exploration using the Sphere-Based Map}
% In this section, we detail how the proposed mapping method is highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a monocular camera and IMU.
In this section, we describe a monocular exploration approach enabled by the proposed mapping method.
\todo{[to show that map good for explo?]}
In principle, we employ the commonly used next-best-view (NBV) \cite{paper_frontier_grandpa} strategy, which has so far been deemed unsuitable for monocular exploration in existing works \cite{from_monoslam_to_explo, los_maps}.
% We introduce several major differences in methodology that are neccessary in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments, detailed in the following sections.
% In the following sections, we introduce several major differences in methodology that are neccessary for NBV in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.
In the following sections, we introduce several major differences in methodology that are critical for allowing the NBV strategy to be used for robots equipped with only a monocular camera for depth sensing.
% \todo{write assumptions on perception here? mapping doesnt care}

\subsection{Frontier Sampling on Free-Space Polyhedron}
Firstly, since the proposed map representation is fundamentally different from an occupancy grid, we need to detect frontiers (i.e. the boundary of free and unknown space) in a different way than with an occupancy grid.
We propose to sample points along the visible free-space polyhedron described in \autoref{sec:map_building} at each map update, and add the points as frontiers, if they do not lie inside any free-space sphere and if they are at some user-defined distance from all map obstacle points.
If they do not meet these criteria in any following update, they are deleted. 
% \todo{complexity and vs octomap?}

In enclosed spaces, which are usually considered in autonomous exploration literature, all frontiers are usually worth exploring.
However, in outdoor exploration with areas of open space, it is also important to assign different value to different frontiers, especially for UAVs. 
We propose to constrain the exploration to only consider frontiers that lie near some obstacle points.
This way, the UAV does not explore up into the sky.
Additionally, this forces the UAV to only explore near texture-rich areas, thus avoiding losing visual odometry tracking.
% \todo{Lastly, we add new exploration viewpoints into the map only if a sufficient number of such frontier points would be visible from a given viewpoint at a close distance.}
These frontier points are used to generate exploration viewpoints such that at least some frontier points are visible form each viewpoint.

% EXPLORATION EXPLANATION IMG%%{
\begin{figure}[!htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_930.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1263_peekA1.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1261_peekC1.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1262_peekD1.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_931.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1262_peekA2.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1262_peekC2.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1261_peekD2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \caption{
    Illustration of the exploration approach. The UAV moves to a selected exploration viewpoint aiming at a wall.
    % As described in \autoref{sec:align}, 
    The planned trajectory aligns the UAV with the viewpoint's direction before reaching it so that the monocular SLAM has enough parallax to estimate distances to textured surfaces in that direction.
    The frontier points (blue) near textured surfaces are replaced with newly added obstacle points (red).
    % As explained in \autoref{sec:blocking}, the UAV also blocks viewpoint sampling near visited viewpoints (large black arrows).
    % The UAV also blocks viewpoint sampling near visited viewpoints (large black arrows) to avoid returning to viewpoints facing textureless areas.
    }
  \label{fig:exploration_explanation}
\end{figure}
% %%}


\subsection{Forced Translation when reaching Viewpoints}
\label{sec:align}
In traditional exploration approaches with dense distance sensors, it is sufficient to move the robot to a frontier, aim the sensors towards the unknown space, and assume that the distance sensors will uncover some additional space behind the frontier and thus expand the map.
This approach will not, in principle, work reliably with a robot relying on a monocular camera that estimates depth from motion.
Such a robot requires \textit{translational} motion to obtain correct distance estimates of visual keypoints. 
% as in monocular structure-from-motion (SfM) TODO-CITE (we do not consider deep learning methods which exploit knowledge about sizes of objects).
% Excessive rotation without translation cannot lead to reasonable distance estimates.
% Too much rotation with too little translation of the camera will cause the monocular SLAM or any SfM-based approach to not give reliable depth estimates.
With rotation only, or motions that have too much rotation compared to translation, reliable depth estimates cannot be obtained.

To solve this, we propose a simple perception-aware planning approach: 
% When finding a path to a goal viewpoint that could uncover some frontiers, we compute the target headings on the path so that the UAV maintains a fixed heading in the viewpoint's direction for at least $d_{c}$ meters on the path before reaching the viewpoint, as visualized in \autoref{fig:exploration_explanation}.
When finding a path to an exploration  viewpoint, we force the trajectory to aim the UAV in the viewpoint's direction after getting to a pre-defined distance $d_c$ from the goal, as visualized in \autoref{fig:exploration_explanation}.
% In the experiments detailed in Sec. \autoref{sec:experiments}, we set $d_{c}=TODO$.
This way, the UAV reaches the viewpoint with at least $d_c$ meters of translational motion.
If textured surfaces are visible from that viewpoint, the UAV will most likely observe sufficient parallax to estimate their distances.

\subsection{Explored Viewpoint Blocking}
\label{sec:blocking}
% This is an important part of the method also due to the second major distinction -- we block the sampling of new exploration viewpoints near explored viewpoints.
Another necessary distinction for NBV exploration with a monocular camera is that the UAV blocks sampling of new exploration viewpoints near visited viewpoints.
This is due to the fact that real-world environments often contain textureless areas (mainly in buildings or other man-made structures).
Since obstacle points correspond to textured points triangulated by the visual SLAM, no points ever will be added on textureless surfaces.
Frontier points will be generated there instead, since the surfaces lie on the edge of the visible free space polyhedron and can be far enough from any obstacle points.
% but they will be on the boundary of free-space, so frontiers will be generated there.
However, such frontiers cannot uncover any new space, since there is nothing behind them.
For this reason, we block the sampling of new viewpoints near visited viewpoints, visualized as large black arrows in \autoref{fig:exploration_explanation}
This stops the UAV from getting stuck repeatedly trying to uncover such surfaces.
% , so that the UAV doesn't keep coming back to look at a blank wall.
% We also put additional blocked viewpoints into the map based on where the UAV flies.
% \todo{we also add 

% \subsection{Local-Global Exploration}
% To allow exploration even in large-scale environments, we employ a local-global approach as in \cite{beneath} and \cite{dang2020graph}.
% The UAV first tries finding paths to any exploration viewpoints up to a user-defined "local exploration radius".
% If no reachable viewpoints are found in the radius, the UAV tries finding paths to all exploration viewpoints stored in the map.
% Thanks to the planning efficiency of the sphere graph, this global replanning usually does not take more than a few seconds, but could be made even faster by implementing any sort of long-distance planning abstraction graph, as in \cite{spheremap, topomap}.

\subsection{Safety-Aware Planning}
% Lastly, using the sphere-based representation allows us to flexibly weigh path length and path proximity to obstacles. We use the same path cost criterion as in \cite{spheremap}, and thus we can 
% Lastly, because the free space is represented by a graph of spheres, we immediately have an upper bound on the distance of obstacles at each graph node, and thus it is possible to efficiently weigh path length versus path proximity to obstacles, 
% As a last safety measure, the UAV is controlled to fly in the direction of its camera, when not aligning itself with a viewpoint as described in \autoref{sec:align}.
% Because of the properties of visual SLAM,
Lastly,
the UAV is controlled to always fly in the direction of its camera when not aligning itself with a viewpoint as described in \autoref{sec:align}.
% Free-space might always be wrongly initialized, but when flying in the direction of the camera, and with quick periodic replanning, the risk of collision can be minimized.
This approach is crucial for ensuring collision-free flight in the case of incorrectly initialized free space.
As discussed in \autoref{sec:fake}, some obstacles (e.g. thin wires, twigs) might not be visible in the cameras from a large distance to be detected by the visual SLAM (especially when using fisheye cameras), and can be wrongly estimated as free space.
Using the forward-facing flight, the UAV has a higher chance to see these obstacles that were previously not detected, and quickly replan.
To further increase safety and allow agile maneuvers, the paths are planned with a strong preference for high distance from obstacles, for which the graph-of-spheres representation is highly suitable, as described in \cite{spheremap, bubbleplanner, spheregraph}.
% Furthermore, 
% the measurement distance of free-space spheres could be used for adaptively decreasing the UAV speed in areas with higher probability of non-detected or wrongly localized obstacles, but we leave this for future work.
% With all of these considerations, a UAV with only a monocular camera and IMU can perform 3D volumetric exploration in large-scale outdoor environments, as we demonstrate experimentally in Sec. \autoref{sec:experiments}.
% Furthermore, similarly as we did in the case of LiDAR-based exploration in DARPA SubT scenarios,
% As future work for increasing the safety even more, the measurement distance of free-space spheres could be stored and used.
% With this information, trajectory generation could, for example, force the UAV to fly slower through spheres that have been observed from a large distance and might contain some obstacles not observable from far away.
% \todo{As future work for increasing the safety even more, the measurement distance of free-space spheres could be used for decreasing the UAV speed in areas with higher probability of overlooked or wrongly localized obstacles.}

\section{Experiments}
\label{sec:experiments}
In this section, we show the performance of the proposed method in large-scale real-world (Sec. \ref{sec:real_exper}) and simulated (Sec. \ref{sec:sim_exper}) environments.
Our implementation of the proposed mapping and exploration methods is single-threaded and written in python.
% Even with this unoptimized implementation, the mapping keeps below $\SI{1.5}{\second}$ per map update on a standard CPU.
Even with this unoptimized implementation, the mapping takes $\SI{0.5}{\second}$ per map update on average on a standard CPU.
% This has shown to be sufficient for safe flight up to $\SI{3}{\meter/\second}$ in the tested environments, but a more optimized implementation could allow even faster navigation.
% Combined with a collision-checking thread that checks the predicted trajectory for potential collisions with local data at a high rate, this is suitable for safe flight.
% Our implementation also checks the predicted trajectory for potential collisions with local data at a high rate, this is suitable for safe flight.

The UAV used in the experiments was equipped with a monocular global-shutter grayscale fisheye Bluefox camera, and ICM-42688 IMU.
% The UAV used in the experiments was equipped with a monocular global-shutter grayscale fisheye Bluefox camera, and a standard commercial UAV IMU.
The MRS UAV system \cite{mrs_uav_system} was used for low-level motion planning and control.
% Damping in the form of 3D-printed flexible elements 
In real-world experiments, the camera and IMU are separated from the rest of the UAV by 3D-printed damping elements, which significantly reduces the IMU noise caused by propellers and improves the SLAM performance.
For state estimation and as the source of sparse visual SLAM points, we used OpenVINS \cite{openvins} in the inverse-depth measurement mode.
% , without loop closures.
However, any other monocular-inertial SLAM could be used as the fronted of our method, as long as it tracks points approximately uniformly in the camera's FOV.
% This is so that the assumption that small objects will be detected at a close range, discussed in \autoref{sec:distance-based}, is fulfilled.
This property is necessary to assure the assumption that small objects will be detected at a close range, as discussed in \autoref{sec:distance-based}.
% However, any other monocular-inertial SLAM could be used as the fronted of our method, as long as it has the features described in \autoref{sec:
% \todo{ale muzem pouzit cokoliv}.


\subsection{Real-World UAV Monocular-Inertial Exploration}
\label{sec:real_exper}
% \todo{One of the realized real-world experiments is illustrated in \autoref{fig:exper_real}.}
% In this experiment, the UAV explored up to $\SI{60}{\meter}$ forward around the side of an abandoned farmhouse fully autonomously in an $8\text{min}$ mission.
% In the experiment shown in \autoref{fig:exper_real}, 
In \autoref{fig:exper_real}, we present a \SI{20}{\minute} experiment where our approach is validated in real-world conditions.
We bounded the area for generating exploration goals to a $\SI{70x10x8}{\meter}$ box around the side of an abandoned farmhouse, and the UAV fully autonomously explored nearly all the available space in this region.
The UAV successfully avoided and mapped all the debris, bushes and the house walls in the area, and when battery levels were getting low, it autonomously returned to the starting position.
Also note in that the UAV explored a considerable amount of space in the open field to the left of the house.
This was made possible by our approach to safe estimation of free space in open-area areas, described in \autoref{sec:fake}.
% The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and mapper modules.

We have tested the proposed approach extensively in a variety of other real-world environments, including an orchard \todo{, a forest path, and a campsite,} which our method also explored.
Videos from real-world experiments are available in the multimedia materials. 
% \todo{zkratit, neprisli jsme nato u testovani ale vime o tom a jak to resit (future work - lajny / shortrange sensory a pouzivat tohle na longrange)}
% \todo{Some of these experiments required the safety pilot to take control due to an important limitation of the method that we encountered during testing --- 
% since the visual SLAM tracks and triangulates visual key\textit{points}, visual \textit{lines} (caused by e.g. thin tree branches or smooth manmade poles) are not sensed in our mapping pipeline.
% This caused the UAV to nearly crash into tree crowns multiple times.
% This limitation could be resolved by integrating short-range depth sensors as well, even ultrasound sensors could potentially be fused into the map.
% Additionally, the visual SLAM could be modified to estimate 3D poses of lines as well, as for example in TODO-CITE, but we leave this for future work.
% }

% REAL EXPERIMENT
% %%{
\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t126.png}};
      % \draw [latex-latex](3.2,0.9) -- (3.2,6.8);
      % \node[align=center] at (3.6, 4) {\scriptsize \color{black}\SI{70}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=126\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t206.png}};
     %    \begin{scope}[x={(a.south east)},y={(a.north west)}]
     %      \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=206\text{s}$};
     %    \end{scope}
     %  \end{tikzpicture}
     % \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t370.png}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=370\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t516.png}};
      \draw [latex-latex](4.05,0.4) -- (4.05,7.3);
      \node[align=center] at (3.6, 3.4) {\scriptsize \color{black}\SI{80}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=516\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=90, origin=c, width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/house.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=180, origin=c, width=1.0\linewidth, trim={{0.15\width} {0.3\height} {0.2\width} {0.0\height}}, clip=true]{fig/house.png}};
        % \begin{scope}[x={(a.south east)},y={(a.north west)}]
        %   \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=516\text{s}$};
        % \end{scope}
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    Visualization of the real-world autonomous exploration experiment described in \autoref{sec:real_exper}, along with a satellite image of the explored abandoned farm area. Free space (green) is shown alongside mapped obstacle points (red), frontiers (purple) and explored viewpoints (black). 
  }
  \label{fig:exper_real}
\end{figure}
% %%}

% FIREWORLD ENDMAPS
% %%{
\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/comparsion_fireworld_700.png}};
      \end{tikzpicture}
     \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/fireworld_gazebo1.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/fireworld_astar_final.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/fireworld_monospheres_final.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \centering
  \caption{Best-result maps of the Grid-based exploration approach (bottom left) and MonoSpheres (bottom right) in the urban world (top right) exploration experiments, and the exploration progress of all runs (top left). Green = MonoSpheres, Blue = Grid-based.
  }
  \label{fig:exper_fireworld}
\end{figure}
% %%}

% CAVEWORLD ENDMAPS
% %%{
\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/comparsion_cave_700.png}};
      \end{tikzpicture}
     \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1232.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_astar_final.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_mono_final2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \centering
  \caption{Best-result maps of the Grid-based exploration approach (bottom left) and MonoSpheres (bottom right) in the cave world (top right) exploration experiments, and the exploration progress of all runs (top left). Green = MonoSpheres, Blue = Grid-based. 
  }
  \label{fig:exper_cave}
\end{figure}
% %%}


% SIM TABLE
% %%{
\begin{table}[h!]
\centering
  \begin{tabular}{p{16mm} p{8mm} p{13mm} p{13mm} p{12mm}} 
 \hline
    Method & World & Mean $A$ & Max $A$ & Collisions \\ [0.5ex] 
 \hline\hline
    MonoSpheres  & Urban & 5118 $m^2$ & 5425 $m^2$ & 2/5 \\ 
    Grid-Based & Urban  & 1825 $m^2$ & 1981 $m^2$ & 1/5 \\
    % OFS-disabled & Urban & TODO $m^2$ & TODO $m^2$ & 0/1 \\ 
    \hline
    MonoSpheres  & Cave & 5581 $m^2$ & 6012 $m^2$ & 0/3 \\ 
    Grid-Based & Cave  & 3119 $m^2$ & 3375 $m^2$ & 0/3 \\
    % OFS-disabled  & Cave & 4910 $m^2$ & 4910 $m^2$ & 0/1 \\ 
 \hline
\end{tabular}
  \caption{Quantitative results of the simulation experiments described in \autoref{sec:sim_exper}. }
\label{table:sim_exper}
\end{table}
% %%}

% \subsection{Comparison with Grid-Based Approach in Simulation}
\subsection{Simulation Experiments}
\label{sec:sim_exper}
% In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.
% In the following experiments, we showcase the capabilities of our approach in simulated  urban areas with multiple buildings, collapsed trees, cars and other obstacles.
% TODO
% In this section, we quantitatively evaluate the performance of our method.
As the previously published monocular exploration methods do not have code available (with the exception of \cite{simon2023mononav}, which, however, is not ROS-compatible as of writing this paper), we prepared a simple grid-based mapping and exploration pipeline for comparison with MonoSpheres.
This reference method uses the same mapping approach as \cite{from_monoslam_to_explo, los_maps} (i.e. initializing free space between the camera and the visual SLAM points that have a low position covariance). 
To explore, the method periodically samples free-space points uniformly in a 40x40x10 area around the robot and sends these as goals to a grid-based A* path planning module, which finds path through the occupancy grid that would move the robot as close as possible to the goal while respecting the same distance from obstacles and uknown space as the MonoSpheres method (\SI{1.5}{\meter} in these experiments).
The robot then follows these paths with the camera pointing forward and replans at \SI{5}{\hertz} to handle measurement inacurracies. 
We use OctoMap \cite{octomap} for the grid-based mapping with a \SI{0.5}{\meter} cell size.
This value was chosen because smaller cell sizes caused the robot to not find any safe paths due to the free-space gaps documented in \cite{from_monoslam_to_explo} and larger cell sizes blocked exploration of narrow areas. 

As the evaluation metric, the volume of the constructed map is commonly used in exploration literature.
However, because of the open-area free-space sampling scheme and the depth interpolation, MonoSpheres constructs considerably more free space than the raytracing mapping approach over the same trajectories. 
For this reason, the volume metric would not be fair towards methods using the grid-based raytracing mapping. 
We instead divide the target exploration area into 2.5x2.5m columns that are marked as explored if the constructed map (sphere-based or grid-based) contains any element in a given column. 
The metric, $A$, is then the top-down 2D area of all the explored columns.

We compare the grid-based approach with MonoSpheres in multiple 20min experiments on 2 different worlds, with the resulting maps and exploration progress in \autoref{fig:exper_fireworld}, \autoref{fig:exper_cave} and the explored area and crash rate in \autoref{table:sim_exper}.
Videos from these experiments are available in the multimedia materials.
The experiments were conducted in the Gazebo simulator, with essentially the same UAV and sensory setup as in the real-world experiments.
Exploration goal generation was constrained to a \SI{60x50x7}{\meter} area of interest in the urban world for both methods, and to \SI{170x110x20}{\meter} in the cave world.

\subsection{Discussion}
% In the simulated experiments, our proposed approach was able to cover both environments.
In all of the experiments, our approach achieved long periods of collision-free exploration even near complex obstacles such as trees, collapsed houses, and cars in uncommon configurations.
Since the outdoor environment contains several textureless and thing objects (e.g. exposed metal rods, burned black surfaces), both our approach and the grid-based approach collide with these obstacles occasionally. 
This is due to the limitation of the utilized point-based SLAM which cannot detect these obstacles, thus not assuring the assumption described in \autoref{sec:fake}.
However, this could likely be resolved by making the mapping pipeline multimodal and incorporating measurements from low-cost short-range distance sensors (e.g. ultrasonic).
Alternatively, designing a visual front-end that would prioritize detection of such obstacles could solve this as well. 
Either way, the experiments show that our method can explore large-scale areas even with this off-the-shelf visual frontend and challenging obstacles.
% but that is outside the scope of this paper.
% As expected, the documented collisions were caused by occasional incorrect free-space sampling (discussed in detail in \autoref{sec:fake}) near small featureless obstacles (such as blackened parts of the simulated burned buildings or exposed metal rods) that were not detected by the point-based visual SLAM frontend.
% Nevertheless, this could be resolved by incorporating single-point measurements from a low-cost short-range distance sensor (e.g. ultrasonic) into the mapping pipeline, or by leveraging a different visual front-end designed to prioritize detection of such obstacles.
% Nevertheless, this could be resolved by incorporating measurements from low-cost short-range distance sensors (e.g. ultrasonic) into the mapping pipeline.

In the "cave" world, which is a texture-rich indoor environment, the grid-based approach performed similarly to MonoSpheres in terms of initial exploration speed.
% and with sufficient side-to-side motion, was able to create sufficient free space to explore the cave corridor.
With a more sophisticated viewpoint selection approach, the grid-based approach could likely explore as much area in the end as MonoSpheres, supporting the viability of this mapping approach in indoor, textured areas claimed in \cite{from_monoslam_to_explo, los_maps}.
% This finding supports that the mapping approach presented in \cite{from_monoslam_to_explo, los_maps} for textured, indoor spaces.

However, in the "urban" world -- an open-space, texture-sparse environment -- the grid-based approach was never able to explore a large portions of the area that require the UAV to fly over a building or move across an open field. 
In comparison, MonoSpheres (with the same parameters as in the cave world) was able to consistently cover the outdoor environment.
These results demonstrate that our proposed open-space free-space sampling approach described in \autoref{sec:fake} enables exploration of outdoor areas, where the ray-tracing grid-based mapping is not able to create sufficient free space, while also safely mapping indoor areas.
% \todo{To confirm the utility of this feature of the mapping, we also evaluated the performance of the pipeline with the open-space free-space sampling disabled. As expected, this ablation leads to performance similar as MonoSpheres in the enclosed cave world, but explores only the small enclosed area as the grid-based approach in the outdoor world.}
% DISCUSSIONThe mapping approach A is more conservative. This leads to fewer crahes, but limits the areas the UAV can explore. The mapping approach proposed in this paper can occasionally overestimate free space, but allows the UAV to safely move through open-space areas.TODO - disposable so crashes not a problem! and also fixable by better monodepth outside the scope of this!

% Both of these issues could be solved in future work by using a visual front-end that uses line-based or dense features, or by leveraging low-cost ultrasound sensors as a bumper for the UAV.
% As described in \autoref{sec:fake}, small featureless obstacles (such as the smooth metal pole near the fast-food building) occasionally cause the UAV to crash, but this can be solved by a visual front-end that uses line-based features, or by leveraging low-cost ultrasound sensors as a bumper for the UAV.
% As described in \autoref{sec:fake}, small featureless obstacles (such as the smooth metal pole near the fast-food building) occasionally cause the UAV to crash, but this can be solved by a visual front-end that uses line-based features, or by leveraging low-cost ultrasound sensors as a bumper for the UAV.
% Also, the explored area could be increased by implementing a more sophisticated exploration strategy, but even the simplistic greedy approach used in this paper demonstrates the usefulness of our mapping approach.

% In such more topologically complex environment, our greedy exploration strateg
% Our exploration strategy is greedy, 
% After running the exploration mission 20 times, the UAV managed to explore the entire environment in 18/20 runs.
% In the remaining 2 runs, the UAV crashed either into a large featureless metal pole or the roof of a gazebo where the texture only consists of lines.
% The crashes were caused by the same limitation as discussed in \autoref{sec:real_exper}.
% Aside from this limitation, the mapping and exploration can be said to work robustly in static environments.
% Note also that in the experiment shown in \autoref{fig:exper_sim}, the UAV explored up on the roofs of the buildings when it was able to use the large towers to estimate depth and create enough free space for safely moving up to the roof.

% \subsection{Ablation Study - Open-Area Free-Space Sampling }
% \label{sec:ablation}
% In this experiment, conducted also in the Gazebo simulation, we analyze the method over multiple runs in a smaller urban environment.
% As can be seen in the table 


% LARGESCALE EXPLO TOWN
% \begin{figure}[htb]
%   \def\subfigwidth{0.89\linewidth}
%   \centering
%      % \begin{subfigure}[b]{\subfigwidth}
%      %  \begin{tikzpicture}
%      %    \centering
%      %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/sim_exper_gazebo.png}};
%      %  \end{tikzpicture}
%      % \end{subfigure}
%      % \hfill
%      % \begin{subfigure}[b]{\subfigwidth}
%      %  \begin{tikzpicture}
%      %    \centering
%      %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/sim_exper_end.png}};
%      %  \end{tikzpicture}
%      % \end{subfigure}
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/flood3.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.05\height} {0.0\width} {0.0\height}}, clip=true]{fig/flood_end_freespace.png}};
%       \draw [latex-latex](0.3,0.1) -- (0.3,6.85);
%       \node[align=center] at (0.75, 4.5) {\scriptsize \color{black}\SI{130}{\meter}};
%         \begin{scope}[x={(a.south east)},y={(a.north west)}]
%           % \node[align=center] at (0.4, 0.925) {\footnotesize \color{black}$t=516\text{s}$};
%         \end{scope}
%       \end{tikzpicture}
%      \end{subfigure}
%   \centering
%   \caption{The resulting map (bottom) after 33min of exploring a simulated post-earthquake urban environment (top).
%   The experiment is described in more detail in \autoref{sec:sim_exper}. Black arrows indicate visited frontier viewpoints, frontiers are shown as purple points. }
%   \label{fig:exper_sim}
% \end{figure}



\section{CONCLUSION}
% \todo{[In this paper, we reformulate 3D volumetric mapping and exploration to use a graph-of-spheres map instead of a traditionally used voxel-based one.]}
In this paper, we proposed a novel approach to building a navigable map of 3D space, departing from the paradigm of tracing rays through a voxel-based grid.
% [building a graph-of-spheres representation using sparse depth estimates provided by monocular-inertial SLAM. 
% Our approach shows that it is possible to actively build a map allowing safe navigation and large-scale exploration, even with very sparse and rough depth estimates from a monocular camera. 
Our results show that it is possible to actively build a map allowing safe navigation in unstructured environments, even with very sparse depth estimates from a monocular camera. 
% Our method only requires a monocular camera and an IMU, and can run on a standard onboard computer without needing a GPU.
Compared to the existing approaches to 3D monocular exploration, which have so far presented autonomy at the scale of a few indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav}, our proposed approach enables autonomous exploration both indoors and outdoors, at the scale of an urban area with multiple buildings, trees and wide open spaces.
With the same configuration, our method allows exploration of both indoor and outdoor environments.
In future work, our method could be improved by adopting a submap-based approach and a better-suited depth estimation frontend, to achieve reliable monocular exploration at the same scale as LiDAR based systems.
% Our proposed method significantly reduces the cost of autonomous unknown-space navigation capabilities, as it only requires a single camera, a standard IMU and no GPU for dense depth estimation.
% Our method also significantly reduces the cost of autonomous unknown-space exploration and safe navigation that was previously only documented using LiDARs and expensive depth sensors, as it only requires a single camera, a standard IMU and 
% no GPU for dense depth estimation.
% a CPU.
% Thus, we hope our method can be a step towards economically viable real-world deployment of UAV systems, especially in search-and-rescue missions.
% As future work, we intend to complement the imperfections of the depth estimation by integrating low-cost, short-range distance sensors (e.g. ultrasound) into the mapping pipeline, thus detecting even thin and textureless objects.
% As future work, we intend to complement the problems of monocular depth estimation near thin or textureless obstacles by integrating low-cost short-range distance sensors (e.g. ultrasound) into the mapping pipeline.
% Our approach could be extended in future work by integrating low-cost short-range distance sensors (e.g. ultrasound) into the mapping pipeline to complement the problem of depth estimation near textureless objects.
% Furthermore, the proposed map representation could be extended with other geometrical primitives for clustering large areas of free or occupied space, thus increasing performance and allowing 
% \todo{Additionally, we plan to use the proposed mapping approach for building submaps instead of a single large map and performing large-scale map optimization combined with vision-based place recognition 
% to achieve low-cost autonomous exploration at the scale of kilometers, which has only been demonstrated with expensive LiDAR-based systems so far. [NOT IMPORTANT?]}
% and then perform large-scale map optimization using place recognition, 

% As we analyzed in this paper, 

% In future work, we intend to use  
% Thus, the proposed method 
% In this paper, we presented a novel sphere-based spatial mapping method 
% % together with a robust monocular inverse-depth estimator,
% using sparse point cloud data from monocular visual sensors,
% and demonstrated how it can enable any UAV equipped with a single camera and any form of odometry to quickly and safely navigate and explore unknown, unstructured 3D environments.
% We have demonstrated, through testing in simulation and in the real world, that our new mapping approach and spatial representation enable safe, large-scale autonomous navigation and exploration on UAVs without traditionally used expensive range sensors.

% The spatial representation proposed in this paper, consisting of spheres and points, offers an alternative to the traditionally used grid-based representations.
% We have presented a method of constructing this representation on-board a UAV, using only the keypoints from sparse monocular-inertial SLAM as input depth information.
% % We have experimentally demonstrated how this new representation allows a UAV to explore and navigate large-scale outdoor environments in the real-world.
% We have demonstrated how this new mapping approach enables 3D exploration and navigation in large-scale real-world outdoor environments on a UAV equipped with only a monocular camera and IMU as its sensors.


% Trough extensive experimental evaluation, we have showed the feasibility of doing pose estimation, autonomous mapping, and exploration on-board a single UAV equipped with a only a monocular camera and IMU as the used sensors.

% In future work, we intend to explore the potential of the proposed representation for multimodal sensing -- e.g. intelligently fusing points and spheres based on the modality that they were created from.

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word acknowledgment in America is without an e after the g. Avoid the stilted expression, One of us (R. B. G.) thanks . . .  Instead, try R. B. G. thanks. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, Synthetic structure of industrial plastics (Book style with paper title and editor), 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 1564.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, An approach to graphs of linear forms (Unpublished work style), unpublished.
% \bibitem{c5} E. H. Miller, A note on reflector arrays (Periodical styleAccepted for publication), IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleSubmitted for publication), IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style), IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, Infrared navigationPart I: An assessment of feasibility (Periodical style), IEEE Trans. Electron Devices, vol. ED-11, pp. 3439, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, A clustering technique for digital communications channel equalization using radial basis function networks, IEEE Trans. Neural Networks, vol. 4, pp. 570578, July 1993.
% \bibitem{c12} R. W. Lucky, Automatic equalization for digital communication, Bell Syst. Tech. J., vol. 44, no. 4, pp. 547588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, On the compatibility of adaptive controllers (Published Conference Proceedings style), in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 816.
% \bibitem{c14} G. R. Faulhaber, Design of service systems with priority reservation, in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 38.
% \bibitem{c15} W. D. Doyle, Magnetization reversal in films with biaxial anisotropy, in 1987 Proc. INTERMAG Conf., pp. 2.2-12.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, Radio noise currents n short sections on bundle conductors (Presented Conference Paper style), presented at the IEEE Summer power Meeting, Dallas, TX, June 2227, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, An analysis of surface-detected EMG as an amplitude-modulated noise, presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, Narrow-band analyzer (Thesis or Dissertation style), Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, Parametric study of thermal and chemical nonequilibrium nozzle flow, M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, Nonlinear resonant circuit devices (Patent style), U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
