%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
% \newcommand{\vk}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\todo}[1]{{\hypersetup{allcolors=red}{\color{red} {#1}}}}
\newcommand{\todocaption}[1]{{\color{red} {#1}}}

% \newcommand{\vk}[1]{#1}
% \newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
% Monocular Sphere-Based Occupancy Mapping and Large-Scale Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Sparse Monocular SLAM for Robust Large-Scale 3D Exploration on Inexpensive UAVs}

% FINAL TWO
% Sphere-Based Mapping using Sparse Monocular SLAM Points for Robust Large-Scale 3D Exploration on Inexpensive UAVs}
% Monocular-Inertial Exploration of Large-Scale Outdoor Environments using Sparse SLAM Keypoints on an Inexpensive UAV}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Unstructured Environments using Sparse Visual SLAM}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by UAVs}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by Low-Cost UAVs}
% Monocular-Inertial Sphere-Based Mapping for a UAV Exploring Large-Scale Unstructured 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale Unstructured 3D Environments}
Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% Monocular-Inertial 3D Exploration of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Interital Sphere-Based Mapping for Large-Scale UAV Exploration}

% % PolySphereMap - Occupancy Mapping using Sparse Monocular SLAM for Large-Scale 3D Exploration on Inexpensive UAVs}
% PolySphereMap - Mapping and Exploring Large-Scale 3D Environments using a Monocular Camera on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%

  \thanks{%
    Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
}
  \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  We present a novel approach to mapping 3D space for navigation purposes, using only pose estimates and pointclouds generated by sparse monocular-inertial SLAM as input.
  % \todo{SPARSE DIST-MESUREMENTS, EG FROM MONO-IN SLAM? (not too specific for SLAM? how did those in room formulate this?)}
  % Our main contribution is representing an environment by a graph of intersecting free-space spheres and a set of triangulated visual keypoints with remembered measurement uncertainty, which is fundamentally different from grid-based occupancy maps. 
  We propose to represent an environment by a graph of intersecting free-space spheres and a set of obstacle points instead of traditionally used grid-based maps. 
  To build this map, we inscribe free-space spheres inside a polyhedron of estimated free space between the camera and SLAM points at each keyframe.
  % Instead of treating the pointclouds as LIDAR measurements, we 
  % SLAM points are added into the map as obstacles along with the distance they were measured at.
  % This allows us to work with the distance uncertainty specific to visual SLAM, 
  % Additionally, 
  % Our method remembers the distance at which obstacle points were measured, which allows us to 
  % We present a method of building this representation in real-time, using only sparse 3D point data obtained from monocular SLAM by inscribing spheres inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  % This representation can be built in real-time, using only sparse 3D point data obtained from monocular SLAM by inscribing spheres inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  % In our approach, the map is built in real-time by roughly interpolating depth between visible 3D points from visual SLAM while 
  % Spheres are inscribed inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  % A polyhedron of approximated visible free-space is formed at each map iteration from tracked SLAM points and the camera's position.
  % Spheres are inscribed inside this polyhedron.
  % This polyhedron is used for updating sphere radii and deleting obstacle points.
  % Spheres are inscribed inside the estimated visible free-space polyhedron formed by the visual keypoints and the camera position.
  Furthermore, we propose a perception-aware exploration approach that is enabled by the proposed mapping method, and is tightly coupled to the properties of monocular SLAM.
  % By working tightly with the properties of visual SLAM, our method can safely handle occlusions, featureless areas and large open spaces.
The proposed methods combined are, to the authors' best knowledge, the first to achieve autonomous exploration in unknown, unstructured indoor/outdoor 3D environments at the scale of several buildings, using only a single monocular camera and an IMU onboard a possibly micro-scale UAV.
  We open-source the code for our methods to provide 3D mapping and navigation capabilities to any UAV equipped with a monocular camera and IMU.

Code--- %\href{
\href{TODO}{TODO}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
In recent years, significant advances have been presented in autonomous robotic exploration, achieving missions that span kilometers in scale \cite{darpa_cerberus_wins, beneath}. 
These advances are critical for enabling deployment of UAVs in real-world, unknown environments, where global navigation satellite systems (GNSS) are partially or completely unavailable.
% However, the state-of-the-art mapping and exploration methods depend heavily on dense range sensors (lidars - heavy, depth cameras - shortrange, stereo cameras - expensive, not available in thermal).
However, state-of-the-art mapping and exploration methods depend on dense range sensors such as LiDARs or depth cameras.
These sensors are heavy, expensive, and thus severely limit the affordability and flight time of autonomous UAVs.

Monocular cameras and inertial measurement units (IMUs), on the other hand, are abundant in consumer electronics and vastly more commercially available.
% Additionally, cameras are used for object recognition and other types of vision-based perception beneficial for autonomous UAVs, and thus 
% it makes sense to have them equipped on many UAV applications already.
% they are already equipped on UAVs in most real-world applications.
% Additionally, most of the computer vision methods for perception, such as object classificiation, semantic segmentation, object tracking etc. use monocular camera images and do not need dense range sensors.
Enabling UAVs to autonomously map and navigate unknown environments using only such small, light-weight and inexpensive sensors could significantly lower the cost of many real world UAV applications, such as swarms of disposable UAVs for search and rescue missions, wildfire detection or reforestation.
% , which are not viable due to the heavy and costly dense range sensors.

Navigating an unknown 3D environment using only a monocular camera and IMU is, however, still a challenging research problem.
% The existing approaches present success only at the scale of one or two indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav, cnn_explo_singleroom}, or outdoor, but with severely limiting assumptions on the environmental structure.
Full 3D exploration has only been presented at the scale of several indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav, cnn_explo_singleroom}.
Some works have demonstrated limited outdoor exploration, but with highly specific assumptions on the environmental structure.
\todo{[CITE]}
% The autonomous UAV systems operating in unknown environments documented in recent literature \cite{darpa_cerberus_wins, beneath} usually construct an occupancy grid \cite{occupancy_moravec}, such as the efficient octree-based implementation of OctoMap \cite{octomap}, and use it as the base representation for planning, exploration and constructing higher-level spatial abstractions \cite{topomap, spheremap}.
% However, building a detailed occupancy grid requires dense pointclouds of range measurements due to the raycasting nature of occupancy grid updates.
% which is to cast rays towards the measurement points and update the occupancy of traversed cells.
% TODO - check if all

% Building a map for exploration using only a monocular camera for depth sensing is challenging for many reasons \todo{vagni}, and has so far been demonstrated only in small-scale indoor environments.


% INTRO IMG%%{
% \begin{figure}[!t]
%   % \includegraphics[width=8.5cm]{fig/smap2.png}
%   % \includegraphics[width=8.5cm]{fig/explor_forest2.png}
%   % \includegraphics[width=8.5cm]{fig/intro_poly.png}
%     % \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.05\height} {0.05\width} {0.1\height}}, clip=true]{fig/intro_poly.png}
%     \adjincludegraphics[width=8.5cm, trim={{0.0\width} {0.00\height} {0.00\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}
%   \centering
%   % \caption{Visualization of the proposed map representation and how it is updated. The green polyhedron is an estimate of currently visible free space and is formed by the visible triangulated keypoints and the camera's position (axes). The mapped spheres (green), obstacle points (red) and frontier points (purple) are updated by the polyhedron and visible keypoints, as described in \autoref{sec:map_building}}
%   \caption{Visualization of the proposed map representation as described in \autoref{sec:map_building}}
%   \label{fig:smap_intro}
% \end{figure}

\begin{figure}[!t]
  \centering
     \begin{subfigure}[b]{0.98\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.1\height} {0.0\width} {0.1\height}}, clip=true]{fig/sar_mono_dji_2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_912_cam.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}};
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    % Illustration of the mapping pipeline. 
    Illustration of the proposed method evaluated in a real-world outdoor environment with large open areas. 
    % The UAV (top) only uses a monocular camera and IMU as its sensors.
    The method uses information from sparse monocular-inertial SLAM (left) running onboard the UAV (top) to construct a large-scale map consisting of free-space spheres and obstacle points (right) for safety-aware exploration.
  }
  % \label{fig:mapping_pipeline}
  \label{fig:smap_intro}
\end{figure}
% %%}


In this paper, we present a novel approach to 3D mapping of unstructured large-scale environments. 
Our method utilizes a sparse pointcloud of 3D points produced by monocular-inertial SLAM, which is already a necessary part of many vision-based UAV systems.
Instead of traditionally casting rays through a grid and updating its cells \cite{octomap, voxblox}, we construct a polyhedron of approximate visible free space at each frame, inscribe spheres of free space inside that polyhedron, and connect them together to form a graph of intersecting spheres.
By storing obstacles as a set of points, we can assign additional metrics to each point, such as measurement distance. 
% or, in future work, the modality of its origin.
This approach allows for rule-based measurement fusion, which is critical for dealing with the challenges of monocular depth estimation, as we explain in \autoref{sec:distance-based}.
% In this representation, we assign additional metrics
% This representation allows for fast and safety-aware planning \cite{spheremap}.
We further show how the proposed mapping method enables autonomous exploration in real-world, \todo{large-scale} indoor/outdoor environments on a UAV using only a single monocular camera and IMU, which, to the authors' best knowledge, has not yet been presented in literature.
This paper brings the following contributions:

% To enable safety-aware monocular vision-based autonomous exploration in unknown, unstructured 3D environments of large scales, our paper brings the following contributions:

% Our paper brings the following contributions:
% To enable autonomous navigation in unstructured 3D environments of large scales for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
% To enable autonomous navigation in general environments for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
\begin{enumerate}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  % \item A novel 3D mapping approach that constructs a sphere-based map directly from sparse pointclouds obtained by monocular-inertial SLAM, without needing any intermediate occupancy map, mapping both open-space and cluttered areas.
  \item A novel 3D mapping approach that constructs a sphere-based map for safe map-based navigation using only sparse pointclouds and the trajectory obtained by monocular-inertial SLAM as input, with less conservative free-space estimation than traditional ray-casting methods.
    % , without needing any intermediate occupancy map.
    % , mapping both open-space and cluttered areas.
    \item A perception-aware exploration approach enabled by the proposed mapping method that achieves large-scale 3D outdoor exploration on a UAV equipped with only a monocular camera and IMU, 
      validated in real-world conditions.
    \item Open-sourced code for the proposed mapping and exploration methods, along with example simulation scripts for replicating our results.
\end{enumerate}


\section{Related Works}
\label{sec:representations_rw}

% The environment representation similar to our previous work \cite{spheremap} -- in \cite{spheremap}, we also built a graph of intersecting free-space spheres, but 
In the field of robotic exploration, environments are most often represented by a grid-based map of occupied, unknown, and free space, either in the form of a signed distance field \cite{voxblox} or an occupancy grid \cite{occupancy_moravec, octomap}.
These grid-based representations are the backbone for nearly all of robotic path planning, exploration \cite{darpa_cerberus_wins, beneath} and higher-level spatial abstraction methods \cite{topomap, skeletons, spheremap}.
However, these maps rely on dense distance sensors, such as LiDARs or depth cameras, to accurately map the true free and occupied space.

% , has several drawbacks for usage on UAVs without dense range sensors in varying-scale environments.

% Firstly, both ESDFs and occupancy grids are updated by casting rays towards measured 3D points and updating the values of traversed voxels.
% With sensors that produce dense pointclouds, such as LiDARs or depth cameras, this approach accurately maps the true free space.
% However, using this technique with sparse distance measurements can lead to large gaps in the map, as documented in \cite{from_monoslam_to_explo}.
A monocular camera does not produce dense distance measurements by itself and thus it is difficult to apply the vast amount of available approaches for robot autonomy on monocular systems.
A recent line of research tries to bridge this gap by turning a monocular camera into a dense depth sensor using deep learning. 
% The authors of \cite{simon2023mononav, cnn_explo_singleroom} use deep-learning-based single image depth estimation models to obtain dense point clouds from a monocular camera, essentially turning it into a depth camera.
The authors of \cite{simon2023mononav, cnn_explo_singleroom} have demonstrated using such depth estimation models combined with traditional navigation approaches.
However, the authors demonstrate autonomy only at the scale of a single room, while also encountering collisions and map deformations due to depth estimation errors.
As discussed in recent surveys \cite{deep_mono_depth_2021, deep_mono_depth_2019}, current depth estimation models still struggle to provide real-time onboard performance and reliable generalization in domains that the model was not trained on.
In addition, a GPU significantly raises the cost and weight of a UAV.
Thus, in this paper, we chose not to go with learning-based depth estimation.
% In addition, to the fact that a GPU significantly raises the cost and weight of a UAV, this 

A different line of research focuses on taking the sparse 3D points from monocular SLAM and working with those as the only depth measurements.
Most of this research has been on reactive obstacle avoidance \cite{flame}[TODO] or exploration with strong assumptions on the environment structure [TODO].
% A different line of research \cite{from_monoslam_to_explo, los_maps} focuses on taking the points from monocular SLAM, treating them as sparse distance measurements, and building the grid-based maps in the same way as with dense pointclouds.

For monocular exploration in unstructured environments, the authors of \cite{from_monoslam_to_explo, los_maps} proposed taking the points from monocular SLAM, treating them as sparse distance measurements, and building grid-based maps in the same way as with dense pointclouds.
% A few these works \cite{from_monoslam_to_explo, los_maps} have demonstrated autonomous exploration in unstructured, indoor environments, 
% As documented in \cite{from_monoslam_to_explo} and as shown in [\todo{TODO-FIG}], this approach causes gaps to appear in the free space of the map, whenever the environment is not texture-rich.
As documented in \cite{from_monoslam_to_explo} and as illustrated in [\todo{TODO-FIG}], this approach causes gaps to appear in the free space of the map when the environment is not texture-rich.
The authors propose an exploration method specifically to cover these gaps.
However, this only reduces the gaps, as this mapping method can never estimate free space near texture-sparse areas, and the authors show this exploration approach to only work at the scale of a single room.
% \todo{This technique accurately covers free space when dense input pointclouds are available, such as from a LiDAR or depth camera. 
% If the voxel size is too small and/or the data is sparse, gaps will appear in the free space.
% In large open areas where many features are too far to be accurately triangulated, this can lead to no free space being initiated except near the ground, causing the robot to not explore into the open space.
% }.
% Existing works address this issue in multiple ways:
% Treating sparse points from monocular SLAM as 
% The existing works on using using monocular SLAM points for 3D mapping treat the SLAM points as range measurements and perform the raycasting method as well. 
% With only sparse 
% This becomes problematic when the available measured points are very sparse, such as points tracked by sparse monocular SLAM.
% Some works were motivated to overcome this limitation -- 
% In \cite{from_monoslam_to_explo}, the authors take points from semi-dense monocular SLAM and raycast towards them to build an OctoMap \cite{octomap} occupancy grid.
% However, they only demonstrate their local exploration approach built on top of this mapping pipeline to work inside a single room with textured walls.
% The authors of \cite{los_maps} work around the gap issue by setting the voxel size to be relatively larger compared to the UAV size, which causes more voxels to be passed by the sparse rays towards measured points.
% They present exploration on the scale of several indoor rooms and in simulation only. 

The authors of \cite{los_maps} cast rays towards sparse SLAM points as well, and present exploration on the scale of several indoor rooms and in simulation only.
They work around the gap issue by setting the map voxel size to be relatively large compared to the UAV size.
A larger voxel size causes a higher percentage of voxels to be passed by the sparse rays towards measured points, but leads to the following problem:
% This approach can however only work in constant-scale environments.
% This approach can however only work in constant-scale environments without narrow corridors.
% Such large voxel size, however, can cause narrow pathways to appear untraversible if the map is not perfectly aligned.

Grid-based maps in general suffer from the problem of choosing the voxel resolution in multi-scale environments.
If the voxels are too big, then narrow passages can wrongly appear as untraversible or even completely closed, severely limiting the robot's navigation. 
If the minimum voxel size is too small compared to the density of data, gaps can appear in free space due to rays passing only a thin strip of voxels
Moreover, such high map resolution requires large amounts of raycasts which can go beyond the computational power available on the onboard computers.
\todo{rict ze sfery maj min radius, ale nevadi to scalovani, neni to to samy co grid resolution}

% We argue that using a smaller voxel size would not solve this issue, since that would leave many gaps in the free space.
% and in [ROOM-DENSEDEPTH], the authors essentially perform dense pixel-based estimation with known poses from sparse SLAM, which works in well-illuminated rooms but may have problems in larger-scale or texture-poor environments.
% Second, even though memory-efficient octree-based representations of 3D grids are available, most notably the OctoMap library \cite{octomap}, a fundamental problem of grids is that the smallest-voxel size must be set by the user.

% This is the second fundamental problem of grid-based representations -- the smallest-voxel size must be set by the user.
% If this is set as too large, narrow passages will not be captured in the map, and if it is too small, the map will need large amounts of raycasts to set all the cells' occupancy values, which is computationally expensive.
% In our previous work \cite{spheremap}, we have shown how a graph of intersecting spheres can be built out of an occupancy grid, and that this representation allows path planning that has explicit information about distance to obstacles, in addition to being orders of magnitude faster than on an occupancy grid.

In previous work \cite{spheremap, spheregraph, bubbleplanner}, it has been shown that using a graph of intersecting free-space spheres, rather than a voxel grid, can solve this scaling issue.
\todo{TODO - address octree repre vs spheremap, also smallest sphere size}
A graph-of-spheres representation efficiently describes large spaces with a few nodes, while keeping detailed information in narrow passages.
% In previous work \cite{spheremap, spheregraph, bubbleplanner}, it has been shown that representing free space by a graph of intersecting spheres \todo{solves this problem by} can efficiently describe large-space while keeping detailed information in narrow spaces.
In additon, it allows orders of magnitude faster path planning \cite{spheremap, spheregraph} than grid-based representations.
% In \cite{spheremap}, we also demonstrated that this representation allows path planning that has explicit information about distance to obstacles, in addition to allowing orders of magnitude faster path planning than on an occupancy grid.
In this paper, we represent free space using spheres as well, but with two major differences: 
% In this paper, we represent the free space using spheres as well, but also add the obstacle points into the map and use them for constraining the sphere radii, but with \todo{two major differences 

\begin{itemize}
    % \item now \textit{we do not need to build the occupancy grid as an intermediate step}, which frees up significant computational resources, and \textit{we construct the map using sparse point-clouds from monocular SLAM}, which are problematic to use for occupancy grids.
  % \item In this paper, we build the graph of spheres \textit{directly} out of the sparse 3D pointclouds and pose estimates obtained from monocular SLAM, without needing to build an intermediate occupancy grid before computing the sphere radii as in \cite{spheremap}, which frees up significant computational resources.
  \item In \cite{spheremap, spheregraph, bubbleplanner}, the method required building a local occupancy grid with a LiDAR for updating the spheres. 
    In this paper, \todo{we build} the graph of spheres \textit{directly} out of 3D measurement points without any intermediate occupancy grid, and only require a monocular camera running visual SLAM instead of a LiDAR.
    
% In addition, there is no current documented way to build an occupancy grid using sparse point clouds, although this could also be interesting to research.
% TODO-CHECK
  \item \todo{Additionally, the representation proposed in this paper stores visual keypoints as obstacle points and remembers their measurement distance. 
    These points are important for correctly updating free space when previously seen obstacles are momentarily not detected by the visual SLAM (see \ref{sec:distance-based}).}
        % to overcome the challenge of incorrect measurements at larger distances.
\end{itemize}

% Vision-based UAV autonomous exploration has not yet reached the levels of autonomy as when using dense depth sensors, and remains a challenge, as documented in recent surveys \cite{drones_survey}, but some progress has been made in small-scale indoor deployment and by using stereo cameras.
% TODO - mention reactive only
% In \cite{explor_stereo}, the authors presented the first ever vision-based UAV system that can explore and build a map (a 3D occupancy grid) on a UAV equipped with stereo cameras and also a supporting downward-facing optical flow sensor for better motion estimation.
% The authors used dense pointclouds coming from the stereo cameras for building an OctoMap \cite{octomap} occupancy representation in the same way as with depth data from an RGB-D camera or LiDAR.
% This allowed them to use similar exploration techniques as on systems with such sensors.

% For UAVs equipped with only a monocular camera, to the authors' best knowledge, all the existing works have demonstrated autonomous 3D mapping and exploration on a UAV only indoor and at the small scale of a single room \cite{from_monoslam_to_explo, cnn_explo_singleroom} or a few simple rooms \cite{los_maps, simon2023mononav}.
% Monocular SLAM, both sparse and dense, cannot produce measurement points on textureless areas, and the existing approaches either accept that the data from visual SLAM can be very sparse, or they use deep-learning-based depth estimation to obtain dense data.

% The authors of \cite{from_monoslam_to_explo, los_maps} argue against using frontier-based methods \cite{paper_frontier_grandpa}, due to the facts that no points will ever be generated on textureless areas, and thus those frontiers would not be uncovered by a monocular camera.
% The authors have presented alternative exploration approaches, but they have only shown them to work in small-scale environments.
% In our exploration approach, thanks to not using raycasting in the mapping pipeline and also blocking explored viewpoints, we show that frontier-based exploration is possible and assures global coverage of the explored environments.

% \todo{TODO - cite corridors!!!}

% In \cite{corridors_mono_nav_2009}, the authors presented a system for monocular UAV navigation and demonstrated autonomous flight and map-building in an office corridor.

% As in \cite {from_monoslam_to_explo, los_maps}, we also use 
% The mapping method proposed in this paper constructs free space in a fundamentally different, less conservative way than in \cite{from_monoslam_to_explo, los_maps}.
To build a graph-of-spheres representation directly out of sensory data, one must build the map in a completely different manner than for a grid-based map.
% It interpolates the depth between the sparse SLAM points and samples free-space spheres instead of casting rays of free space.
We propose to interpolate the depth between the sparse SLAM points and sample free-space spheres instead of casting rays of free space.
Additionally, we present a method for sampling free space in wide open areas where objects are too far to be accurately triangulated. 
In special cases, this can cause the free space to be overestimated, but it allows the UAV to fly where the sparse raycasting approach \cite{from_monoslam_to_explo, los_maps} would never estimate enough free space to even begin navigation. 
% This sampling approach allows the UAV to fly where the sparse raycasting approach would never estimate enough free space to even begin navigation. 
% Additionally, we show that when this is coupled coupled with a perception-aware path planning and exploration, the UAV can navigate safely even when occasionally overestimating free space.
% We show that when coupled with perception-aware planning and an exploration approach suited for the novel map representation, the UAV can safely and completely explore unknown environments even when occasionally overestimating free space.
% This mapping approach is tightly coupled with perception-aware planning and an exploration approach suited for the novel map representation, which handles the occational overestimation of free space.
% This mapping approach is tightly coupled with perception-aware planning and an exploration approach suited for the novel map representation, which handles the occational overestimation of free space.
Our proposed perception-aware exploration strategy, tightly coupled with the mapping method, handles the free-space inaccuracies and achieves safe and complete exploration.
% Thus, the proposed method allows safe and complete explore unknown, multi-scale, indoor/outdoor environments, even without dense sensors or GPU-based depth estimation.
% In special cases concerning textureless objects, this can cause the UAV to 
% Compared to all the mentioned approaches to 3D monocular mapping and exploration, we demonstrate that our approach achieves exploration and navigation in 3D using only a monocular camera and an IMU in outdoor, large-scale environments, with all of the computations running on-board the UAV, without requiring a GPU.
% PIPELINE IMG%%{
\begin{figure*}[!htb]
  \def\subfigwidth{0.24\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_968_poly1_A.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_970_poly2_A.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_969_poly1_B.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_971_poly2_B.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_914_right_poly.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_927_right_free2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \caption{
    Illustration of the mapping pipeline. 
    (a) The stable 3D points (red) used by the monocular SLAM are the only input distance measurements. 
    Delaunay triangulation is performed on the input points projected to 2D as a rough depth interpolation. 
    % Additionally, , the TODO-fake points are added to the depth estimation.
    (b) The active assumed-freespace points (blue, see \autoref{sec:fake}) above the ground, where the SLAM cannot accurately localize any points, are added to construct the
    (c) resulting visible free-space polyhedron $\mathbf{P}_f$.
    % (d) Side view of how new spheres are sampled in the polyhedron and added to the others, as well as how new obstacle points are added to the map.
    (d) New spheres are sampled in the polyhedron and added to the graph, and the visible SLAM points are added to the set of obstacle points $\mathbf{X}$.
    % TODO - desc + delaunay w fake fspace vis
  }
  \label{fig:mapping_pipeline}
\end{figure*}
% %%}

\section{Sphere-Based Mapping Using Sparse Visual SLAM Points}
% \section{Direct Sphere-Based Mapping from Sparse Pointclouds}
\label{sec:map_building}


% In this section, we explain how such map can be built directly from the outputs of visual SLAM, without needing any intermediate occupancy grid computation, and how the sparsity and high depth uncertainty of the input pointclouds is handled. 
% In this section, we first explain the two fundamental features of our method for handling the problems of using sparse monocular SLAM keypoints as depth measurements.
% Th
% Then, we describe the process of constructing the map.
% This section describes the proposed mapping approach.

% This section describes the proposed mapping approach.
% In \autoref{sec:structure}, we introduce the novel spatial representation and its properties.
% In \autoref{sec:polyhedron}, we construct an estimated visible free-space polyhedron using motion information and SLAM points available at a given time.
% In \autoref{sec:distance-based}, the free-space polyhedron and keypoint measurement distances are used to determine which points are added or deleted from the map.
% Next, in \autoref{sec:existing_spheres_update}, we describe how the free-space polyhedron is used to update the radii of existing and newly sampled free-space spheres.
% % As the last step, in \autoref{sec:existing_spheres_update}, we update the connections of modified spheres and prune the sphere graph to keep it sparse.
% As the last step, in \autoref{sec:graph_update}, we update the connections of the modified spheres and prune the sphere graph to keep it sparse for rapid path planning.

% \subsection{Map Data Structure}
% \subsection{Free-Sphere and Obstacle-Point Map Representation}
% \subsection{Map Representation and Notation}
% \label{sec:structure}
% We propose to represent free space by a graph of intersecting spheres.
The proposed mapping approach represents free space by a graph of intersecting spheres.
\todo{kdtree of centers}
% and obstacles as a set of 3D points corresponding to triangulated visual keypoints from visual SLAM.
Each sphere at the $k$-th update iteration has a static center $\mathbf{c}$ and changing radius $r_k$.
The radius $r_k$ represents the distance from $\mathbf{c}$ to the nearest unknown space or obstacle at time step $k$.
% We maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres, and use it for path planning.
To allow rapid path planning, we maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres.
% Furthermore, we maintain a set of obstacle points $\mathbf{X}$, which correspond to the textured surfaces measured by the visual SLAM.
% We represent obstacles by a set of points $\mathbf{X}$, which are .
To represent obstacles, we accumulate the stable (i.e. with low position covariance according to the SLAM) 3D points estimated by the monocular SLAM into a set of points $\mathbf{X}$, based on criteria described in \autoref{sec:distance-based}.
% For simplicity, we do not store the position covariances of the points given by the SLAM.
% Instead, we parametrize the minimum distance $\rho_{min}$ between any two obstacle points.
% Instead, we set a minimum distance $\rho_{min}$ between any two obstacle points as a parameter.
% For simplicity, we parametrize the minimum distance $\rho_{min}$ between any two obstacle points instead of maintaining the position covariance of each SLAM point.
% For simplicity, we do not store the position covariance for each point, but rather we fuse points together up to a user-defined distance $\rho_{obs}$.

% For each obstacle point $\mathbf{x} \in \mathbf{X}$, we store the lowest distance that the point was measured from $d_{\mathbf{X}, \min}$.
% A lower minimal measurement distance of a visual keypoint means that the point is less likely to be noise and we make it more difficult to erase it from the map.
% This is a necessary feature for safe flight when using sparse visual SLAM keypoints as the only depth information, as explained more in \autoref{sec:distance-based}. 
The mapping runs onboard the UAV in real-time, using only the estimated pose and 3D points from monocular-inertial SLAM as its inputs.
In this paper, we use the sparse OpenVINS \cite{openvins} as the SLAM frontend, but our method could work with any other monocular-inertial SLAM.
A single map update iteration consists of the following steps in order:
\subsection{Constructing the Visible Free-Space Polyhedron}
\label{sec:polyhedron}
The first step is to construct a polyhedron of space that is estimated to contain free space based on the information in the current timestep.
To interpolate depth between the sparse SLAM points, we employ a simplified version of the approach described in FLAME \cite{flame}.
There, the authors focus on constructing a precise mesh from visual keypoint measurements
In this paper, we care primarily about mapping the free space for navigation purposes, and thus we do not perform the mesh optimization or split the 2D interpolations as in \cite{flame}.

We project the currently tracked visual SLAM points into the image plane and compute their Delaunay triangulation. 
By connecting the points in 3D according to their Delaunay triangulation in 2D, we obtain a mesh $\mathcal{F}_d$ that we call a \textit{depth mesh}, which is used only for the current frame.
We estimate the currently visible free space as the volume between the camera's position $\mathbf{p}_{cam}$ and all points on $\mathcal{F}_d$.
This space is enclosed by connecting all points that lie on the edge of $\mathbf{F}_d$ to the camera's focal point, which forms an \textit{estimated visible free space polyhedron} $\mathbf{P}_f$, illustrated in \autoref{fig:distbased}.

\subsection{Estimating Free Space in Open Areas}
\label{sec:fake}
When moving in open areas, such as a grassy field without vertical obstacles, visual SLAM often tracks only a few keypoints on the ground, as illustrated in \autoref{fig:mapping_pipeline}.
% This would \todo{normally} cause free space to be estimated only below the UAV and not allow flight across the field.
Using standard free-space estimation methods, this would cause free space to be estimated only below the UAV and not allow flight across the field.
% This causes free space to be estimated only below the UAV and not allow flight across the field.
% Using standard methods of free-space estimation, 
% To allow safe flight in the absence of visual keypoints in front of the UAV, we keep track of a set of \textit{virtual keypoints} in front of the UAV.
% \todo{mention unknown $=$ free?}
% To allow safe flight in open areas, we make the following assumption: 
% To allow safe flight in open areas, we make the following assumption: 
To allow safe flight in open areas, our method estimates additional free space based on the following assumption: 
% \todo{reformulate - consider an area of the image -> fake depth -> but up to some max dist -> see image}
Consider a virtual keypoint $\mathbf{x}_{vir}$ at time $t$ that has been in the UAV's FoV from a given time $t-x$ in the past up to $t$ (i.e. it could have been tracked by visual SLAM).
Then, if there is a point on the UAV's trajectory between $t-x$ and $t$ such that there would be enough parallax for estimating the distance of $\mathbf{x}_{vir}$ (according to a user-defined threshold), and if the visual SLAM frontend has not estimated any stable 3D point at approximately that position, then $\mathbf{x}_{vir}$ must lie in free space.
% \todo{reformulate - "second order of depth estim = guessing?"}
% This approximation does not hold close to large textureless walls, but works quite well in the vast majority of real-world environments.
% This assumption does not hold close to large textureless objects, and free space could thus be wrongly estimated in the rare cases when such objects occur.
This assumption does not hold close to large textureless objects.
Therefore, free space could thus be wrongly estimated in the rare cases when such objects occur in the real world.
However, this could be easily solved by adding low-cost, short-range distance sensors (e.g. a single-point LiDAR or ultrasonic sensor), which are otherwise not usable for traditional dense raycast-based 3D mapping, but that would be outside the scope of this paper.
% However, when coupled with perception-aware flight
% However, such features are uncommon in most large-scale real-world environments.
% \todo{tadyten assumption jeste lip specifikovat a hodit mozna na zacatek / na konec k limitations}

% We keep a set of virtual points $\mathbf{x}_vir$ in front of the UAV at all times, and if some of the points become are estimated as free based on the above assumption, 
We periodically check the above described condition for a set of virtual points $\mathbf{x}_{vir}$ at a fixed distance in front of the UAV (see \autoref{fig:mapping_pipeline}) at each mapping iteration. 
This is only done for a few points, as checking the condition for all points in the UAV's FOV would be computationally demanding. 
% We take the points that fulfill this condition and add them to the creation of the \textit{depth mesh} $\mathbf{F}_d$.
The points that fulfill this condition are added to the creation of the \textit{depth mesh} $\mathbf{F}_d$.
% To further mitigate the depth estimation problem near textureless areas,
% we discard points such points that would fall into the Delaunay triangulation of the true obstacle points, since there we have a better depth estimate from the SLAM points.
However, we discard points that would fall into the Delaunay triangulation of the true obstacle points, since there we have a \todo{better} depth estimate from the SLAM points.
This also helps to correctly estimate depth when there is at least a little amount of visual texture near to textureless areas.
% This way, the problem of large featureless objects is mitigated in the majority of cases, when textured objects are visible nearby. 
% This way, we can safely estimate additional freespace, in the case of sufficiently textured wide outdoor areas.
% \todo{tadyten assumption jeste lip specifikovat a hodit mozna na zacatek / na konec k limitations}
% These virtual points are only used for estimating free space, and are not added as obstacles and cannot be used to delete existing points

% The visual SLAM usually has 

\begin{figure}[!h]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/distbased9.pdf}
  \centering
  \caption{Top-down illustration of sphere sampling and map point management: 
  The visible free-space polyhedron $\mathcal{P}_f^{t_2}$ is created using the currently tracked SLAM points (black crosses) and the camera's position $\mathbf{p}_{cam}^{t_2}$ at time $t_2$.
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  The size of a point $\mathbf{x}$ negatively corresponds to the distance $d_{\mathbf{x}}$ that it was measured from.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black), as there are better localized (smaller) points near them.
  The (dashed red) point seen far away from the camera at a previous time $t_1$ now lies in $\mathcal{P}_f^{t_2}$ close to the camera and will thus be deleted as noise.
  % The (dashed red) point will be deleted, as it lies in the free-space polyhedron at $t_2$, and it .
  The (dashed black) two points observed far from the camera at $t_2$ are not added to the map, as there are more precisely localized (red) points close to them.
  % New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  % New spheres are inscribed inside the visible free-space polyhedron $\mathbf{P}_f$.
  % Visible free-space polygon $\mathbf{P}_f$ (-);
  % Newly sampled spheres (O);
  % Triangulated keypoints visible at current frame (X); 
  % Points already in the map (X) added from a previous pose (red). 
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black).
  }
  \label{fig:distbased}
\end{figure}


\subsection{Updating Obstacle Points}
\label{sec:distance-based}
As the next step of the update, we decide which tracked SLAM points to add into the map points $\mathbf{X}$, and which points already in the map to delete.
An important part of the proposed mapping method is that we store the minimum of the distances that any point $\mathbf{x}$ has been observed from, denoted further as $d_{m, x}$.
% This is due to the fact that the distance uncertainty of any point \todo{grows drastically} with the distance from camera in monocular SLAM \cite{inverse_depth}.
\todo{
An apparent problem can arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 20m).
% The visible points corresponding to the wall might not be detected as SLAM keypoints while p.
A situation can occur where the previously mapped points are now triangulated at an incorrect position, or are not triangulated by the visual SLAM at all (e.g. due to insufficient parallax), while some other points that lie beyond the original points are triangulated.
}
% We also cannot simply delete every map point that falls into $\mathbf{P}_f$, because the UAV might be at a distance where the object corresponding to the point is just too far to be seen as a keypoint in the image.
Thus, we cannot simply delete every map point that falls into $\mathbf{P}_f$, because then the original points (small red points in \autoref{fig:distbased}) would be wrongly deleted and assumed as free space.

We solve this problem by deleting any map point $\mathbf{x}$ that falls into $\mathbf{P}_f$ only if
\begin{equation}
  |\mathbf{x} - \mathbf{p}_{cam}| < \alpha \cdot d_{m,x}
\end{equation}
where $p_{cam}$ is the position of the camera and $\alpha$ is a paremeter, which is usually set to $1.1$ to account for distance measurements uncertainty.
Thanks to this condition, when a small obstacle seen previously from a short distance is currently not visible when further away, but objects are visible behind it, that obstacle will not be wrongly deleted from the map.
% where $p_{cam}$ is the position of the camera and $\alpha$ is a paremeter.
% We usually set $\alpha$ to $1.1$ to account for distance measurements uncertainty.
% We usually set $\alpha$ to $1.1$ to allow the mapping to modify points for a while when moving away from them.
% The parameter $\alpha$ could be set to $1$ to allow the mapping to modify points for a while when moving away from them.
% A higher value of $\alpha$ can be set 
The points that lie in the free-space polyhedron $\mathbf{P}_f$ and are not deleted are called \textit{protected points} $\mathbf{X}_p$.
These points are used to constrain sphere radii in the following update step.

Additionally, to prioritize closer measurements, points seen from a closer distance re-lace points seen from larger distances, if they are measured close enough to them. 
In the same way, new points seen at a large distance are not added to the map, if there are more accurately measured points near them, which is also visualized in \autoref{fig:distbased}. We enforce this through a simple algorithm shown in 
\todo{TODO - rewrite as sort?}

\subsection{Updating Existing Spheres}
\label{sec:existing_spheres_update}
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the protected points $\mathbf{X}_{p}$.
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the input points $\mathbf{X}_{in}$.
Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the input points $\mathbf{X}_{in}$.
To bound the update time of this step, we check the largest sphere radius $r_{max}$ in the map, which allows us to quickly filter out all spheres whose centers fall outside a bounding box around $\mathbf{P}_f$, inflated by $r_{max}$. 
Then, for any remaining sphere with a center $\mathbf{c}$ and radius $r_{k}$, the updated radius is computed as
\begin{equation}
  r_{k+1} = \min \left( \max \left( r_{k}, d(\mathbf{c}, \mathbf{P}_f) \right)
  , d(\mathbf{c}, \mathbf{X}_{in} \cup \mathbf{X}_p) \right).
  % r_{k+1} = \min \left( d(\mathbf{c}, \mathbf{X}_{in} \cup \mathbf{X}_p)
  % , \max \left( r_{k}, d(\mathbf{c}, \mathbf{P}_f) \right) \right).
  \label{eq:update}
\end{equation}
% where $d(x, \mathbf{P}_f)$ is the signed distance to $\mathbf{P}_f$ (positive if the point is inside the polyhedron, negative if outside) \todo{and} $d(\mathbf{x},\mathbf{X}_{in} \cup \mathbf{X}_p)$ is the minimum distance to all input obstacle points $\mathbf{X}_{in}$, and to the protected map points $\mathbf{X}_p$ described in \autoref{sec:distance-based}.
In this equation, $d(x, \mathbf{P}_f)$ is the signed distance to $\mathbf{P}_f$ (positive if the point is inside the polyhedron, negative if outside).
The value of $d(\mathbf{x},\mathbf{X}_{in} \cup \mathbf{X}_p)$ is the minimum distance to all input obstacle points $\mathbf{X}_{in}$, and to the protected map points $\mathbf{X}_p$ described in \autoref{sec:distance-based}.
Essentially, this equation means that the polyhedron $\mathbf{P}_f$ is used to increase the radius of a sphere, whereas the protected $\mathbf{X}_p$ and input points $\mathbf{X}_{in}$ are used to constrain it.
As the last part of this step, we delete all spheres with $r_{k+1} < r_{min}$, where $r_{min}$ is the smallest \todo{allowed} sphere radius specified by the user.
% , usually slightly lower than the UAV's minimal allowed distance to obstacles. 

% where TODO.
\subsection{Sampling New Spheres}
\label{sec:adding_spheres}
To introduce new spheres into the map, we sample a fixed number of points inside $\mathbf{P}_f$ at random distances between the camera and the depth mesh $F_o$.
% \todo{This is a simplistic approach, and could be improved for faster flight, for example by sampling along the predicted trajectory of the UAV at high speeds.}
A potential new sphere's radius is determined in the same way as for the old spheres in the previous step in Eq. \ref{eq:update} with $r_k = 0$.
If the potential radius is larger than $r_{min}$, the sphere is added to the map.

\subsection{Recomputing and Sparsifying Sphere Graph}
\label{sec:graph_update}
After all the sphere radii updates have been made, we update the graph of spheres used for path planning, so that all intersecting spheres are connected in the graph.
% TODO - EXPLAIN???
Furthermore, to constrain map update time and path planning time, we perform a redundancy check on the updated and added spheres in the same way as in our previous work \cite{spheremap}.
If any sphere is found to be redundant, it is deleted from the map.
% \todo{this approach is important to cover large...}
This step is important to cover large open areas by only a few spheres to allow rapid planning, and tight corridors by a higher density of spheres, capturing the information about potential paths and distances in more detail.
% This way, large open areas are covered by only a few spheres and can be planned over quickly, and tight corridors have a higher density of spheres, capturing the information about potential paths and distances to obstacles in more detail.

% \section{Monocular-Inertial Exploration Using the Sphere-Based Map}
% \section{Safety-Aware Monocular-Inertial 3D Exploration}
% \section{Safe Monocular-Inertial Exploration using the Sphere-Based Map}
\section{Perception-Aware Exploration using the Sphere-Based Map}
In this section, we detail how the proposed mapping method is highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a single camera \todo{and IMU?}.
In principle, we employ the commonly used next-best-view (NBV) \cite{paper_frontier_grandpa} strategy, which has so far been deemed unsuitable for monocular exploration in existing works \cite{from_monoslam_to_explo, los_maps}.
% We introduce several major differences in methodology that are neccessary in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments, detailed in the following sections.
% In the following sections, we introduce several major differences in methodology that are neccessary for NBV in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.
In the following sections, we introduce several major differences in methodology that are critical for the NBV strategy in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.
% \todo{write assumptions on perception here? mapping doesnt care}

\subsection{Frontier Sampling on Free-Space Polyhedron}
Firstly, since the proposed map representation is fundamentally different from an occupancy grid, we need to detect frontiers (i.e. the boundary of free and unknown space) in a different way than with an occupancy grid.
We propose to sample points along the visible free-space polyhedron described in \autoref{sec:map_building} at each map update, and add the points as frontiers, if they do not lie inside any free-space sphere and if they are at some user-defined distance from all map obstacle points.
If they do not meet these criteria in any following update, they are deleted. 
% \todo{complexity and vs octomap?}

In enclosed spaces, which are usually considered in autonomous exploration literature, all frontiers are usually worth exploring.
However, in outdoor exploration with areas of open space, it is also important to assign different value to different frontiers, especially for UAVs. 
We proposed to constrain the exploration to only consider frontiers that lie near some obstacle points.
This way, the UAV does not explore up into the sky.
Additionally, this forces the UAV to only explore near texture-rich areas, thus avoiding losing visual odometry tracking.
% \todo{Lastly, we add new exploration viewpoints into the map only if a sufficient number of such frontier points would be visible from a given viewpoint at a close distance.}
These frontier points are used to generate exploration viewpoints such that at least some frontier points are visible form each viewpoint.

% EXPLORATION EXPLANATION IMG%%{
\begin{figure}[!htb]
  \def\subfigwidth{0.9\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_923.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_930.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_925.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/exploration/Selection_931.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
  \caption{
    Illustration of the exploration approach. The UAV moves to a selected exploration viewpoint (yellow arrows).
    % As described in \autoref{sec:align}, 
    The planned path aligns the UAV with the viewpoint's direction before reaching it so that the monocular SLAM has enough parallax to estimate distances to textured surfaces in that direction.
    The frontier points (purple) near textured surfaces are replaced with newly added obstacle points (red).
    % As explained in \autoref{sec:blocking}, the UAV also blocks viewpoint sampling near visited viewpoints (large black arrows).
    The UAV also blocks viewpoint sampling near visited viewpoints (large black arrows) to avoid returning to viewpoints facing textureless areas.
  }
  \label{fig:exploration_explanation}
\end{figure}
% %%}


\subsection{Forced Translation when reaching Viewpoints}
\label{sec:align}
In traditional exploration approaches with dense distance sensors, it is sufficient to move the robot to a frontier, aim the sensors towards the unknown space, and assume that the distance sensors will uncover some additional space behind the frontier and thus expand the map.
This approach will not, in principle, work well with a robot relying on a monocular camera that estimates depth from motion.
Such a robot requires \textit{translational} motion to obtain correct distance estimates of visual keypoints. 
% as in monocular structure-from-motion (SfM) TODO-CITE (we do not consider deep learning methods which exploit knowledge about sizes of objects).
% Excessive rotation without translation cannot lead to reasonable distance estimates.
% Too much rotation with too little translation of the camera will cause the monocular SLAM or any SfM-based approach to not give reliable depth estimates.
With rotation only, or motions that have too much rotation compared to translation, reliable depth estimates cannot be obtained.

To solve this, we propose the following perception-aware planning approach: 
When finding a path to a goal viewpoint that could uncover some frontiers, we compute the target headings on the path so that the UAV maintains a fixed heading in the viewpoint's direction for at least $d_{c}$ meters on the path before reaching the viewpoint, as visualized in \autoref{fig:exploration_explanation}.
% In the experiments detailed in Sec. \autoref{sec:experiments}, we set $d_{c}=TODO$.
This way, the UAV has enough translational motion before reaching a given viewpoint.
Therefore, if there are textured surfaces visible from that viewpoint, the UAV will most likely have enough parallax to estimate their distances.

\subsection{Explored Viewpoint Blocking}
\label{sec:blocking}
% This is an important part of the method also due to the second major distinction -- we block the sampling of new exploration viewpoints near explored viewpoints.
Another necessary distinction for NBV exploration with a monocular camera is that the UAV blocks sampling of new exploration viewpoints near visited viewpoints.
This is due to the fact that real-world environments often contain textureless areas (mainly in buildings or other man-made structures).
Since obstacle points correspond to textured points triangulated by the visual SLAM, no points ever will be added on textureless surfaces.
Frontier points will be generated there instead, since the surfaces lie on the edge of the visible free space polyhedron and can be far enough from any obstacle points.
% but they will be on the boundary of free-space, so frontiers will be generated there.
However, such frontiers cannot uncover any new space, since there is nothing behind them
For this reason, we block the sampling of new viewpoints near visited viewpoints, visualized as large black arrows in \autoref{fig:exploration_explanation}, so that the UAV doesn't keep coming back to look at a blank wall.
% We also put additional blocked viewpoints into the map based on where the UAV flies.
% \todo{we also add 

% \subsection{Local-Global Exploration}
% To allow exploration even in large-scale environments, we employ a local-global approach as in \cite{beneath} and \cite{dang2020graph}.
% The UAV first tries finding paths to any exploration viewpoints up to a user-defined "local exploration radius".
% If no reachable viewpoints are found in the radius, the UAV tries finding paths to all exploration viewpoints stored in the map.
% Thanks to the planning efficiency of the sphere graph, this global replanning usually does not take more than a few seconds, but could be made even faster by implementing any sort of long-distance planning abstraction graph, as in \cite{spheremap, topomap}.

\subsection{Safety-Aware Planning}
% Lastly, using the sphere-based representation allows us to flexibly weigh path length and path proximity to obstacles. We use the same path cost criterion as in \cite{spheremap}, and thus we can 
% Lastly, because the free space is represented by a graph of spheres, we immediately have an upper bound on the distance of obstacles at each graph node, and thus it is possible to efficiently weigh path length versus path proximity to obstacles, 
% As a last safety measure, the UAV is controlled to fly in the direction of its camera, when not aligning itself with a viewpoint as described in \autoref{sec:align}.
% Because of the properties of visual SLAM,
Lastly,
the UAV is controlled to always fly in the direction of its camera when not aligning itself with a viewpoint as described in \autoref{sec:align}.
% Free-space might always be wrongly initialized, but when flying in the direction of the camera, and with quick periodic replanning, the risk of collision can be minimized.
This approach is crucial for ensuring collision-free flight in the case of incorrectly initialized free space.
As discussed in \todo{ref}, some obstacles (e.g. thin wires, twigs) might not be visible in the cameras from a large distance to be detected by the visual SLAM (especially when using fisheye cameras), and can be wrongly estimated as free space.
Using the forward-facing flight, the UAV has a higher chance to see these obstacles that were previously not detected, and quickly replan.

To further increase safety and allow agile maneuvers, the UAV also plans paths with a strong preference for high distance from obstacles, for which the graph-of-spheres representation is highly suitable, as described in \cite{spheremap, bubbleplanner, spheregraph}.
Furthermore, 
the measurement distance of free-space spheres could be used for adaptively decreasing the UAV speed in areas with higher probability of non-detected or wrongly localized obstacles, but we leave this for future work.
% With all of these considerations, a UAV with only a monocular camera and IMU can perform 3D volumetric exploration in large-scale outdoor environments, as we demonstrate experimentally in Sec. \autoref{sec:experiments}.
% Furthermore, similarly as we did in the case of LiDAR-based exploration in DARPA SubT scenarios,
% As future work for increasing the safety even more, the measurement distance of free-space spheres could be stored and used.
% With this information, trajectory generation could, for example, force the UAV to fly slower through spheres that have been observed from a large distance and might contain some obstacles not observable from far away.
% \todo{As future work for increasing the safety even more, the measurement distance of free-space spheres could be used for decreasing the UAV speed in areas with higher probability of overlooked or wrongly localized obstacles.}

\section{Experiments}
\label{sec:experiments}
In this section, we present the performed experiments and show the performance of the proposed method in large-scale real-world (Sec. \ref{sec:real_exper}) and simulated (Sec. \ref{sec:sim_exper}) environments.
Our implementation of the proposed mapping and exploration methods is single-threaded and written in python.
As shown in Sec \ref{sec:runtimes}, even with this unoptimized implementation, the mapping keeps below $1s$ per map update on a standard CPU, making it suitable for real-world deployment.

The UAV used in the experiments was equipped with a monocular global-shutter grayscale fisheye Bluefox camera, and \todo{TODO} IMU.
The MRS UAV system \cite{mrs_uav_robust_system} was used for trajectory generation and control.
\todo{baca jint paper}
% Damping in the form of 3D-printed flexible elements 
In real-world experiments, the camera and IMU are separated from the rest of the UAV by 3D-printed damping elements, which significantly reduces the IMU noise caused by propellers and improves the SLAM performance.
For state estimation and as the source of sparse visual SLAM points, we used OpenVINS \cite{openvins} in the inverse-depth measurement mode, without loop closures.
However, any other monocular-inertial SLAM could be used as the fronted of our method, as long as it tracks points approximately uniformly in the camera's FOV.
This is so that the assumption that small objects will be detected at a close range, discussed in \autoref{sec:distance-based}, is fulfilled.
% However, any other monocular-inertial SLAM could be used as the fronted of our method, as long as it has the features described in \autoref{sec:
% \todo{ale muzem pouzit cokoliv}.


\subsection{Real-World UAV Monocular-Inertial Exploration}
\label{sec:real_exper}
\todo{openvins used as odom so they dont think gps}
One of the realized real-world experiments is illustrated in \autoref{fig:exper_real}.
% In this experiment, the UAV explored up to $\SI{60}{\meter}$ forward around the side of an abandoned farmhouse fully autonomously in an $8\text{min}$ mission.
In this experiment, we bounded the area for generating exploration goals to a $\SI{70x10x8}{\meter}$ bounding box around the side of an abandoned farmhouse, and the UAV fully autonomously explored nearly all the available space in this region.
The UAV successfully avoided and mapped all the debris, bushes and the house walls in the area, and when battery levels were getting low, it autonomously returned to the starting position.
Also note in \autoref{fig:exper_real} that the UAV explored a considerable amount of space in the open field to the left of the house.
This was made possible by our approach to safe estimation of free space in the absence of triangulated SLAM keypoints, described in \autoref{sec:polyhedron}.
% The UAV is running OpenVINS \cite{openvins} for odometry estimation, and our proposed inverse depth etstimator and mapper modules.

We have tested the proposed approach extensively in a variety of other real-world environments, including a forest path, a tree in a wide open field, and around a large house.
Videos from all real-world experiments are available at TODO-YOUTUBE-LINK. 
\todo{zkratit, neprisli jsme nato u testovani ale vime o tom a jak to resit (future work - lajny / shortrange sensory a pouzivat tohle na longrange)}
\todo{Some of these experiments required the safety pilot to take control due to an important limitation of the method that we encountered during testing --- 
since the visual SLAM tracks and triangulates visual key\textit{points}, visual \textit{lines} (caused by e.g. thin tree branches or smooth manmade poles) are not sensed in our mapping pipeline.
This caused the UAV to nearly crash into tree crowns multiple times.
This limitation could be resolved by integrating short-range depth sensors as well, even ultrasound sensors could potentially be fused into the map.
Additionally, the visual SLAM could be modified to estimate 3D poses of lines as well, as for example in TODO-CITE, but we leave this for future work.
}

% \begin{figure}[!htb]
%   % \includegraphics[width=8.5cm]{fig/exper1.png}
%   \includegraphics[width=8.5cm]{fig/exper4.png}
%   % \includegraphics[width=8.5cm]{fig/exper3.png}
%   % \includegraphics[width=8.5cm]{fig/exper4.png}
%   \centering
%   \caption{TEMPORARY IMG - Exploration experiment visualization}
%   \label{fig:exper_real}
% \end{figure}

\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t126.png}};
      % \draw [latex-latex](3.2,0.9) -- (3.2,6.8);
      % \node[align=center] at (3.6, 4) {\scriptsize \color{black}\SI{70}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=126\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t206.png}};
     %    \begin{scope}[x={(a.south east)},y={(a.north west)}]
     %      \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=206\text{s}$};
     %    \end{scope}
     %  \end{tikzpicture}
     % \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t370.png}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=370\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t516.png}};
      \draw [latex-latex](4.05,0.4) -- (4.05,7.3);
      \node[align=center] at (3.6, 3.4) {\scriptsize \color{black}\SI{80}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=516\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=90, origin=c, width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/house.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=180, origin=c, width=1.0\linewidth, trim={{0.15\width} {0.3\height} {0.2\width} {0.0\height}}, clip=true]{fig/house.png}};
        % \begin{scope}[x={(a.south east)},y={(a.north west)}]
        %   \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=516\text{s}$};
        % \end{scope}
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    Visualization of the real-world autonomous exploration experiment described in \autoref{sec:real_exper}, along with a satellite image of the explored abandoned farm area.
  }
  \label{fig:exper_real}
\end{figure}


\subsection{Large-Scale Exploration in Simulation}
\label{sec:sim_exper}
% In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.
In this experiment, we showcase the capabilities of our approach in a simulated $\SI{120x120x20}{\meter}$ urban area.
The experiment was conducted in the Gazebo simulator, with essentially the same UAV and sensory setup as in the real-world experiments.
\autoref{fig:exper_sim} shows the resulting map of an example exploration mission.
After running the exploration mission 20 times, the UAV managed to explore the entire environment in 18/20 runs.
In the remaining 2 runs, the UAV crashed either into a large featureless metal pole or the roof of a gazebo where the texture only consists of lines.
The crashes were caused by the same limitation as discussed in \autoref{sec:real_exper}.
Aside from this limitation, the mapping and exploration can be said to work robustly in static environments.
Note also that in the experiment shown in \autoref{fig:exper_sim}, the UAV explored up on the roofs of the buildings when it was able to use the large towers to estimate depth and create enough free space for safely moving up to the roof.

\begin{figure}[htb]
  \def\subfigwidth{0.89\linewidth}
  \centering
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/sim_exper_gazebo.png}};
     %  \end{tikzpicture}
     % \end{subfigure}
     % \hfill
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/sim_exper_end.png}};
     %  \end{tikzpicture}
     % \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/flood3.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/flood_end.png}};
      \end{tikzpicture}
     \end{subfigure}
  \centering
  \caption{The resulting map (bottom) after 20min of exploring an urban environment (top) in simulation, explained in more detail in \autoref{sec:sim_exper}. Black arrows indicate visited frontier viewpoints, frontiers are shown as purple points. Notice that the UAV also went to explore up on the rooftops, where it could see the metal towers.}
  \label{fig:exper_sim}
\end{figure}


\subsection{Runtime analysis}
\label{sec:runtimes}
% Here we analyze the runtimes of this method, and TODO.
To provide more insight into the real-time capability of the proposed mapping method, we measured the runtimes of individual parts of the map update iterations during one of the simulated exploration missions described in \autoref{sec:sim_exper} and visualized in \autoref{fig:exper_sim}. 
The resulting graph is shown in \autoref{fig:runtimes}.
In that mission, the UAV explored the entire available environment.
Thanks to only updating the map near the UAV, the update time remains relatively bounded, even when the visible free-space polyhedron is large.


\begin{figure}[!htb]
  % \includegraphics[width=8.5cm]{}
  \adjincludegraphics[width=9cm, trim={{0.05\width} {0.00\height} {0.05\width} {0.00\height}}, clip=true]{fig/smap_runtimes_tmp.pdf}
  \centering
  \caption{Runtime analysis of parts of the map update detailed in \autoref{sec:map_building}. TODO-MEANINGS OF PARTS + REDO WITH FIXED DATA}
  \label{fig:runtimes}
\end{figure}



\section{CONCLUSION}
In this paper, we proposed a novel approach to building a 3D map for navigation using a graph of spheres instead of a traditionally used voxel grid as the base representation, and presented how to use such a map for safe exploration. 
Our method only requires a monocular camera and an IMU, and can run on a standard onboard computer without needing a GPU.
Compared to the existing approaches to 3D monocular mapping and exploration, which have so far presented autonomy only at the scale of a few indoor rooms \cite{from_monoslam_to_explo, los_maps,deep_mono_depth_2021, deep_mono_depth_2019}, our proposed approach enables autonomous exploration in large multi-scale indoor/outdoor environments, with all of the computations running on-board the UAV.

The proposed method opens the way for micro-scale UAVs operating fully autonomously in unknown indoor/outdoor large-scale environments with obstacles, but there are still many challenges to be solved.
Primarily, monocular depth estimation methods are not yet completely reliable, which can lead to crashes due to non-detected obstacles, even when using the perception-aware planning proposed in this paper.
As future work, we intend to complement the imperfections of the depth estimation by integrating low-cost, short-range distance sensors (e.g. ultrasound) into the mapping pipeline, thus detecting even thin and textureless objects.
% Furthermore, the proposed map representation could be extended with other geometrical primitives for clustering large areas of free or occupied space, thus increasing performance and allowing 
Additionally, we plan to use the proposed mapping approach for building submaps instead of a single large map and performing large-scale map optimization combined with vision-based place recognition to achieve low-cost autonomy at the scale of kilometers, which has only been demonstrated with LiDAR-based systems so far.
% and then perform large-scale map optimization using place recognition, 

% As we analyzed in this paper, 

% In future work, we intend to use  
% Thus, the proposed method 
% In this paper, we presented a novel sphere-based spatial mapping method 
% % together with a robust monocular inverse-depth estimator,
% using sparse point cloud data from monocular visual sensors,
% and demonstrated how it can enable any UAV equipped with a single camera and any form of odometry to quickly and safely navigate and explore unknown, unstructured 3D environments.
% We have demonstrated, through testing in simulation and in the real world, that our new mapping approach and spatial representation enable safe, large-scale autonomous navigation and exploration on UAVs without traditionally used expensive range sensors.

% The spatial representation proposed in this paper, consisting of spheres and points, offers an alternative to the traditionally used grid-based representations.
% We have presented a method of constructing this representation on-board a UAV, using only the keypoints from sparse monocular-inertial SLAM as input depth information.
% % We have experimentally demonstrated how this new representation allows a UAV to explore and navigate large-scale outdoor environments in the real-world.
% We have demonstrated how this new mapping approach enables 3D exploration and navigation in large-scale real-world outdoor environments on a UAV equipped with only a monocular camera and IMU as its sensors.


% Trough extensive experimental evaluation, we have showed the feasibility of doing pose estimation, autonomous mapping, and exploration on-board a single UAV equipped with a only a monocular camera and IMU as the used sensors.

% In future work, we intend to explore the potential of the proposed representation for multimodal sensing -- e.g. intelligently fusing points and spheres based on the modality that they were created from.

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word acknowledgment in America is without an e after the g. Avoid the stilted expression, One of us (R. B. G.) thanks . . .  Instead, try R. B. G. thanks. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, Synthetic structure of industrial plastics (Book style with paper title and editor), 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 1564.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, An approach to graphs of linear forms (Unpublished work style), unpublished.
% \bibitem{c5} E. H. Miller, A note on reflector arrays (Periodical styleAccepted for publication), IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleSubmitted for publication), IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style), IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, Infrared navigationPart I: An assessment of feasibility (Periodical style), IEEE Trans. Electron Devices, vol. ED-11, pp. 3439, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, A clustering technique for digital communications channel equalization using radial basis function networks, IEEE Trans. Neural Networks, vol. 4, pp. 570578, July 1993.
% \bibitem{c12} R. W. Lucky, Automatic equalization for digital communication, Bell Syst. Tech. J., vol. 44, no. 4, pp. 547588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, On the compatibility of adaptive controllers (Published Conference Proceedings style), in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 816.
% \bibitem{c14} G. R. Faulhaber, Design of service systems with priority reservation, in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 38.
% \bibitem{c15} W. D. Doyle, Magnetization reversal in films with biaxial anisotropy, in 1987 Proc. INTERMAG Conf., pp. 2.2-12.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, Radio noise currents n short sections on bundle conductors (Presented Conference Paper style), presented at the IEEE Summer power Meeting, Dallas, TX, June 2227, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, An analysis of surface-detected EMG as an amplitude-modulated noise, presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, Narrow-band analyzer (Thesis or Dissertation style), Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, Parametric study of thermal and chemical nonequilibrium nozzle flow, M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, Nonlinear resonant circuit devices (Patent style), U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
