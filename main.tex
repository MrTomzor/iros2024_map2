%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\UseRawInputEncoding
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\usepackage[utf8]{inputenc}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
% \usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
% \usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
% \usepackage{cite}

% \usepackage{amsmath} % assumes amsmath package installed
\usepackage{amsmath, bm} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage{siunitx}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{url}
\usepackage[]{hyperref}
% \hypersetup{
%   colorlinks,
%   citecolor=black,
%   filecolor=black,
%   linkcolor=blue,
%   urlcolor=blue,
%   pdfauthor={},
%   pdfsubject={},
%   pdftitle={}
% }
\usepackage{cite}

% REVISIONS
\newcommand{\fix}[1]{{\hypersetup{allcolors=blue}{\color{blue} {#1}}}}
% \newcommand{\vkcaption}[1]{{\color{blue} {#1}}}
\newcommand{\todo}[1]{{\hypersetup{allcolors=red}{\color{red} {#1}}}}
\newcommand{\todocaption}[1]{{\color{red} {#1}}}

% \newcommand{\vk}[1]{#1}
% \newcommand{\vkcaption}[1]{#1}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\simname}{\text{HARDNAV}}

% \newcommand{\real}{\mathbb{R}}

\title{\LARGE \bf
% Novel Benchmark for Navigation and Active Place Recognition in Confusing and Changing Environments
% Monocular-RGB Online 3D Sphere-Based Occupancy Mapping}
% Sphere-Based Occupancy Mapping for Fast and Safe Navigation and Exploration on Monocular-Inertial UAV Systems}
% Monocular-Inertial UAV Exploration of 3D Environments Using Lightweight Sphere-Based Occupancy Mapping}
% Monocular Sphere-Based Occupancy Mapping and Large-Scale Safety-Aware Exploration on a UAV}
% Sphere-Based Occupancy Mapping using Sparse Monocular SLAM for Robust Large-Scale 3D Exploration on Inexpensive UAVs}

% FINAL TWO
% Sphere-Based Mapping using Sparse Monocular SLAM Points for Robust Large-Scale 3D Exploration on Inexpensive UAVs}
% Monocular-Inertial Exploration of Large-Scale Outdoor Environments using Sparse SLAM Keypoints on an Inexpensive UAV}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Inertial UAV Exploration and 3D Mapping of Large-Scale Unstructured Environments using Sparse Visual SLAM}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by UAVs}
% Monocular-Inertial 3D Mapping for Exploring Large-Scale Unstructured Environments by Low-Cost UAVs}
% Monocular-Inertial Sphere-Based Mapping for a UAV Exploring Large-Scale Unstructured 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale Unstructured 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSphere: Monocular-Inertial Sphere-Based Mapping for UAV Exploration in Large-Scale 3D Environments}
% MonoSpheres: Monocular-Inertial Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSpheres: Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% Monocular-Inertial Sphere-Based Mapping for UAV Exploration of Large-Scale 3D Environments}
% MonoSpheres: Direct Sphere-Based Mapping for Large-Scale Monocular-Inertial UAV Exploration}
% MonoSpheres: Monocular-Inertial Sphere-Based Mapping for Large-Scale UAV Exploration}
% MonoSpheres: Sphere-Based Mapping with Sparse Depth for Large-Scale Monocular-Inertial 3D Exploration}
%
% MonoSpheres: Large-Scale Monocular UAV Exploration using Direct Sphere-Based Mapping}
% MonoSpheres: Large-Scale Monocular UAV Exploration through Perception-Coupled Mapping and Planning}
MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning}
% From Monocular SLAM to Large-Scale UAV Exploration}
% Large-Scale Monocular UAV Exploration using Optimistic Free-Space Estimation and Cautious Navigation}
% From Sparse Monocular SLAM to Large-Scale UAV Exploration using Tightly Coupled Mapping and Planning}
% MonoSpheres: Tightly Coupling Sparse Visual SLAM to 3D Mapping and Navigation for Large-Scale Monocular UAV Exploration}
% Tightly Coupling Sparse Visual SLAM to 3D Mapping and Navigation for Large-Scale Monocular UAV Exploration}
% Large-Scale Monocular UAV Exploration using Tightly Coupled Sphere-Based Mapping and Sparse Visual SLAM}
% Monocular UAV Exploration using Tightly Coupled 3D Mapping and Sparse Visual SLAM}
% Large-Scale Monocular Outdoor UAV Exploration using Tightly Coupled Mapping and Planning}
% From Sparse Monocular SLAM to Large-Scale Outdoor UAV Exploration using Perception-Specific 3D Mapping}
% Large-Scale Monocular UAV Exploration using Tightly Coupled Mapping and Planning}
%
% MonoSphere: Monocular-Inertial Mapping and Exploration for UAV Exploration in Large-Scale 3D Environments}
% Monocular-Inertial 3D Exploration of Large-Scale Outdoor Environments using Sparse Visual SLAM}
% Monocular-Interital Sphere-Based Mapping for Large-Scale UAV Exploration}

% % PolySphereMap - Occupancy Mapping using Sparse Monocular SLAM for Large-Scale 3D Exploration on Inexpensive UAVs}
% PolySphereMap - Mapping and Exploring Large-Scale 3D Environments using a Monocular Camera on a UAV}
% Sphere-Based Occupancy Mapping using Monocular Vision for Navigation and Exploration}
% Sphere-Based Occupancy Mapping using Monocular Vision for Rapid Safety-Aware Planning}
% Sphere-Based Occupancy Mapping using Sparse Pointclouds from Monocular Cameras}


% ORCID %%{
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
    svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
    svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
        \begin{tikzpicture}[yscale=-1,transform shape]
          \pic{orcidlogo};
        \end{tikzpicture}
}{|}}}}
% %%}

% \author{Tomas Musil, Matej Petrlik, Martin Saska% <-this % stops a space
% % \thanks{*This work was not supported by any organization}% <-this % stops a space
% % \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
% %         University of Twente, 7500 AE Enschede, The Netherlands
% %         {\tt\small albert.author@papercept.net}}%
% % \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
% %         Dayton, OH 45435, USA
% %         {\tt\small b.d.researcher@ieee.org}}%
% }
\author{
  % Tom\'{a}\v{s} Musil$^{\orcidicon{0000-0002-9421-6544}}$, 
  % Mat\v{e}j Petrl\'{i}k$^{\orcidicon{0000-0002-5337-9558}}$,
  % Martin Saska$^{\orcidicon{0000-0001-7106-3816}}$%
  Anonymous Authors
  % \thanks{%
  %   Authors are with the Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, 166 36 Prague 6, {\tt\footnotesize\{\href{mailto:musilto8@fel.cvut.cz}{musilto8}|\href{mailto:matej.petrlik@fel.cvut.cz}{matej.petrlik}|\href{mailto:martin.saska@fel.cvut.cz}{martin.saska}\}@fel.cvut.cz}
% }
  % \thanks{Digital Object Identifier (DOI): see top of this page.}
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with a single monocular camera and no dense range sensors.
  In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning.
  The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty.
  The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control.
  We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless areas are taken into account.
  We evaluate our approach extensively in various real-world and simulated environments and ablation studies.
  % To the best of the authors' knowledge, the proposed method is the first to achieve both indoor and outdoor 3D monocular exploration in unstructured environments up to 170x110m in scale. 
  To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments.
  % TODO - and sthg in simulation
  % and it consistently covers more area than the state-of-the-art method in various simulated environments.
  We open-source the code of the proposed method 
  to benefit the community.

Code--- %\href{
In attached multimedia materials

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
In recent years, significant advances have been presented in mobile robot autonomy, enabling robot systems to explore unknown 3D environments that span kilometers in scale \cite{darpa_cerberus_wins, csiro_darpa, beneath}. 
% These advances are critical for enabling deployment of UAVs in real-world, unknown environments, where global navigation satellite systems (GNSS) are partially or completely unavailable.
% \todo{These advances are critical for the deployment of UAVs in real-world missions, such as in urban areas after an earthquake, a forest during a wildfire, or exploring caves on other planets.}
% \todo{These advances are critical for allowing UAVs to be deployed for search-and-rescue missions in urban areas after natural disasters, monitoring wildfires, or mapping caves on other planets.}
These advances are critical for allowing the deployment of uncrewed aerial vehicles (UAVs) in real-world applications, such as search-and-rescue, wildfire response or planetary exploration.
% Thus, we are getting closer to swarms of UAVs that quickly create a map of an urban disaster site or a forest on fire for rescue teams.
% Thus, we could soon see swarms of UAVs that assist rescue teams by quickly mapping an urban disaster site or a forest on fire.
% This research could enable for example swarms of autonomous UAVs that assist rescue teams by quickly searching for survivors during natural disasters such as floods, earthquakes or wildfires. 
% Such environments include natural disaster sites 
% However, the state-of-the-art mapping and exploration methods depend heavily on dense range sensors (lidars - heavy, depth cameras - shortrange, stereo cameras - expensive, not available in thermal).
% However, state-of-the-art mapping and exploration methods depend on dense range sensors such as LiDARs or depth cameras.
However, the mapping and planning methods used in unknown environments currently rely on dense range sensors, such as LiDARs or depth cameras.
% , for building 3D maps for exploration and path planning.
Such sensors are heavy, expensive, and thus severely limit the affordability and flight time of autonomous UAVs.
% Monocular cameras and inertial measurement units (IMUs), on the other hand, are abundant in consumer electronics and vastly more commercially available.
% Enabling UAVs to autonomously map and navigate unknown environments using only such light-weight and inexpensive sensors could significantly lower the cost of many real world UAV applications.
% Monocular cameras and inertial measurement units (IMUs), on the other hand, are orders of magnitude lighter and less expensive.


% Monocular cameras and inertial measurement units (IMUs) are orders of magnitude lighter and less expensive, but autonomous navigation using only these sensors remains an open research problem.
Monocular cameras and inertial measurement units (IMUs) are orders of magnitude lighter and less expensive, but monocular-only exploration of unknown environments has only been demonstrated the scale of a few indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav, cnn_explo_singleroom}.
The main challenge can be attributed to the fact that the existing methods depend on \textit{reliable} and \textit{dense} depth estimates and obtaining such estimates from a monocular camera is not yet fully solved \cite{deep_mono_depth_2021, monodepth_survey_big_2024}.
Thus, when when classical mapping methods are used with any monocular existing depth estimation method, the resulting maps are either too sparse to allow planning in large-scale environments \cite{from_monoslam_to_explo, los_maps}, or they contain wrongly estimated free space, which leads to crashes \cite{simon2023mononav, mono_airsim_nav_2025}.

% INTRO IMG%%{
% \begin{figure}[!t]
%   % \includegraphics[width=8.5cm]{fig/smap2.png}
%   % \includegraphics[width=8.5cm]{fig/explor_forest2.png}
%   % \includegraphics[width=8.5cm]{fig/intro_poly.png}
%     % \adjincludegraphics[width=8.5cm, trim={{0.4\width} {0.05\height} {0.05\width} {0.1\height}}, clip=true]{fig/intro_poly.png}
%     \adjincludegraphics[width=8.5cm, trim={{0.0\width} {0.00\height} {0.00\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}
%   \centering
%   % \caption{Visualization of the proposed map representation and how it is updated. The green polyhedron is an estimate of currently visible free space and is formed by the visible triangulated keypoints and the camera's position (axes). The mapped spheres (green), obstacle points (red) and frontier points (purple) are updated by the polyhedron and visible keypoints, as described in \autoref{sec:map_building}}
%   \caption{Visualization of the proposed map representation as described in \autoref{sec:map_building}}
%   \label{fig:smap_intro}
% \end{figure}

\begin{figure}[!t]
  \centering
     \begin{subfigure}[b]{0.995\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.1\height} {0.0\width} {0.1\height}}, clip=true]{fig/sar_mono_dji_2.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_971_poly2_B}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_912_cam.png}};
% Selection_971_poly2_B - DELAUNAY+VIR
% Selection_912_cam.png - VIO
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\linewidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/mono_sar_cover.png}};
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    % Illustration of the proposed method in a real-world outdoor environment with large open areas. 
    Illustration of the proposed approach. 
    % The mapping pipeline uses information from sparse monocular-inertial SLAM (left) running onboard the UAV (top) to construct a large-scale map consisting of free-space spheres and obstacle points (right) for safety-aware exploration.
    The mapping pipeline estimates depth using virtual (blue) and tracked (red) points from sparse monocular-inertial SLAM (left) running onboard the UAV (top) to construct a large-scale map consisting of free-space spheres and obstacle points (right) 
    % for safety-aware exploration.
  }
  % \label{fig:mapping_pipeline}
  \label{fig:smap_intro}
\end{figure}
% %%}


% The core idea of our paper is that if the properties of sparse monocular SLAM are explicitly taken into account in designing both the mapping and the planning parts of an exploration system, its performance can be superior to methods that apply dense mapping approaches to the sparse depth frontend.
In this paper, we identify three core properties of motion-based depth sensing and show that explicitly designing the mapping and planning based on these principles can enable autonomy both indoor and outdoor, and at scales comparable to dense sensing methods.
% In essence, our mapping approach (\autoref{fig:smap_intro}) focuses on building a reliable map of both obstacles and free space by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty.
% In essence, our mapping approach interpolates depth and intelligently estimates free space in texture-sparse areas, solving the problem of sparse free space
% , but potentially overestimates free space.
% In classical mapping, this would allow wrong deletion of obstacles, but our approach handles this by keeping track of obstacle position uncertainties.
% This can naturally lead to overestimation of free space, but we show that this can be handled in planning through rapid replanning and forced forward flight.
% , and it keeps track of obstacle position uncertainties to avoid deleting 
In essence, our mapping approach interpolates depth and intelligently estimates free space in texture-sparse areas, solving the problem of sparse free space, while keeping track of obstacle position uncertainties to avoid wrongly deleting mapped obstacles.
% Our planning approach then 
Our planning approach handles the uncertain depth estimates by flying in the direction of the camera, rapidly replanning when new information is obtained and ensuring pure translational motion when visiting exploration viewpoints.
% We also show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless areas are taken into account.

We evaluate our approach in multiple real-world scenarios, simulations and ablation studies and show that our approach 
achieves exploration in both indoor and outdoor environments, and at considerably larger scales than existing monocular exploration methods.
% achieves exploration of large-scale unstructured indoor and outdoor areas.
% achieves exploration at a scale comparable to methods using dense range sensors.
% which feature both texture-sparse wide open spaces and cluttered passages.
% As a step towards low-cost vision-based UAVs safely and efficiently navigating any large-scale, unknown 3D environments, our paper brings the following contributions:
As a step towards low-cost vision-based UAV autonomy in unknown environments, our paper brings the following contributions:

% Our paper brings the following contributions:
% To enable autonomous navigation in unstructured 3D environments of large scales for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
% To enable autonomous navigation in general environments for low-cost UAV platforms equipped with only monocular-inertial sensors, our paper brings the following contributions:
\begin{itemize}
    % \item A method of building an occupancy map consisting of spheres and points using only sparse visual points and odometry as inputs
  % \item A novel 3D mapping approach that constructs a sphere-based map directly from sparse pointclouds obtained by monocular-inertial SLAM, without needing any intermediate occupancy map, mapping both open-space and cluttered areas.
  % \item A perception-aware mapping method that constructs a sphere-based map for safety-aware navigation using only sparse pointclouds and pose estimates from monocular-inertial SLAM as input,
  %   with the ability to map sufficient free space for flight even in texture-sparse open-space areas.
  % \item A monocular 3D mapping method that builds a reliable map of obstacles and navigable space thanks to tight coupling with sparse monocular-inertial SLAM, 
  \item A monocular 3D mapping method with perception-coupled modules that significantly increase the reliability of mapped free space and obstacles with unreliable, sparse, motion-based depth data as input 
  \item A perception-aware exploration approach, enabled by the proposed mapping method, that achieves large-scale 3D outdoor exploration on a UAV equipped with only a monocular camera and IMU, 
      validated in real-world conditions.
    \item Open-sourced code for the proposed methods, along with example simulation scripts for replicating our results, providing a benchmark for new monocular exploration methods.
    % \item First monocular outdoor exploration, first method that builds a sphere-based map directly from depth data.
\end{itemize}

\section{Problem Statement}
% The problem addressed in this paper is actively building an explicit 3D map of free and occupied space in an unknown environment with motion-based monocular depth sensing facilitated by a monocular-inertial SLAM frontend.
The problem addressed in this paper is actively building an explicit 3D map of free and occupied space in a bounded unknown environment.
In the case of robots equipped with dense depth sensors (most commonly LiDAR, RGBD, stereo cameras), this is a well-studied research problem \cite{darpa_cerberus_wins, beneath, paper_frontier_grandpa}.
% Autonomous navigation of unknown environments usually separated into 2 main parts: mapping and planning.
% Exploration, and navigation in general, are usually separated into 2 main parts: mapping and planning.
% The vast majority of existing works on mapping and planning rely on dense range sensors, such as LiDARs or depth cameras, to build a 3D occupancy map of the environment \cite{darpa_cerberus_wins, beneath, csiro_darpa, octomap, voxblox, ufomap}.
% These works implicitly assume that the environment is composed of occupied and free space, and that all occupied obstacles within the sensor's field-of-view and range will be detected.
% These works implicitly assume that at any given pose, the sensor (usually LiDAR or depth camera) will correctly classify all occupied and free space within its field-of-view and visible range. 
% These works implicitly assume that the distance measurements from the sensor are dense and accurate enough to build a reliable occupancy map by tracing rays from the sensor origin towards the measured points and classifying the traversed space as occupied or free \cite{occupancy_moravec, octomap, voxblox, ufomap}.
These works implicitly assume that from any given viewpoint, the sensor will uncover all obstacles up to its sensing range, and that non-detection of obstacles means that free-space can be initialized in the given direction  \cite{occupancy_moravec, octomap, voxblox, ufomap}.

However, this assumption does not apply for monocular depth sensing.
Motion-based depth sensing, as used in monocular SLAM algorithms, can only estimate depth for textured areas and requires sufficient parallax to do so.
Single-image learning-based monocular depth estimation methods \cite{deep_mono_depth_2021, monodepth_survey_big_2024} can provide dense depth estimates, but they are not yet reliable enough for safe navigation in general unknown environments \cite{simon2023mononav, mono_airsim_nav_2025}.
% However, this assumption is not applicable when using a monocular camera, as there is currently no method that can reliably provide dense and accurate depth measurements in all real-world environments.
% However, monocular cameras do not provide direct range measurements and there is currently no method that can reliably provide dense and accurate depth measurements in all real-world environments.
% Without this assumption, the problem of mapping and planning becomes significantly more challenging -- undetected obstacles can lead to collisions, while undetected free space can lead to overly conservative plans that prevent exploration.
% Classical frontier-based exploration \cite{paper_frontier_grandpa} is also difficult to apply since some objects might have textureless areas, leading to frontiers that cannot be uncovered by motion-based depth estimation, as discussed in \cite{from_monoslam_to_explo}.
% Thus, it is not possible to directly apply the same mapping and planning approaches as with dense range sensors and the existing exploration approaches either fail to explore large-scale environments or make limiting assumptions on the environment structure.
% Thus, it is not possible to directly apply the same mapping and planning approaches as with dense range sensors.
% Thus, it would be good to design a 
Thankfully, motion-based depth sensing does have some regularities that can be exploited to design mapping and planning that could handle the uncertain and sparse depth estimates.
We base our solution to this problem on the following key assumptions/observations about monocular motion-based depth sensing applicable to most of the monocular SLAM algorithms:
\begin{enumerate}
  \item {Depth accuracy generally increases with decreasing distance to obstacles}, as the \textit{inverse} of depth errors are gaussian \cite{inverse_depth} and since objects occupy more pixels in the image, making them more likely to be detected.
  \item In outdoor environments, {the UAV can encounter large textureless areas where no depth can be estimated by classic motion-based methods} (e.g. above horizon on a clear day) that initialize depth by tracking texture.
  \item {Measuring depth requires sufficient translational motion}, making the depth measurements \textit{trajectory dependent}, which is not the case with dense range sensors.
\end{enumerate}

\section{Related Works}
\label{sec:related_works}
Existing approaches to solving the monocular exploration problem can be broadly divided into three main categories: 
methods that use monocular SLAM for sparse depth estimation,
methods using monocular depth neural networks to turn the monocular camera into a dense depth sensor,
% or implicit methods that do not build an explicit 3D occupancy map.
or methods that simplify the problem through limiting assumptions on the environment structure.

The authors of \cite{simon2023mononav, cnn_explo_singleroom} have demonstrated using such depth estimation models combined with traditional navigation approaches.
However, the authors of \cite{simon2023mononav} demonstrate autonomy only at the scale of a single room, while also encountering collisions and map deformations due to depth estimation errors.
The authors of \cite{mono_airsim_nav_2025} present navigation with obstacle avoidance using pretrained and finetuned monocular depth models in larger outdoor environments in simulation, but also note a high collision and navigation failure rate due to the same problems.
As further discussed in recent surveys \cite{deep_mono_depth_2021, monodepth_survey_big_2024}, current depth estimation models still struggle to provide real-time onboard performance and reliable generalization in domains that the model was not trained on.
In addition, they require a GPU which significantly raises the cost and weight of a UAV.

A different line of research focuses on taking sparse 3D points estimated by visual SLAM and working with those as the only depth measurements.
Most of this research has been on reactive obstacle avoidance \cite{flame, avoidance_mono1} or exploration with strong assumptions on the environment structure (e.g. corridors only \cite{corridors_mono_nav_2009} or considering a single object with no other obstacles \cite{information_driven_bbx_mapping}).
For general monocular exploration in unstructured environments, the authors of \cite{from_monoslam_to_explo, los_maps} propose building grid-based maps by tracing rays of free space towards the monocular SLAM points. 
As documented in \cite{from_monoslam_to_explo}, this approach causes gaps to appear in the free space of the map when the environment is not texture-rich.
The authors of \cite{from_monoslam_to_explo} further present an exploration method specifically to cover these gaps and demonstrate an increase in mapped volume.
However, the authors show this exploration approach to only work at the scale of a single room.
% However, this only reduces the gaps, as this mapping method can never estimate free space near texture-sparse areas, and the authors show this exploration approach to only work at the scale of a single room.
Most importantly though, this mapping approach fundamentally cannot estimate sufficient free space for navigation in texture-sparse areas, especially outdoor (as shown in \autoref{sec:sim_exper}).
The authors of \cite{los_maps} also trace rays towards sparse SLAM points, and they present exploration on the scale of multiple indoor rooms in simulation.
They work around the gap issue by setting the map voxel size to be relatively large compared to the UAV size.
A larger voxel size causes a higher percentage of voxels to be passed by the sparse rays towards measured points, reducing the gaps.

% In summary, the existing approaches to monocular exploration either rely on depth neural networks that are not yet reliable enough for safe deployment, or they use sparse monocular SLAM depth in a way that leads to large gaps in free space, prohibiting large-scale exploration.

In this paper, we present an approach to monocular exploration that works by explicitly taking into account the properties of sparse monocular SLAM in both mapping and planning.
Our system takes advantage of the potential of mapping-planning coupling, while at the same time producing a semi-dense 3D map of free and occupied space.
Thanks to the coupling, the approach is able to explore large-scale unstructured environments, including outdoor areas with large open spaces, which has not yet been demonstrated in literature.

% PIPELINE IMG%%{
\begin{figure*}[!ht]
  % \includegraphics[width=1.0\textwidth]{./fig/monospheres_big_diagram_v3.pdf}
     \begin{subfigure}[b]{0.99\linewidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.1\width} {0.00\height} {0.05\width} {0.05\height}}, clip=true]{fig/monospheres_big_diagram_math3.pdf}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.1\width} {0.00\height} {0.05\width} {0.05\height}}, clip=true]{fig/monospheres_big_diagram_bigfinal.pdf}};
      \end{tikzpicture}
     \end{subfigure}
  % \def\subfigwidth{0.24\linewidth}
  % \centering
  %    \begin{subfigure}[b]{\subfigwidth}
  %     \begin{tikzpicture}
  %       \centering
  %       \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_968_poly1_A.png}};
  %       % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_970_poly2_A.png}};
  %     \end{tikzpicture}
  %    \end{subfigure}
  %    \hfill
  %    \begin{subfigure}[b]{\subfigwidth}
  %     \begin{tikzpicture}
  %       \centering
  %       \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_969_poly1_B.png}};
  %       % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_971_poly2_B.png}};
  %     \end{tikzpicture}
  %    \end{subfigure}
  %    \hfill
  %    \begin{subfigure}[b]{\subfigwidth}
  %     \begin{tikzpicture}
  %       \centering
  %       \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_914_right_poly.png}};
  %     \end{tikzpicture}
  %    \end{subfigure}
  %    \hfill
  %    \begin{subfigure}[b]{\subfigwidth}
  %     \begin{tikzpicture}
  %       \centering
  %       \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/pipeline/Selection_927_right_free2.png}};
  %     \end{tikzpicture}
  %    \end{subfigure}
  %    \hfill
  \caption{
    Mapping pipeline overview. The OVDE and DBOF modules are illustrated in \autoref{fig:ofs} and \autoref{fig:distbased} respectively.
    % Illustration of the mapping pipeline. 
    % (a) The stable 3D points (red) used by the monocular SLAM are the only input depth measurements. 
    % Delaunay triangulation is performed on the input points projected to 2D as a rough depth interpolation. 
    % % Additionally, , the TODO-fake points are added to the depth estimation.
    % (b) The active assumed-freespace points (blue, see \autoref{sec:fake}) above the ground, where the SLAM cannot accurately localize any points, are added to construct the
    % (c) resulting visible free-space polyhedron $\mathcal{P}_f$.
    % % (d) Side view of how new spheres are sampled in the polyhedron and added to the others, as well as how new obstacle points are added to the map.
    % (d) New spheres are sampled in the polyhedron and added to the graph, and the visible SLAM points are added to the set of obstacle points $\mathcal{X}$.
    % TODO - desc + delaunay w fake fspace vis
  }
  \label{fig:mapping_pipeline}
\end{figure*}
% %%}

\section{Sphere-Based Mapping Using Sparse Visual SLAM Points}
\label{sec:map_building}
The map representation used in this paper represents free space by a graph of intersecting spheres and obstacles as a set of 3D points.
The graph-of-spheres representation was chosen for its scalable free-space representation and rapid path-planning capabilities \cite{spheregraph, spheremap, bubbleplanner}. 
However, most of the perception-aware mapping concepts introduced in this paper could be adapted to a voxel-based mapping approach as well.
% \todo{kdtree of centers}
% and obstacles as a set of 3D points corresponding to triangulated visual keypoints from visual SLAM.

Each sphere at the $k$-th update iteration has a static center $\mathbf{c}$ and changing radius $r_k$.
The radius $r_k$ represents the distance from $\mathbf{c}$ to the nearest unknown space or obstacle at time step $k$.
% We maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres, and use it for path planning.
To allow rapid path planning, we maintain a graph $\mathbf{G}$, which contains an edge for each pair of intersecting spheres.
% Furthermore, we maintain a set of obstacle points $\mathbf{X}$, which correspond to the textured surfaces measured by the visual SLAM.
% We represent obstacles by a set of points $\mathcal{X}$, which are .
\todo{To represent obstacles, we accumulate the stable 3D points (i.e. with low position covariance) estimated by the monocular SLAM into a set of points $\mathcal{X}$ with user-defined minimal point-to-point distance.}
The obstacle points are updated, merged and removed based on criteria designed specifically for monocular sensing, described in \autoref{sec:distance-based}.
% The mapping runs onboard the UAV in real-time, using only the estimated pose and 3D points from monocular-inertial SLAM as its inputs.
% In this paper, we use the sparse point-based OpenVINS \cite{openvins} method as the SLAM frontend, but our method could work with other monocular-inertial SLAMs.
A single map update iteration consists of the following steps, visualized also in \autoref{fig:mapping_pipeline}.
% \subsection{Constructing the Visible Free-Space Polyhedron}
\subsection{Depth Interpolation and Polyhedron Construction}
\label{sec:polyhedron}
\todo{The first step is} to construct a polyhedron of space that is likely to contain free space based on the information in the current timestep.
We interpolate depth between the sparse SLAM points using a simplified version of the approach described in FLAME \cite{flame}, where
a precise mesh is constructed from visual keypoint measurements.
In this paper, we care primarily about mapping the free space for navigation purposes, and thus we do not perform the mesh optimization or split the 2D interpolations as in \cite{flame}.

We project the currently tracked visual SLAM points into the image plane and compute their Delaunay triangulation. 
By connecting the points in 3D according to their Delaunay triangulation in 2D, we obtain a mesh $\mathcal{F}_d$ that we call a \textit{depth mesh}, which is used only for the current frame.
We estimate the currently visible free space as the volume between the camera's position $\mathbf{p}$ and all points on $\mathcal{F}_d$.
This space is enclosed by connecting all points that lie on the edge of $\mathcal{F}_d$ to the camera's focal point, which forms an \textit{estimated visible free space polyhedron} $\mathcal{P}_f$.
It is only used for the update step at the current time and discarded afterwards.

% \subsection{Estimating Free Space in Open Areas}

% OFS figure, crop top and down
\begin{figure}[!h]
  % \includegraphics[width=0.5\textwidth]{fig/monospheres_ofs_v2.pdf}
     \begin{subfigure}[b]{1\linewidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.05\height} {0.0\width} {0.15\height}}, clip=true]{fig/mononspheres_ofs_diagram_wide2.pdf}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.05\height} {0.0\width} {0.15\height}}, clip=true]{fig/mononspheres_ofs_diagram_narrow5.pdf}};
      \end{tikzpicture}
     \end{subfigure}
  \centering
  \caption{Open-area virtual-depth estimation diagram. The thick green points $\mathbf{x}_{vir, k}$ satisfy the condition defined in \autoref{sec:fake} and are added to the construction of $\mathcal{F}_d$}
  \label{fig:ofs}
\end{figure}

% \subsection{Open-Area Free-Space Sampling (OFS)}
\subsection{Open-Area Virtual-Depth Estimation (OVDE)}
\label{sec:fake}
When moving in open areas, such as a grassy field without vertical obstacles in \autoref{fig:mapping_pipeline}, point-based visual SLAM can reliably localize only nearby points corresponding to the ground. 
Using the ray-tracing grid-based mapping method \cite{from_monoslam_to_explo, los_maps}, this would cause free space to be estimated only in a downward direction and not allow flight across the field.
To allow flight in open areas, our method estimates additional free space based on the following assumption: 
Consider a virtual point in space $\mathbf{x}_{vir}$ that falls into the camera's FoV at the current pose $\mathbf{p}_k$ and also at previous poses $\mathbf{p}_{k-n}, ...,\mathbf{p}_{k-2}, \mathbf{p}_{k-1}$.
If there is sufficient parallax between any two of these poses for estimating the distance of $\mathbf{x}_{vir}$ (given by the max covariance of the measured SLAM points), and if the visual SLAM frontend has not detected any obstacle point approximately in the direction of  $\mathbf{x}_{vir}$, then $\mathbf{x}_{vir}$ must lie in free space (see \autoref{fig:ofs}).

The mapping method periodically checks 
the described condition for a set of virtual points $\mathbf{x}_{vir}$ at fixed positions relative to the UAV (see \autoref{fig:mapping_pipeline}) at each mapping iteration and for a fixed number of previous poses. 
The points that fulfill this condition are used to recompute the \textit{depth mesh} $\mathcal{F}_d$ as if they were measured by the SLAM frontend.
However, we discard points that would fall into the Delaunay triangulation of the visible obstacle points, since there is a more reliable depth estimate from interpolating the SLAM points.
Free space estimated in this manner also cannot erase existing obstacle points in the map.

This approach allows flexibly adding more free space in open areas while using the SLAM points for estimating free space near obstacles.
Of course, the assumption does not hold close to obstacles that are not detectable by the sparse visual SLAM frontend (e.g. textureless objects or thin wires).
However, this can be resolved by using a dense SLAM frontend (at a higher computational cost), or by adding low-cost ultrasonic sensors to detect close thin obstacles.

\begin{figure}[!h]
  % \includegraphics[width=8.5cm]{fig/smap2.png}
  \includegraphics[width=8.5cm]{fig/distbased9.pdf}
  \centering
  \caption{Top-down illustration of sphere sampling and obstacle point management: 
  The visible free-space polyhedron $\mathcal{P}_f^{t_2}$ is created using the currently tracked SLAM points (black crosses) and the camera's position $\mathbf{p}_{cam}^{t_2}$ at time $t_2$.
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  The size of a point $\mathbf{x}$ corresponds to the distance $d_{\mathbf{x}}$ that it was measured from.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black), as there are better localized (smaller) points near them.
  The (dashed red) point seen far away from the camera at a previous time $t_1$ now lies in $\mathcal{P}_f^{t_2}$ close to the camera and will thus be deleted as noise.
  % The (dashed red) point will be deleted, as it lies in the free-space polyhedron at $t_2$, and it .
  The (dashed black) two points observed far from the camera at $t_2$ are not added to the map, as there are more precisely localized (red) points close to them.
  % New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  New spheres are inscribed inside $\mathcal{P}_f^{t_2}$, but their radii are also bound by the protected points (non-dashed red) $\mathcal{X}_p$, which were added to the map at $t_1$.
  % New spheres are inscribed inside the visible free-space polyhedron $\mathbf{P}_f$.
  % Visible free-space polygon $\mathbf{P}_f$ (-);
  % Newly sampled spheres (O);
  % Triangulated keypoints visible at current frame (X); 
  % Points already in the map (X) added from a previous pose (red). 
  % Smaller points and higher-opacity spheres have been observed from lower distances.
  % Dashed crosses correspond to points that will be deleted (red) or not added to the map (black).
  }
  \label{fig:distbased}
\end{figure}


\subsection{Distance-Based Obstacle Filtering (DBOF)}
\label{sec:distance-based}
As the next step of the update, we decide which tracked SLAM points to add into the map points $\mathcal{X}$, and which points already in the map to delete.
An important piece of information is the position covariance of each SLAM point.
Since in inverse depth parametrization, the position covariance of a point grows with the distance from which it was measured \cite{inverse_depth}, we approximate the covariance simply as the lowest distance $d_{m, x}$ that a point has been observed from.
This information is crucial for solving problems that can arise when viewing some surface from a much higher distance than before (e.g. observing a wall from 5 m, and then later from 20m).

As illustrated in \autoref{fig:distbased}, points corresonding to a small obstacle might momentarily not be detected by the SLAM frotend, while a larger obstacle behind the small one is detected.
Thus, we cannot simply delete every map point that falls into $\mathcal{P}_f$, because then the original points would be wrongly deleted and assumed as free space.
We solve this problem by deleting any map point $\mathbf{x}$ that falls into $\mathcal{P}_f$ only if
\begin{equation}
  \mathbf{x} \in \mathcal{P}_f \quad \text{and}  \quad |\mathbf{x} - \mathbf{p}_{cam}| < \alpha \cdot d_{m,x},
\end{equation}
where $\mathbf{p}_{cam}$ is the position of the camera and $\alpha$ is a paremeter, which we empirically set to $1.1$ to account for distance measurement noise.
The points that lie in the free-space polyhedron $\mathcal{P}_f$ and are not deleted are called \textit{protected points} $\mathcal{X}_p$.
These points are used to constrain sphere radii in the following update step.

Additionally, to prioritize closer measurements, points seen from a closer distance replace points seen from larger distances, if they are measured close enough to them. 
In the same way, new points seen at a large distance are not added to the map, if there are more accurately localized points near them, which is also visualized in \autoref{fig:distbased}. 
We enforce this rule by deleting any newly added or previously mapped point $\mathbf{x}$ if
\begin{equation}
  \exists \mathbf{x}_{prev}: |\mathbf{x} - \mathbf{x}_{prev}| < a \quad \text{and} \quad d_{m,x_{prev}}  < d_{m,x}
\end{equation}
where $\mathbf{x}_{prev}$ is another mapped or newly added point with a higher position accuracy and $a$ is the minimal point-to-point distance set by the user.
% \todo{TODO - rewrite as sort?}

\subsection{Updating and Adding New Spheres}
\label{sec:existing_spheres_update}
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the protected points $\mathbf{X}_{p}$.
% Next, we recompute the radii of spheres that could be updated by $\mathbf{P}_f$ or the input points $\mathbf{X}_{in}$.
Next, we recompute the radii of existing spheres that could be updated by $\mathcal{P}_f$ or the input points $\mathcal{X}_{in}$.
To bound the update time of this step, we check the largest sphere radius $r_{max}$ in the map, which allows us to quickly filter out all spheres whose centers fall outside a bounding box around $\mathcal{P}_f$, inflated by $r_{max}$. 
Then, for any remaining sphere with a center $\mathbf{c}$ and radius $r_{k}$, the updated radius is computed as
\begin{equation}
  r_{k+1} = \min \left( \max \left( r_{k}, d(\mathbf{c}, \mathcal{P}_f) \right)
  , d(\mathbf{c}, \mathcal{X}_{in} \cup \mathcal{X}_p) \right).
  % r_{k+1} = \min \left( d(\mathbf{c}, \mathbf{X}_{in} \cup \mathbf{X}_p)
  % , \max \left( r_{k}, d(\mathbf{c}, \mathbf{P}_f) \right) \right).
  \label{eq:update}
\end{equation}
% where $d(x, \mathbf{P}_f)$ is the signed distance to $\mathbf{P}_f$ (positive if the point is inside the polyhedron, negative if outside) \todo{and} $d(\mathbf{x},\mathbf{X}_{in} \cup \mathbf{X}_p)$ is the minimum distance to all input obstacle points $\mathbf{X}_{in}$, and to the protected map points $\mathbf{X}_p$ described in \autoref{sec:distance-based}.
In this equation, $d(\mathbf{c}, \mathcal{P}_f)$ is the signed distance to $\mathcal{P}_f$ (positive if the point is inside the polyhedron, negative if outside).
The value of $d(\mathbf{c},\mathcal{X}_{in} \cup \mathcal{X}_p)$ is the minimum distance to all input obstacle points $\mathcal{X}_{in}$, and to the protected map points $\mathcal{X}_p$ described in \autoref{sec:distance-based}.
Essentially, this equation means that the polyhedron $\mathcal{P}_f$ is used to increase the radius of a sphere, whereas the protected $\mathcal{X}_p$ and input points $\mathcal{X}_{in}$ are used to constrain it.
As the last part of this step, we delete all spheres with $r_{k+1} < r_{min}$, where $r_{min}$ is the smallest allowed sphere radius specified by the user.
% , usually slightly lower than the UAV's minimal allowed distance to obstacles. 

% where TODO.
To introduce new spheres into the map, we sample a fixed number of points inside $\mathcal{P}_f$ at random distances between the camera and the depth mesh $F_o$.
% \todo{This is a simplistic approach, and could be improved for faster flight, for example by sampling along the predicted trajectory of the UAV at high speeds.}
A potential new sphere's radius is determined in the same way as for the old spheres in the previous step in Eq. \ref{eq:update} with $r_k = 0$.
If the potential radius is larger than $r_{min}$, the sphere is added to the map.

\subsection{Recomputing and Sparsifying Sphere Graph}
\label{sec:graph_update}
After all the sphere radii updates have been made, we update the graph of spheres used for path planning, so that all intersecting spheres are connected in the graph.
% TODO - EXPLAIN???
Furthermore, to constrain map update time and path planning time, we perform a redundancy check on the updated and added spheres in the same way as in \cite{spheremap}.
If any sphere is found to be redundant, it is deleted from the map.
% \todo{this approach is important to cover large...}
This step is important to cover large open areas by only a few spheres to allow rapid planning, and tight corridors by a higher density of spheres, capturing the information about potential paths and distances in more detail.
% This way, large open areas are covered by only a few spheres and can be planned over quickly, and tight corridors have a higher density of spheres, capturing the information about potential paths and distances to obstacles in more detail.

% \section{Perception-Aware Exploration using the Sphere-Based Map}
\section{Perception-Aware Exploration with a Sparse Sphere-Based Map}
% In this section, we detail how the proposed mapping method is highly suitable for autonomous path planning and exploration, and how volumetric exploration can be achieved, even with only a monocular camera and IMU.
In this section, we describe a monocular exploration approach enabled by the proposed mapping method.
In principle, we employ the commonly used next-best-view (NBV) \cite{paper_frontier_grandpa} strategy, which has so far been deemed unsuitable for monocular exploration in previous works \cite{from_monoslam_to_explo, los_maps}.
% We introduce several major differences in methodology that are neccessary in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments, detailed in the following sections.
% In the following sections, we introduce several major differences in methodology that are neccessary for NBV in the case of a robot with a monocular camera for depth sensing in large-scale, real-world environments.
In the following sections, we introduce several major differences in methodology that are critical for allowing the NBV strategy to be used for robots equipped with only a monocular camera for depth sensing.
% \todo{write assumptions on perception here? mapping doesnt care}

\subsection{Frontier Sampling on Free-Space Polyhedron}
Firstly, since the proposed map representation is fundamentally different from an occupancy grid, we need to detect frontiers (i.e. the boundary of free and unknown space) in a different way than with an occupancy grid.
We propose to sample points along the visible free-space polyhedron $ \mathcal{P}_f $  described in \autoref{sec:map_building} at each map update, and add the points as frontiers, if they do not lie inside any free-space sphere and if they are at a \todo{user-defined distance} from all map obstacle points and other frontiers.
If they do not meet these criteria in any following update, they are deleted. 
To avoid the loss of SLAM feature tracking we also delete frontiers that are too far from any obstacle points.
% \todo{complexity and vs octomap?}
% In enclosed spaces, which are usually considered in autonomous exploration literature, all frontiers are usually worth exploring.
% However, in outdoor exploration with areas of open space, it is also important to assign different value to different frontiers, especially for UAVs. 
% We propose to constrain the exploration to only consider frontiers that lie near some obstacle points.
% This way, the UAV does not explore up into the sky.
% Additionally, this forces the UAV to only explore near texture-rich areas, thus avoiding losing visual SLAM tracking.
% \todo{Lastly, we add new exploration viewpoints into the map only if a sufficient number of such frontier points would be visible from a given viewpoint at a close distance.}
These frontier points are used to generate exploration viewpoints such that at least some frontier points are visible from each viewpoint.

% EXPLORATION EXPLANATION IMG%%{
\begin{figure}[!htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
  % --- Subfigure (a)
  \begin{subfigure}[b]{\subfigwidth}
    \begin{tikzpicture}
      \node[anchor=south west,inner sep=0] (a) at (0,0)
        {\adjincludegraphics[width=1.0\linewidth,
          trim={{0.08\width} {0.1\height} {0.08\width} {0.1\height}},
          % clip=true]{fig/monospheres_exploration_v2_part1.pdf}};
          % clip=true]{fig/monospheres_exploration_cyan_part1.pdf}};
          clip=true]{fig/monospheres_exploration_purple_red.pdf}};
      % label (a) in lower-left corner
      \node[anchor=south west, font=\bfseries, text=black, xshift=2pt, yshift=2pt] at (a.south west) {(a)};
    \end{tikzpicture}
  \end{subfigure}
  \hfill
  % --- Subfigure (b)
  \begin{subfigure}[b]{\subfigwidth}
    \begin{tikzpicture}
      \node[anchor=south west,inner sep=0] (b) at (0,0)
        {\adjincludegraphics[width=1.0\linewidth,
          trim={{0.08\width} {0.1\height} {0.08\width} {0.1\height}},
          % clip=true]{fig/monospheres_exploration_v2_part2.pdf}};
          % clip=true]{fig/monospheres_exploration_cyan_part2.pdf}};
          clip=true]{fig/monospheres_exploration_part2_purple_red.pdf}};
      % label (b) in lower-left corner
      \node[anchor=south west, font=\bfseries, text=black, xshift=2pt, yshift=2pt] at (b.south west) {(b)};
    \end{tikzpicture}
  \end{subfigure}
     \hfill
  \caption{
    Top-down illustration of the exploration approach.
    In (a), the UAV navigates to one of the sampled viewpoints (yellow) aimed at a feature-sparse wall.
    This does not uncover any frontiers (magenta), so to stop returning to this or nearby viewpoints, the viewpoint is blocked (black).
    In both (a) and (b), the UAV flies facing forward and aligns itself with the viewpoint's direction when nearing it (green poses).
    }
  \label{fig:exploration_explanation}
\end{figure}
% %%}


\subsection{Forced Translation when reaching Viewpoints}
\label{sec:align}
In traditional exploration approaches with dense distance sensors, it is sufficient to move the robot to a frontier, aim the sensors towards the unknown space, and assume that the distance sensors will uncover some additional space behind the frontier and thus expand the map.
This approach will not, in principle, work reliably with a robot using a monocular camera for estimating depth from motion.
Such a robot requires \textit{translational} motion to obtain correct distance estimates of visual keypoints. 
% as in monocular structure-from-motion (SfM) TODO-CITE (we do not consider deep learning methods which exploit knowledge about sizes of objects).
% Excessive rotation without translation cannot lead to reasonable distance estimates.
% Too much rotation with too little translation of the camera will cause the monocular SLAM or any SfM-based approach to not give reliable depth estimates.
With rotation only, or motions that have too much rotation compared to translation, reliable motion-based depth estimates cannot be obtained.

To solve this, we propose a simple perception-aware planning approach: 
% When finding a path to a goal viewpoint that could uncover some frontiers, we compute the target headings on the path so that the UAV maintains a fixed heading in the viewpoint's direction for at least $d_{c}$ meters on the path before reaching the viewpoint, as visualized in \autoref{fig:exploration_explanation}.
When finding a path to an exploration  viewpoint, we force the trajectory to aim the UAV in the viewpoint's direction after getting to a pre-defined distance $d_c$ from the goal, as visualized in \autoref{fig:exploration_explanation}.
% In the experiments detailed in Sec. \autoref{sec:experiments}, we set $d_{c}=TODO$.
This way, the UAV reaches the viewpoint with at least $d_c$ meters of translational motion.
If textured surfaces are visible from that viewpoint, the UAV will most likely observe sufficient parallax to estimate their distances.

\subsection{Explored Viewpoint Blocking}
\label{sec:blocking}
% This is an important part of the method also due to the second major distinction -- we block the sampling of new exploration viewpoints near explored viewpoints.
Another necessary distinction we propose for NBV exploration with a monocular camera is to block sampling of new exploration viewpoints near visited viewpoints.
This is due to the fact that real-world environments often contain textureless areas (mainly in buildings or other man-made structures).
Since obstacle points correspond to textured points localized by the visual SLAM, no points will ever be added on textureless surfaces.
Frontier points will be generated there instead, since the surfaces lie on the edge of the visible free space polyhedron and can be far enough from any obstacle points.
% but they will be on the boundary of free-space, so frontiers will be generated there.
However, such frontiers cannot uncover any new space, since there is nothing behind them.
For this reason, we block the sampling of new viewpoints near visited viewpoints, visualized as large black arrows in \autoref{fig:exper_real}
This stops the UAV from getting stuck repeatedly trying to uncover such surfaces.

\subsection{Safety-Aware Planning}
Lastly,
the UAV is controlled to always fly in the direction of its camera when not aligning itself with a viewpoint as described in \autoref{sec:align}.
This approach is crucial for ensuring collision-free flight in the case of incorrectly initialized free space.
As discussed in \autoref{sec:discussion}, some obstacles (e.g. thin wires, twigs) might not be visible in the cameras from a large distance to be detected by the visual SLAM (especially when using fisheye cameras), and can be wrongly estimated as free space.
Using the forward-facing flight, the UAV has a higher chance to see these obstacles that were previously not detected, and quickly replan.
To further increase safety and allow agile maneuvers, the paths are planned with a strong preference for high distance from obstacles, for which the graph-of-spheres representation is highly suitable, as described in \cite{spheremap, bubbleplanner, spheregraph}.


% ORCHARD IMG%%{
\begin{figure}[!htb]
  \centering
  \begin{tikzpicture}
    % --- Subfigure (a)
    % \node[anchor=south west,inner sep=0] (suba) at (0,0)
    %   {\begin{subfigure}[b]{0.49\linewidth}
    %     \adjincludegraphics[width=\linewidth,
    %       trim={{0.3\width} {0.08\height} {0.15\width} {0.2\height}},
    %       clip=true]{fig/orchard_brightened.png}
    %   \end{subfigure}};

    % \node[anchor=south west,inner sep=0] (suba) at (0,0)
    %   {\begin{subfigure}[b]{0.49\linewidth}
    %     \adjincludegraphics[width=\linewidth,
    %       trim={{0.0\width} {0.07\height} {0.07\width} {0.0\height}},
    %       clip=true]{fig/orchard_brightened2.png}
    %   \end{subfigure}};

    \node[anchor=south west,inner sep=0] (suba) at (0,0)
      {\begin{subfigure}[b]{0.49\linewidth}
        \adjincludegraphics[width=\linewidth,
          trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}},
          clip=true]{fig/orchard_corrected_hot.png}
          % clip=true]{fig/orchard_corrected_cool.png}
      \end{subfigure}};
      
    % --- Subfigure (b)
    \node[anchor=south west,inner sep=0] (subb) at ([xshift=0.5\linewidth+0.02\linewidth]suba.south west)
      {\begin{subfigure}[b]{0.49\linewidth}
        \begin{tikzpicture}
          \node[anchor=south west,inner sep=0] (image) at (0,0)
            {\adjincludegraphics[width=1.0\linewidth,
              trim={{0.1\width} {0.06\height} {0.1\width} {0.13\height}},
              clip=true]{fig/2025-10-20_17-20_orchard_treesvisible.png}};
          \begin{scope}[x={(image.south east)}, y={(image.north west)}]
            \draw[blue, very thick] (0.45,0.35) circle (10pt);
            \draw[blue, very thick] (0.68,0.34) circle (10pt);
            \draw[blue, very thick] (0.7,0.55) circle (10pt);
            \draw[blue, very thick] (0.72,0.78) circle (10pt);
            \draw[blue, very thick] (0.47,0.57) circle (10pt);
            \draw[blue, very thick] (0.48,0.8) circle (10pt);
            \draw[blue, very thick] (0.43,0.12) circle (10pt);
            \draw[blue, very thick] (0.68,0.10) circle (10pt);
          \end{scope}
        \end{tikzpicture}
      \end{subfigure}};

    % --- Draw arrow between subfigures
    \draw[->, ultra thick, blue]
      ([yshift=-0.7cm, xshift=1.5cm]suba.west) -- ([yshift=1.6cm, xshift=-0.65cm]subb.south);

    \draw[->, ultra thick, blue]
      ([yshift=0.4cm, xshift=3cm]suba.west) -- ([yshift=2.7cm, xshift=-0.6cm]subb.south);

  \end{tikzpicture}
  
  \caption{The 4min orchard exploration experiment. The map is noisy due to low-light conditions, trees are highlighted in blue for clarity.}
  \label{fig:orchard}
\end{figure}
% %%}

\section{Experiments}
\label{sec:experiments}
In this section, we analyze the performance of the proposed method in large-scale real-world (Sec. \ref{sec:real_exper}) and simulated (Sec. \ref{sec:sim_exper}) environments.
The presented implementation of the proposed mapping and exploration methods is currently single-threaded and written in python.
With this implementation, the mapping runs on average at $\SI{4.5}{\hertz}$ on average on a standard CPU.
This is sufficient for exploration planning, but can be slow to detect obstacles at high speeds. For this reason, the UAV also checks the predicted trajectory for potential collisions with the input pointclouds at the SLAM's update rate and stops on collision detection.
% , but could be further optimized with a parallelized implementation or a submap-based approach.
% This has shown to be sufficient for safe flight up to $\SI{3}{\meter/\second}$ in the tested environments, but a more optimized implementation could allow even faster navigation.
% Combined with a collision-checking thread that checks the predicted trajectory for potential collisions with local data at a high rate, this is suitable for safe flight.
% Our implementation also checks the predicted trajectory for potential collisions with local data at a high rate, this is suitable for safe flight.
The UAV used in the experiments was equipped with a monocular global-shutter grayscale fisheye Bluefox camera, and ICM-42688 IMU.
The MRS UAV system \cite{mrs_uav_system} was used for low-level motion planning, control, and the grid-based path planning for the simulation comparisons.
For state estimation and as the source of sparse visual SLAM points, we used OpenVINS \cite{openvins} in the inverse-depth measurement mode.

\subsection{Real-World UAV Monocular-Inertial Exploration}
\label{sec:real_exper}
% \todo{One of the realized real-world experiments is illustrated in \autoref{fig:exper_real}.}
% In this experiment, the UAV explored up to $\SI{60}{\meter}$ forward around the side of an abandoned farmhouse fully autonomously in an $8\text{min}$ mission.
% In the experiment shown in \autoref{fig:exper_real}, 
In \autoref{fig:exper_real}, we present an \SI{8}{\minute} experiment where our approach is validated in real-world conditions.
% We bounded the area for generating exploration goals to a $\SI{70x10x8}{\meter}$ box around the side of an abandoned farmhouse, and the UAV fully autonomously explored nearly all the available space in this region.
We bounded the area for generating exploration goals to a 70x10x8 $\SI{}{\meter}$ box around the side of an abandoned farmhouse, and the UAV fully autonomously explored nearly all the available space in this region.
The UAV successfully avoided and mapped all the debris, bushes and the house walls in the area, and when battery levels were getting low, it autonomously returned to the starting position.
% Also note that the UAV mapped a considerable amount of space in the open field to the left of the house.
% This was made possible by our approach to safe estimation of free space in open-area areas, described in \autoref{sec:fake}.
% SHORTER
Also note that the UAV mapped a considerable amount of space in the open field to the left of the house thanks to our approach to safe estimation of free space in open areas, described in \autoref{sec:fake}.

% We have also tested the proposed approach in a variety of other, smaller real-world environments, including an orchard, which our method fully autonomously explored.
We have also successfully validated our approach in a smaller, more cluttered orchard environment with thin tree branches, shown in \autoref{fig:orchard}.
Videos from these real-world experiments are available in the multimedia materials. 
% \todo{zkratit, neprisli jsme nato u testovani ale vime o tom a jak to resit (future work - lajny / shortrange sensory a pouzivat tohle na longrange)}
% \todo{Some of these experiments required the safety pilot to take control due to an important limitation of the method that we encountered during testing --- 
% since the visual SLAM tracks and triangulates visual key\textit{points}, visual \textit{lines} (caused by e.g. thin tree branches or smooth manmade poles) are not sensed in our mapping pipeline.
% This caused the UAV to nearly crash into tree crowns multiple times.
% This limitation could be resolved by integrating short-range depth sensors as well, even ultrasound sensors could potentially be fused into the map.
% Additionally, the visual SLAM could be modified to estimate 3D poses of lines as well, as for example in TODO-CITE, but we leave this for future work.
% }

% REAL EXPERIMENT
% %%{
\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t126.png}};
      % \draw [latex-latex](3.2,0.9) -- (3.2,6.8);
      % \node[align=center] at (3.6, 4) {\scriptsize \color{black}\SI{70}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=126\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     % \begin{subfigure}[b]{\subfigwidth}
     %  \begin{tikzpicture}
     %    \centering
     %    \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t206.png}};
     %    \begin{scope}[x={(a.south east)},y={(a.north west)}]
     %      \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=206\text{s}$};
     %    \end{scope}
     %  \end{tikzpicture}
     % \end{subfigure}
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t370.png}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=370\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/real_exper_t516.png}};
      \draw [latex-latex](4.05,0.4) -- (4.05,7.3);
      \node[align=center] at (3.6, 3.4) {\scriptsize \color{black}\SI{80}{\meter}};
        \begin{scope}[x={(a.south east)},y={(a.north west)}]
          \node[align=center] at (0.2, 0.925) {\footnotesize \color{black}$t=516\text{s}$};
        \end{scope}
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=90, origin=c, width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/house.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[angle=180, origin=c, width=1.0\linewidth, trim={{0.15\width} {0.3\height} {0.2\width} {0.0\height}}, clip=true]{fig/house.png}};
        % \begin{scope}[x={(a.south east)},y={(a.north west)}]
        %   \node[align=center] at (0.9, 0.075) {\footnotesize \color{black}$t=516\text{s}$};
        % \end{scope}
      \end{tikzpicture}
     \end{subfigure}
  \caption{
    Visualization of the real-world autonomous exploration experiment described in \autoref{sec:real_exper}, along with a satellite image of the explored abandoned farm area. Free space (green) is shown alongside mapped obstacle points (red), frontiers (purple) and explored viewpoints (black). 
  }
  \label{fig:exper_real}
\end{figure}
% %%}

% Define a reusable "white X with black border" marker
\newcommand{\markX}[3][0.01]{%
  % White background "stroke" for contrast
  \draw[line width=2.8pt, black]
    (#2-#1, #3-#1) -- (#2+#1, #3+#1);
  \draw[line width=2.8pt, black]
    (#2-#1, #3+#1) -- (#2+#1, #3-#1);
  % Black outline for visibility
  \pgfmathsetmacro{\innerscale}{0.85}
  \pgfmathsetmacro{\s}{#1*\innerscale}
    \draw[line width=1.2pt, white]
      (#2-\s, #3-\s) -- (#2+\s, #3+\s);
    \draw[line width=1.2pt, white]
      (#2-\s, #3+\s) -- (#2+\s, #3-\s);
}

% COMBINED ENDMAPS
% %%{
\begin{figure*}[!htb]
  \def\subfigwidth{0.24\linewidth}
  \centering
  % ROW 1
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.03\width} {0.0\height} {0.03\width} {0.0\height}}, clip=true]{fig/Selection_1344_fireworld_bright_gazebo.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/fireworld_gazebo1.jpg}};
      \markX[0.15]{1.8}{1.2} 
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.08\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/cavegz6.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.08\width} {0.0\height} {0.1\width} {0.0\height}}, clip=true]{fig/cavegz2.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/cavegz5.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.03\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1232.jpg}};
% Define size of the X marker
\def\xsize{0.3}

% Draw the white X marker at (0.2,0.3)
% \draw[line width=1.2pt, white]
%   (0.2-\xsize,0.3-\xsize) -- (0.2+\xsize,0.3+\xsize);
% \draw[line width=1.2pt, white]
%   (0.2-\xsize,0.3+\xsize) -- (0.2+\xsize,0.3-\xsize);

% \draw[line width=0.4pt, black]
%   (0.2-\xsize,0.3-\xsize) -- (0.2+\xsize,0.3+\xsize);
% \draw[line width=0.4pt, black]
      \markX[0.15]{2.6}{2.2} 
%   (0.2-\xsize,0.3+\xsize) -- (0.2+\xsize,0.3-\xsize);

      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.05\height}}, clip=true]{fig/Selection_1344_rooftops_gazebo2bright.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/Selection_1343_rooftops_gazebo2.png}};
      \markX[0.15]{1.9}{1.7} 
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/sparsebright2.png}};
      \markX[0.15]{0.95}{1.2} 
      \end{tikzpicture}
     \end{subfigure}

    % ROW 2
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % RED BLUE BLACK GREEN
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-11_urban_blue_black_red.png}};
        % HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-24_urban_heightmap_05squares_topdown.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-24_urban_heightmap_05squares_topdown.png}};
        % THICK HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_16-45_urban_heightmap_fallscale2_1m_square_06path.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_16-43_urban_fall_heightmap_scale_1m_squares.png}};
      \markX[0.15]{1.4}{1.5} 
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % RED BLUE BLACK GREEN
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-50_cave_2600_freespace.png}};
        % HEIGHTMAP
        \node[anchor=south west,inner sep=0] (rooftops_map) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_16-28_cave_heightmapscale4.png}};
      \markX[0.15]{3.2}{2.6} 
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % RED BLUE BLACK GREEN
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-23_16-12_rooftops_blackbluered.png}};
        % HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-37_rooftops_heightmap_pts_topdown.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-40_rooftops_topdown_betterheightmap.png}};
        % THICK HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_16-39_rooftops_1m_squares_heightmap_06line.png}};

        % \begin{scope}[x={(rooftops_map.south east)}, y={(rooftops_map.north west)}]

        %     \node[fill=none, rounded corners=2pt, font=\bfseries, text=black]
        %         at (0.1,0.1) {start};
        %     \draw[black, very thick, -] (0.1,0.15) -- (0.4,0.35);
        % \end{scope}

        \markX[0.15]{1.7}{1.6} 
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % RED BLUE BLACK GREEN
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-23_16-03_sparse_trajs3_thinner.png}};
        % HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-44_sparse_heightmap_05squares.png}};
        \node[anchor=south west,inner sep=0] (sparse_map) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-45_sparse_range2_05squares.png}};
        % THICK HEIGHTMAP
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/monoexplo_bigmaps/2025-10-24_15-45_sparse_range2_1msquares.png}};

        % START
        % Define a coordinate system matching the image
        \begin{scope}[x={(sparse_map.south east)}, y={(sparse_map.north west)}]

            % \node[fill=none, rounded corners=2pt, font=\bfseries, text=black]
            %     at (0.1,0.1) {start};
            % \draw[black, very thick, -] (0.1,0.15) -- (0.22,0.47);
        \end{scope}
        \markX[0.15]{0.95}{1.8} 
      \end{tikzpicture}
     \end{subfigure}


  \centering
  \caption{Trajectories of the best MonoSpheres (black) and Grid-Based explorer (blue) simulated experiments per each world (top row) along with the resulting MonoSpheres maps (bottom row). The map points are colored by height for clarity.
  % From left to right: Earthquake, Cave, Rooftops, Sparse.
  }
  \label{fig:exper_fireworld}
\end{figure*}
% %%}

% CAVEWORLD ENDMAPS
% %%{
% \begin{figure}[htb]
%   \def\subfigwidth{0.49\linewidth}
%   \centering
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/comparsion_cave_700.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1232.jpg}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_astar_final.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_mono_final2.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%   \centering
%   \caption{Best-result maps of the Grid-based exploration approach (bottom left) and MonoSpheres (bottom right) in the cave world (top right) exploration experiments, and the exploration progress of all runs (top left). Green = MonoSpheres, Blue = Grid-based. 
%   }
%   \label{fig:exper_cave}
% \end{figure}
% %%}

% FIREWORLD ENDMAPS V1
% %%{
% \begin{figure}[htb]
%   \def\subfigwidth{0.49\linewidth}
%   \centering
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/comparsion_fireworld_700.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/fireworld_gazebo1.jpg}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/fireworld_astar_final.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/fireworld_monospheres_final.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%   \centering
%   \caption{Best-result maps of the Grid-based exploration approach (bottom left) and MonoSpheres (bottom right) in the urban world (top right) exploration experiments, and the exploration progress of all runs (top left). Green = MonoSpheres, Blue = Grid-based.
%   }
%   \label{fig:exper_fireworld}
% \end{figure}
% %%}

% CAVEWORLD ENDMAPS
% %%{
% \begin{figure}[htb]
%   \def\subfigwidth{0.49\linewidth}
%   \centering
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/comparsion_cave_700.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/Selection_1232.jpg}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_astar_final.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0\height}}, clip=true]{fig/monoexplo_cave_mono_final2.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%   \centering
%   \caption{Best-result maps of the Grid-based exploration approach (bottom left) and MonoSpheres (bottom right) in the cave world (top right) exploration experiments, and the exploration progress of all runs (top left). Green = MonoSpheres, Blue = Grid-based. 
%   }
%   \label{fig:exper_cave}
% \end{figure}
% %%}

% ABLATION GRAPH ONLY
% \begin{figure}[htb]
%   \def\subfigwidth{0.95\linewidth}
%   \centering
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/ab_smolsquare_bubblegumgreen.png}};
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/abl_graph_superwide.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%   \centering
%   \caption{ablation test results on the urban world
%   }
%   \label{fig:exper_fireworld}
% \end{figure}

% ABLATION GRAPH AND TRAJS
\begin{figure}[htb]
  \def\subfigwidth{0.49\linewidth}
  \centering
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_graph_markers_tall.pdf}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_graph_markers_verytall_raster.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/abl_graph_noframe_smolsquare.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/abl_graph_fixed_snapped.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/squareholder.png}};
      \end{tikzpicture}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\subfigwidth}
      \begin{tikzpicture}
        \centering
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/ablations1.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_trajs_squre_angled.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_ortho.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.1\width} {0.1\height} {0.1\width} {0.1\height}}, clip=true]{fig/abl_ortho_nogrid.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.15\width} {0.0\height} {0.15\width} {0.0\height}}, clip=true]{fig/2025-10-23_13-49_ab_wide_all.png}};
        \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.15\width} {0.0\height} {0.15\width} {0.0\height}}, clip=true]{fig/2025-10-23_13-49_ab_wide_all.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.1\width} {0.1\height} {0.1\width} {0.1\height}}, clip=true]{fig/2025-10-23_13-47_abl_all_topdown_nogrid.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.1\width} {0.1\height} {0.1\width} {0.1\height}}, clip=true]{fig/abl_ortho_heightmap.png}};
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_trajs_nogrid1.png}};
        %2nd graph
        % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.07\width} {0.0\height}}, clip=true]{fig/ab_smolsquare_bubblegumgreen.png}};
      \end{tikzpicture}
     \end{subfigure}
  \centering
  \caption{Ablation test results on the earthquake world. Left: visualization of exploration progress, best runs per method are thick and a marker signifies a crash or mission completion. Right: best-run trajectories for each method.
  }
  \label{fig:ablations}
\end{figure}

%%{ ABLATION WIDE
% \begin{figure}[htb]
%   \def\subfigwidth{0.9\linewidth}
%   \centering
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_progress_wide1.png}};
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_graph_superwide.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\subfigwidth}
%       \begin{tikzpicture}
%         \centering
%         % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/ablations1.png}};
%         % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_trajs_wide2.png}};
%         \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/2025-10-23_13-49_ab_wide_all.png}};
%         % \node[anchor=south west,inner sep=0] (a) at (0,0) {\adjincludegraphics[width=1.0\linewidth, trim={{0.0\width} {0.0\height} {0.0\width} {0.0\height}}, clip=true]{fig/abl_trajs_squre_angled.png}};
%       \end{tikzpicture}
%      \end{subfigure}
%   \centering
%   \caption{Ablation test results on the Urban world
%   }
%   \label{fig:exper_fireworld}
% \end{figure}
%%}

% ABL TABLE
% %%{
% \begin{table}[h!]
% \centering
%   \begin{tabular}{p{22mm} p{12mm} p{13mm} p{13mm} } 
%  \hline
%     Method & Traveled & Mean $A$ & Max $A$  \\ [0.5ex] 
%  \hline\hline
%     Grid-Based & 20  & 1825 $m^2$ & 1981 $m^2$ \\
%     MonoSpheres  & 18 & 5118 $m^2$ & 5425 $m^2$ \\ 
%     No DistBased  & 5 & 5118 $m^2$ & 5425 $m^2$ \\ 
%     No OVDE  & 20 & 5118 $m^2$ & 5425 $m^2$ \\ 
%     No VPBlocking  & 20 & 5118 $m^2$ & 5425 $m^2$ \\ 
%     % OFS-disabled  & Cave & 4910 $m^2$ & 4910 $m^2$ & 0/1 \\ 
%  \hline
% \end{tabular}
%   \caption{Ablation tests results - 5 runs for each method in the Urban world }
% \label{table:ablations}
% \end{table}
% %%}


% SIM TABLE
% %%{
\begin{table}[h!]
\centering
  \begin{tabular}{m{18mm} m{12mm} m{10mm} m{10mm} m{10mm}} 
 \hline
    % \vspace{0.1mm}
    % Method & World & Mean $A$ & Max $A$ & Runs \\ [0.5ex] 
    \textbf{Method / Ablation} & \textbf{World} & \textbf{Mean} $A \left[m^2 \right] $ & \textbf{Max} $A \left[ m^2 \right] $ & \textbf{Max} $V \left[ m^3 \right] $\\ [0.5ex] 
    % \textbf{Method} & \textbf{World} & \textbf{Mean} $A$ & \textbf{Max} $A$ & \textbf{Runs} \\ [0.5ex] 
 \hline\hline
    % MonoSpheres  & Urban & 4919 $m^2$ & 6081 $m^2$ & 6 \\ 
    % Grid-Based & Urban  & 1911 $m^2$ & 2306 $m^2$ & 6 \\
    % No FFF  & Urban & 2470 $m^2$ & 3181 $m^2$ & 3 \\ 
    % No DBOF  & Urban & 2308 $m^2$ & 3081 $m^2$ & 3 \\ 
    % No OVDE  & Urban & 747 $m^2$ & 862 $m^2$ & 3 \\ 
    % No VPB  & Urban & 3697 $m^2$ & 4175 $m^2$ & 3 \\ 
    % \hline
    % MonoSpheres  & Cave & 7627 $m^2$ & 8006 $m^2$ & 3 \\ 
    % Grid-Based & Cave  & 3171 $m^2$ & 3375 $m^2$ & 3 \\
    % \hline
    % MonoSpheres  & Rooftops & 3760 $m^2$ & 4606 $m^2$ & 3 \\ 
    % Grid-Based & Rooftops  & 1360 $m^2$ & 1556 $m^2$ & 3 \\
    % \hline
    % MonoSpheres  & Sparse & 5127 $m^2$ & 5150 $m^2$ & 3 \\ 
    % Grid-Based & Sparse  & 2108 $m^2$ & 2168 $m^2$ & 3 \\
    % % OFS-disabled  & Cave & 4910 $m^2$ & 4910 $m^2$ & 0/1 \\ 

    MonoSpheres  & Earthquake & 1968& 2432& 17706 \\
    Grid-Based & Earthquake  & 764& 922& 5181 \\
    No FFF  & Earthquake & 988& 1272& 9518 \\
    No DBOF  & Earthquake & 923& 1232& 10356 \\
    No OVDE  & Earthquake & 299& 345& 1837 \\
    No VPB  & Earthquake & 1479& 1670& 13475 \\
    \hline
    MonoSpheres  & Cave & 3051& 3202& 18350 \\
    Grid-Based & Cave  & 1268& 1350& 8794 \\
    \hline
    MonoSpheres  & Rooftops & 1504& 1842& 21713 \\
    Grid-Based & Rooftops  & 544& 622& 3975 \\
    \hline
    MonoSpheres  & Sparse & 2051& 2060& 14193 \\
    Grid-Based & Sparse  & 843& 867& 3413 \\
    % OFS-disabled  & Cave & 1964.0& 1964.0& 0.0/0.4 \\
 \hline
\end{tabular}
  \caption{Quantitative results showing the explored area in the simulation experiments described in \autoref{sec:sim_exper}. }
\label{table:sim_exper}
\end{table}
% %%}

% \subsection{Comparison with Grid-Based Approach in Simulation}
\subsection{Multi-World Simulation Evaluation}
\label{sec:sim_exper}
% In this experiment, we demonstrate that the method is reliable, by running several experiments in the environment shown in TODO-FIG and noting the exploration results and failure cases.
% In the following experiments, we showcase the capabilities of our approach in simulated  urban areas with multiple buildings, collapsed trees, cars and other obstacles.
% TODO
% In this section, we quantitatively evaluate the performance of our method.
\todo{Q: Does the mapping and planning enable exploration at larger scales than conventional approaches?}
As the previously published monocular exploration methods do not have code available (with the exception of \cite{simon2023mononav}, which, however, is not ROS-compatible as of writing this paper), we prepared a simple grid-based mapping and exploration pipeline for comparison with MonoSpheres.
This reference method uses the same mapping approach as \cite{from_monoslam_to_explo, los_maps} (i.e. initializing free space between the camera and the visual SLAM points that have a low position covariance). 
To explore, the method periodically samples free-space points uniformly in a 40x40x10 \SI{}{\meter} area around the robot and sends these as goals to a grid-based A* path planning module, which finds path through the occupancy grid that would move the robot as close as possible to the goal while respecting the same distance from obstacles and uknown space as the MonoSpheres method (\SI{1.5}{\meter} in these experiments).
The robot then follows these paths with the camera pointing forward and replans at \SI{5}{\hertz} to handle measurement inacurracies. 
We use OctoMap \cite{octomap} for the grid-based mapping with a \SI{0.5}{\meter} cell size.
This value was chosen because smaller cell sizes caused the robot to not find any safe paths due to the free-space gaps documented in \cite{from_monoslam_to_explo} and larger cell sizes blocked exploration of narrow areas. 

As the evaluation metric, the volume of the constructed map is commonly used in exploration literature.
However, because of the open-area free-space sampling scheme and the depth interpolation, MonoSpheres constructs considerably more free space than the raytracing mapping approach over the same trajectories. 
For this reason, the volume metric would not be fair towards methods using the grid-based raytracing mapping. 
We instead divide the target exploration area into 2.5x2.5m columns that are marked as explored if the constructed map (sphere-based or grid-based) contains any element in a given column. 
The metric, $A$, is then the top-down 2D area of all the explored columns.

We compare the grid-based approach with MonoSpheres in multiple experiments on 4 different worlds, and terminate each run after \SI{20}{\minute} or sooner if a collision occurs.
The resulting maps and trajectories are shown in \autoref{fig:exper_fireworld} and the explored area and volume in \autoref{table:sim_exper}.
Videos from these experiments are available in the multimedia materials.
The experiments were conducted in the Gazebo simulator \cite{gazebo} with the same UAV platform and sensory setup as in the real-world experiments.
% Exploration goal generation was constrained to a 60x50x7 \SI{}{\meter} area of interest in the earthquake world for both methods, and to 170x110x20 \SI{}{\meter} in the cave world.
Exploration goal generation was constrained by a bounding box in the outdoor worlds.
\todo{[SIZES? NEEED THEM?]}

\subsection{Ablation Experiments}
To determine the contribution of the individual components of our proposed mapping and exploration method, we performed ablation tests in the earthquake world with 3 exploration runs for each ablation.
The results are shown in \autoref{table:sim_exper} and \autoref{fig:exper_fireworld}.
% Ablating OVDE, DBOF and FFF have drastic effects on the exploration performance, confirming the importance of these components.
Without DBOF, the UAV tends to quickly overwrite previously mapped obstacles with free space and crash.
Without forward-facing flight (FFF), where the UAV aligns itself with a goal viewpoint's heading when starting to navigate to it, the UAV is unable to detect previously undetected obstacles and also crashes fast.
Without virtual depth (OVDE), the UAV is unable to explore open-space areas and stays in the starting "valley" between the houses as expected.
Since the grid-based method allows casting rays even to faraway points, it has a better performance than this ablation.
Lastly, removing viewpoint blocking (VPB) does not lead to critical failures, as the simulated environment does not have large featureless walls, but the exploration performance is still notably reduced due to the UAV revisiting some viewpoints.
In summary, these results confirm the importance of all the proposed components of our mapping and exploration approach.

\subsection{Simulation Results Discussion}
\label{sec:discussion}
The main result of the simulation experiments is that our approach is capable of exploring many different topologies of environments -- sparse, dense, indoor, outdoor and environments where flight over obstacles in wide open spaces is required.
As expected based on the findings of \cite{from_monoslam_to_explo, los_maps}, the classical mapping approach was not able to map free space in open-space areas.
In all of the outdoor worlds, the classical mapping method with random exploration goal sampling was only able to map the area close to tall obstacles near the starting position.
% In the "cave" world, which is a texture-rich indoor environment, the grid-based approach performed similarly to MonoSpheres in terms of initial exploration speed.
% With a more sophisticated viewpoint selection approach, the grid-based approach could explore as much area in the end as MonoSpheres, supporting the viability of this mapping approach in indoor, textured areas claimed in \cite{from_monoslam_to_explo, los_maps}.
% This finding supports that the mapping approach presented in \cite{from_monoslam_to_explo, los_maps} for textured, indoor spaces.

We designed the earthquake environment to test the limits of our method, making it contain textureless areas (e.g. smooth cars and burned parts of buildings) and thin obstacles such as exposed metal rods.
As expected, this led to collisions in both methods (see \autoref{fig:ablations}, as these objects cannot be detected by the used OpenVINS frontend.
In particular, the grid-based method collided once with a smooth metal rod, and the MonoSpheres method collided in two runs with a car and with the burned textureless part of a building, albeit after exploring the majority of the environment. 
This is expected, as these objects cannot be detected by the used OpenVINS frontend, and could be improved by using a visual front-end that uses line-based or dense features (at the cost of computational complexity), or by leveraging low-cost ultrasound sensors as a bumper for the UAV.


\section{CONCLUSION}
% \todo{[In this paper, we reformulate 3D volumetric mapping and exploration to use a graph-of-spheres map instead of a traditionally used voxel-based one.]}
In this paper, we presented a novel, perception-coupled approach to mapping and planning for UAV exploration of 3D space with a monocular camera.
Ablation experiments confirm the utility of the individual modules of our proposed method.
% [building a graph-of-spheres representation using sparse depth estimates provided by monocular-inertial SLAM. 
% Our approach shows that it is possible to actively build a map allowing safe navigation and large-scale exploration, even with very sparse and rough depth estimates from a monocular camera. 
% Our method enables low-cost UAVs to actively build a map allowing safe navigation in unstructured environments, even with very sparse depth estimates from monocular-inertial SLAM. 
% Our method only requires a monocular camera and an IMU, and can run on a standard onboard computer without needing a GPU.
Compared to the existing approaches to 3D monocular exploration, which have so far presented autonomy at the scale of a few indoor rooms \cite{from_monoslam_to_explo, los_maps, simon2023mononav}, our proposed approach achieves autonomous exploration at the scale of an urban area with multiple buildings, trees and wide open spaces.
% With the same configuration, our method allows exploration of both indoor and outdoor environments.
With the same configuration, our method allows exploration of various environment topologies -- sparse, dense, indoor, outdoor and environments requiring flight over obstacles in wide open spaces.
% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word acknowledgment in America is without an e after the g. Avoid the stilted expression, One of us (R. B. G.) thanks . . .  Instead, try R. B. G. thanks. Put sponsor acknowledgments in the unnumbered footnote on the first page.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%%%{ BIBLIOGRAPHY
% \balance
\bibliographystyle{IEEEtran}
% DO NOT ERASE THE NEXT LINE,
% ONLY COMMENT IT AND DECOMMENT THE NEXT-NEXT, IF YOU NEED
% if you need it, get the repo git://redmine.laas.fr/laas/users/afranchi/bib.git and configure your bibinput in order to have : bibAlias,bibMain,bibNew,bibAF
\bibliography{main.bib}
%%%}


% \begin{thebibliography}{99}

% \bibitem{c1} G. O. Young, Synthetic structure of industrial plastics (Book style with paper title and editor), 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 1564.
% \bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123135.
% \bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
% \bibitem{c4} B. Smith, An approach to graphs of linear forms (Unpublished work style), unpublished.
% \bibitem{c5} E. H. Miller, A note on reflector arrays (Periodical styleAccepted for publication), IEEE Trans. Antennas Propagat., to be publised.
% \bibitem{c6} J. Wang, Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleSubmitted for publication), IEEE J. Quantum Electron., submitted for publication.
% \bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
% \bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style), IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
% \bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
% \bibitem{c10} J. U. Duncombe, Infrared navigationPart I: An assessment of feasibility (Periodical style), IEEE Trans. Electron Devices, vol. ED-11, pp. 3439, Jan. 1959.
% \bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, A clustering technique for digital communications channel equalization using radial basis function networks, IEEE Trans. Neural Networks, vol. 4, pp. 570578, July 1993.
% \bibitem{c12} R. W. Lucky, Automatic equalization for digital communication, Bell Syst. Tech. J., vol. 44, no. 4, pp. 547588, Apr. 1965.
% \bibitem{c13} S. P. Bingulac, On the compatibility of adaptive controllers (Published Conference Proceedings style), in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 816.
% \bibitem{c14} G. R. Faulhaber, Design of service systems with priority reservation, in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 38.
% \bibitem{c15} W. D. Doyle, Magnetization reversal in films with biaxial anisotropy, in 1987 Proc. INTERMAG Conf., pp. 2.2-12.2-6.
% \bibitem{c16} G. W. Juette and L. E. Zeffanella, Radio noise currents n short sections on bundle conductors (Presented Conference Paper style), presented at the IEEE Summer power Meeting, Dallas, TX, June 2227, 1990, Paper 90 SM 690-0 PWRS.
% \bibitem{c17} J. G. Kreifeldt, An analysis of surface-detected EMG as an amplitude-modulated noise, presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% \bibitem{c18} J. Williams, Narrow-band analyzer (Thesis or Dissertation style), Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% \bibitem{c19} N. Kawasaki, Parametric study of thermal and chemical nonequilibrium nozzle flow, M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% \bibitem{c20} J. P. Wilkinson, Nonlinear resonant circuit devices (Patent style), U.S. Patent 3 624 12, July 16, 1990. 






% \end{thebibliography}




\end{document}
